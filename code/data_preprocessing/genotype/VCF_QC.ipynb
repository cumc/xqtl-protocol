{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noted-verse",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Genotype VCF file quality control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-modeling",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This implements some recommendations from UK Biobank on [sequence data quality control](https://www.medrxiv.org/content/10.1101/2020.11.02.20222232v1.full-text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-brisbane",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this module is to perform QC on VCF files, including \n",
    "\n",
    "1. Handling the formatting of multi-allelic sites, \n",
    "2. Genotype and variant level filtering based on genotype calling qualities. \n",
    "3. Known/novel variants annotation\n",
    "4. Summary statistics before and after QC, in particular the ts/tv ratio, to assess the effectiveness of QC.\n",
    "\n",
    "1 and 2 will change the genotype data. 3 and 4 above are for explorative analysis on the overall quality assessment of genotype data in the VCF files. We annotate known and novel variants because ts/tv are expected to be different between known and novel variants, and is important QC metric to assess the effectiveness of our QC.\n",
    "\n",
    "### Multi-allelic sites\n",
    "\n",
    "Mult-allelic sites can be problematic in many ways for downstreams analysis, even of they are handled in terms of formatting after QC. We provide an optional workflow module to keep only bi-allelic sites from data, although by default we will include these sites in the VCF file we generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "empirical-intention",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Default VCF QC filters\n",
    "\n",
    "\n",
    "1. Genotype depth filters: For WES data, UK Biobank recommends **SNPs DP>10 and Indels DP>10 for indels.** However we think for WGS we can be less stringent, or simply rely on GQ. Users can set it to 1 eg, `--DP 1 --DP-indel 1 `\n",
    "2. Genotype quality GQ>20.\n",
    "3. At least one sample per site passed the allele balance threshold >= 0.15 for SNPs and >=0.20 for indels (heterozygous variants). \n",
    "    - Allele balance is calculated for heterozygotes as the number of bases supporting the least-represented allele over the total number of base observations.\n",
    "4. This module also allows for filtering by HWE and missingness although by default we don't filter on them.\n",
    "\n",
    "Filtering are done with `bcftools`. Here is a [useful cheatsheet from github user @elowy01](https://gist.github.com/elowy01/93922762e131d7abd3c7e8e166a74a0b).\n",
    "\n",
    "## A note on TS/TV summary from VCF genotype data\n",
    "\n",
    "`bcftools stats` command provides useful summary statistics including TS/TV ratio, which is routinely used as a quality measure of variant calls. With dbSNP based annotation of novel and known variants, `bcftools` can compute TS/TV for novel and known variants at variant level, and at sample level. It should be noted that variant level TS/TV does not take sample genotype into consideration -- it simply counts the TS and TV event for observed SNPs in the data. Other tools, such as `snpsift`, implements variant level TS/TV by counting TS and TV events in sample genotypes and compute the ratio after summing up TS and TV across all samples. See [here](https://github.com/samtools/bcftools/issues/1526) some discussions on this issue. We provide these TS/TV calculations before and after QC but users should be aware of the difference when interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-induction",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "1. The target `vcf.gz` file\n",
    "    - If its chromosome name does not have the `chr` prefix and you need it to match with reference `fasta` file, please run `rename_chrs` workflow to add `chr`.\n",
    "    - The vcf.gz file needs to be compressed by bgzip, instead of simple gzip\n",
    "    - It should have a index file accompanying it. The index file can be generated by tabix\n",
    "    - It must be a valid vcf.gz file that can pass bcftools sanity check: i.e. all tags are defined properly\n",
    "    - It must contains following fields:\n",
    "        1. ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Approximate read depth (reads with MQ=255 or with bad mates are filtered)\">\n",
    "        2. ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\">\n",
    "        3. ##FORMAT=<ID=AB,Number=1,Type=Float,Description=\"Allele balance for each het genotype\">\n",
    " \n",
    "2. dbSNP database in `VCF` format\n",
    "\n",
    "3. A reference sequence `fasta` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-tuning",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "1. QC-ed genotype data in VCF format. You can use `vcf_to_plink` implemented in `genotype_formatting.ipynb` to further convert it to PLINK format.\n",
    "2. A set of sumstats to help evaluate quality of genotype before and after QC\n",
    "    - Particularly useful is the TS/TV ratio\n",
    "    \n",
    "## Resource usage parameters\n",
    "\n",
    "- memory:   Usually a whole genome VCF.gz file has the size of 200+GB, after testing, a minimum of 60GB of mem is requried.\n",
    "- walltimes: For every hour qc_1 or qc_2 can process ~14G of data. The default is set to be 24h, corresponding to ~300GB of input. Please set the --walltime parameter according to the size of your input files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-dodge",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "The MWE is generated via \n",
    "```\n",
    "bcftools query -l get-dosage.ALL.vcf.gz | head -40 > MWE_sample_list\n",
    "bcftools view -S MWE_sample_list  get-dosage.ALL.vcf.gz > sample_filtered.vcf &\n",
    "bgzip -c sample_filtered.vcf >  sample_filtered.vcf.gz\n",
    "tabix -p vcf sample_filtered.vcf.gz\n",
    "bcftools view --regions chr1 sample_filtered.vcf.gz > chr1_sample_filtered.vcf &\n",
    "cat chr1_sample_filtered.vcf | head -20000 > MWE_genotype.vcf\n",
    "```\n",
    "and was stored here: https://drive.google.com/file/d/1sxxPdPIyKma0mAl8TKwhgyRHlOh0Oyrc/view?usp=sharing\n",
    "\n",
    "FIXME: point this to the synapse folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranking-schedule",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The MWE was used as follows:\n",
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb rename_chrs \\\n",
    "    --genoFile reference_data/00-All.vcf.gz \\\n",
    "    --cwd reference_data --container bioinfo.sif\n",
    "```\n",
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb dbsnp_annotate \\\n",
    "    --genoFile reference_data/00-All.add_chr.vcf.gz \\\n",
    "    --cwd reference_data --container bioinfo.sif\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb qc    \\\n",
    "    --genoFile data/MWE/MWE_genotype.vcf     \\\n",
    "    --dbsnp-variants data/reference_data/00-All.add_chr.variants.gz  \\\n",
    "    --reference-genome data/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta   \\\n",
    "    --cwd MWE/output/genotype_1 --container bioinfo.sif -J 1 -c csg.yml -q csg\n",
    "```\n",
    "\n",
    "To run in parallel for all genotype data listed in `mwe_genotype_list`,\n",
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb qc    \\\n",
    "    --genoFile data/mwe/mwe_genotype_list    \\\n",
    "    --dbsnp-variants data/reference_data/00-All.add_chr.variants.gz  \\\n",
    "    --reference-genome data/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta   \\\n",
    "    --cwd MWE/output/genotype_4 --container bioinfo.sif --add-chr\n",
    "```\n",
    "\n",
    "To produce the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-negative",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- Total TS/TV for 19639 known variants before QC: 2.599\n",
    "- Total TS/TV for 19573 known variants after QC: 2.600\n",
    "- There is no novel variants included in the MWE.\n",
    "\n",
    "The Total TS/TV is extracted from the last step of QC. For known variant before QC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "present-potential",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.599\n"
     ]
    }
   ],
   "source": [
    "grep Ts/Tv MWE_genotype.leftnorm.known_variant.snipsift_tstv | rev | cut -d',' -f1 | rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-protein",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For known variant after QC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggressive-necessity",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.600\n"
     ]
    }
   ],
   "source": [
    "grep Ts/Tv MWE_genotype.leftnorm.filtered.*_variant.snipsift_tstv | rev | cut -d',' -f1 | rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-importance",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For novel variant before/after QC, TS/TV is not avaible since no novel_variants presented in the MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-spyware",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "grep Ts/Tv MWE_genotype.leftnorm.novel_variant.snipsift_tstv | rev | cut -d',' -f1 | rev\n",
    "grep Ts/Tv MWE_genotype.leftnorm.filtered.novel_variant.snipsift_tstv | rev | cut -d',' -f1 | rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-strap",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sitting-princess",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run VCF_QC.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  rename_chrs\n",
      "  dbsnp_annotate\n",
      "  qc\n",
      "\n",
      "Global Workflow Options:\n",
      "  --genoFile  paths\n",
      "\n",
      "                        input can either be 1 vcf genoFile, or a list of vcf\n",
      "                        genoFile.\n",
      "  --remove-samples . (as path)\n",
      "                        The path to the file that contains the list of samples\n",
      "                        to remove (format FID, IID)\n",
      "  --keep-samples . (as path)\n",
      "                        The path to the file that contains the list of samples\n",
      "                        to keep (format FID, IID)\n",
      "  --cwd output (as path)\n",
      "                        Workdir\n",
      "  --numThreads 1 (as int)\n",
      "                        Number of threads\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 24h\n",
      "                        Walltime\n",
      "  --mem 60G\n",
      "                        Usually a whole genome VCF.gz file has the size of\n",
      "                        200+GB, after testing, a minimum of 60GB of mem is\n",
      "                        requried.\n",
      "  --container ''\n",
      "                        Software container option\n",
      "  --entrypoint ('micromamba run -n' + ' ' + container.split('/')[-1][:-4] + \" --no-capture-output\") if container.endswith('.sif') else \"\"\n",
      "\n",
      "  --[no-]add-chr (default to False)\n",
      "                        use this function to edit memory string for PLINK input\n",
      "\n",
      "Sections\n",
      "  rename_chrs:\n",
      "  dbsnp_annotate:\n",
      "  qc_1:                 Handel multi-allelic sites, left normalization of indels\n",
      "                        and add variant ID\n",
      "    Workflow Options:\n",
      "      --dbsnp-variants VAL (as path, required)\n",
      "                        Path to dbSNP variants generated previously\n",
      "      --reference-genome VAL (as path, required)\n",
      "                        Path to fasta file for HG reference genome, eg\n",
      "                        GRCh38_full_analysis_set_plus_decoy_hla.fa\n",
      "      --[no-]bi-allelic (default to False)\n",
      "      --[no-]snp-only (default to False)\n",
      "  qc_2:                 genotype QC\n",
      "    Workflow Options:\n",
      "      --geno-filter 0.2 (as float)\n",
      "                        Maximum missingess per-variant, default to 0.2\n",
      "      --DP-snp 10 (as int)\n",
      "                        Sample level QC - read depth (DP) to filter out SNPs\n",
      "                        below this value Default to 10, with WES data in mind\n",
      "                        But for WGS, setting it to 2 may be fine considering the\n",
      "                        WGS may have low DP but the GQ filter should be good\n",
      "                        enough\n",
      "      --GQ 20 (as int)\n",
      "                        Sample level QC - genotype quality (GQ) of specific\n",
      "                        sample. This measure tells you how confident we are that\n",
      "                        the genotype we assigned to a particular sample is\n",
      "                        correct\n",
      "      --DP-indel 10 (as int)\n",
      "                        Sample level QC - read depth (DP) to filter out indels\n",
      "                        below this value\n",
      "      --AB-snp 0.15 (as float)\n",
      "                        Allele balance for snps\n",
      "      --AB-indel 0.2 (as float)\n",
      "                        Allele balance for indels\n",
      "      --hwe-filter 0.0 (as float)\n",
      "                        HWE filter, default to 0.0 which means no HWE filter is\n",
      "                        applied\n",
      "  qc_3:\n"
     ]
    }
   ],
   "source": [
    "sos run VCF_QC.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-advocate",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-building",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# input can either be 1 vcf genoFile, or a list of vcf genoFile.\n",
    "parameter: genoFile = paths\n",
    "# The path to the file that contains the list of samples to remove (format FID, IID)\n",
    "parameter: remove_samples = path('.')\n",
    "# The path to the file that contains the list of samples to keep (format FID, IID)\n",
    "parameter: keep_samples = path('.')\n",
    "# Workdir\n",
    "parameter: cwd = path(\"output\")\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Walltime \n",
    "parameter: walltime = '24h'\n",
    "# Usually a whole genome VCF.gz file has the size of 200+GB, after testing, a minimum of 60GB of mem is requried.\n",
    "parameter: mem = '60G'\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "parameter: entrypoint=('micromamba run -a \"\" -n' + ' ' + container.split('/')[-1][:-4] + \" --no-capture-output\") if container.endswith('.sif') else \"\"\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f\"{cwd:a}\")\n",
    "parameter: add_chr = False\n",
    "\n",
    "import os\n",
    "def get_genotype_file(geno_file_paths):\n",
    "    #\n",
    "    def valid_geno_file(x):\n",
    "        suffixes = path(x).suffixes\n",
    "        if suffixes[-1] == '.bed':\n",
    "            return True\n",
    "        if len(suffixes)>1 and ''.join(suffixes[-2:]) == \".vcf.gz\":\n",
    "            return True\n",
    "        return False\n",
    "    #\n",
    "    def complete_geno_path(x, geno_file):\n",
    "        if not valid_geno_file(x):\n",
    "            raise ValueError(f\"Genotype file {x} should be VCF (end with .vcf.gz) or PLINK bed file (end with .bed)\")\n",
    "        if not os.path.isfile(x):\n",
    "            # relative path\n",
    "            if not os.path.isfile(f'{geno_file:ad}/' + x):\n",
    "                raise ValueError(f\"Cannot find genotype file {x}\")\n",
    "            else:\n",
    "                x = f'{geno_file:ad}/' + x\n",
    "        return x\n",
    "    # \n",
    "    def format_chrom(chrom):\n",
    "        if chrom.startswith('chr'):\n",
    "            chrom = chrom[3:]\n",
    "        return chrom\n",
    "    # Inputs are either VCF or bed, or a vector of them \n",
    "    if len(geno_file_paths) > 1:\n",
    "        if all([valid_geno_file(x) for x in geno_file_paths]):\n",
    "            return paths(geno_file_paths)\n",
    "        else: \n",
    "            raise ValueError(f\"Invalid input {geno_file_paths}\")\n",
    "    # Input is one genotype file or text list of genotype files\n",
    "    geno_file = geno_file_paths[0]\n",
    "    if valid_geno_file(geno_file):\n",
    "        return paths(geno_file)\n",
    "    else: \n",
    "        units = [x.strip().split() for x in open(geno_file).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "        if all([len(x) == 1 for x in units]):\n",
    "            return paths([complete_geno_path(x[0], geno_file) for x in units])\n",
    "        elif all([len(x) == 2 for x in units]):\n",
    "            genos = dict([(format_chrom(x[0]), path(complete_geno_path(x[1], geno_file))) for x in units])\n",
    "        else:\n",
    "            raise ValueError(f\"{geno_file} should contain one column of file names, or two columns of chrom number and corresponding file name\")\n",
    "        return genos\n",
    "    \n",
    "genoFile = get_genotype_file(genoFile)\n",
    "if add_chr:\n",
    "    genoFile = [f'{x:nn}.add_chr.vcf.gz' for x in genoFile]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpha-enhancement",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Annotate known and novel variants\n",
    "\n",
    "You can download the known variant reference from [this link](https://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b150_GRCh38p7/VCF/00-All.vcf.gz).\n",
    "\n",
    "For a detailed explanation of the procedure and its rationale, please refer to [this post](https://hbctraining.github.io/In-depth-NGS-Data-Analysis-Course/sessionVI/lessons/03_annotation-snpeff.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-physiology",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rename_chrs: provides = '{filename}.add_chr.vcf.gz']\n",
    "# This file can be downloaded from https://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b150_GRCh38p7/VCF/00-All.vcf.gz.\n",
    "input: f'{filename}.vcf.gz'\n",
    "output: f'{_input:nn}.add_chr.vcf.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:nn}.stderr', stdout = f'{_output:nn}.stdout', entrypoint=entrypoint\n",
    "    for i in {1..22} X Y MT; do echo \"$i chr$i\"; done > ${_output:nn}.chr_name_conv.txt\n",
    "    bcftools annotate --rename-chrs ${_output:nn}.chr_name_conv.txt ${_input} -Oz -o ${_output}\n",
    "    tabix -p vcf ${_output}\n",
    "    rm -f ${_output:nn}.chr_name_conv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-vacuum",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[dbsnp_annotate]\n",
    "input: genoFile\n",
    "output: f\"{cwd}/{_input:bnn}.variants.gz\"\n",
    "task: trunk_workers = 1, trunk_size=job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    # Extract specific fields from the VCF file using bcftools\n",
    "    bcftools query -f'%CHROM\\t%POS\\t%ID\\t%REF\\t%ALT\\n' ${_input} | \\\n",
    "    awk 'BEGIN { \n",
    "            OFS=\"\\t\" \n",
    "         } \n",
    "         {\n",
    "            # Calculate end position based on the length of REF or ALT\n",
    "            if (length($4) > length($5)) {\n",
    "                end_pos = $2 + (length($4) - 1)\n",
    "            } else {\n",
    "                end_pos = $2 + (length($5) - 1)\n",
    "            }\n",
    "            print $1, $2, end_pos, $3\n",
    "         }' | \\\n",
    "    # Compress the output using bgzip\n",
    "    bgzip -c > ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-college",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Genotype QC\n",
    "\n",
    "This step handles multi-allelic sites and annotate variants to known and novel. We add an RS ID to variants in dbSNP. Variants without rsID are considered novel variants. For every hour it can produce ~14Gb of data, please set the --walltime parameter according to the size of your input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baking-liberal",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Handel multi-allelic sites, left normalization of indels and add variant ID\n",
    "[qc_1 (variant preprocessing)]\n",
    "# Path to dbSNP variants generated previously\n",
    "parameter: dbsnp_variants = path\n",
    "# Path to fasta file for HG reference genome, eg GRCh38_full_analysis_set_plus_decoy_hla.fa\n",
    "parameter: reference_genome = path\n",
    "parameter: bi_allelic = False\n",
    "parameter: snp_only = False\n",
    "input: genoFile, group_by = 1\n",
    "output: f'{cwd}/{_input:bnn}.{\"leftnorm\" if not bi_allelic else \"biallelic\"}{\".snp\" if snp_only else \"\"}.vcf.gz'\n",
    "task: trunk_workers = 1, trunk_size=job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    # split multiallelic sites into biallelic records \n",
    "    ${'bcftools norm -m-any' if not bi_allelic else 'bcftools view -m2 -M2'} ${'-v snps' if snp_only else \"\"} ${_input} |\\\n",
    "    # Fix incorrect or missing REF alleles and warn about them\n",
    "    bcftools norm -d exact -N --check-ref ws -f ${reference_genome} -Oz --threads ${numThreads} |\\\n",
    "    # Fill missing tags\n",
    "    bcftools +fill-tags -- -t all,F_MISSING,'VD=sum(FMT/DP)' | \\\n",
    "    # Remove ID and replace with CHROM:POS:REF:ALT format\n",
    "    bcftools annotate -x ID -I +'%CHROM:%POS:%REF:%ALT' | \\\n",
    "    # Annotate with dbSNP rsID\n",
    "    bcftools annotate -a ${dbsnp_variants}  -h <(echo '##INFO=<ID=RSID,Number=1,Type=String,Description=\"dbSNP rsID\">') \\\n",
    "        -c CHROM,FROM,TO,INFO/RSID -Oz --threads ${numThreads} -o ${_output}\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    for i in ${_output} ; do \n",
    "        # Capture file metadata\n",
    "        output_info=\"$i\"\n",
    "        output_size=$(ls -lh \"$i\" | awk '{print $5}')\n",
    "        output_rows=$(zcat \"$i\" | wc -l)\n",
    "        output_column=$(zcat \"$i\" | grep -v \"##\" | head -1 | wc -w)\n",
    "        output_header_row=$(zcat \"$i\" | grep \"##\" | wc -l)\n",
    "        output_preview=$(zcat \"$i\" | grep -v \"##\" | head | cut -f 1-11)\n",
    "\n",
    "        # Write captured information to the stdout file\n",
    "        printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %d\\noutput_column: %d\\noutput_header_row: %d\\noutput_preview:\\n%s\\n\" \\\n",
    "            \"$output_info\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_header_row\" \"$output_preview\" >> ${_output:n}.stdout\n",
    "    done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-superior",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step filter variants based on FILTER PASS, DP and QC, fraction of missing genotypes (all samples), and on HWE, for snps and indels. It will also remove monomorphic sites -- using `bcftools view -c1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "textile-keyboard",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# genotype QC\n",
    "[qc_2 (variant level QC)]\n",
    "# Maximum missingess per-variant, default to 0.2\n",
    "parameter: geno_filter = 0.2\n",
    "# Sample level QC - read depth (DP) to filter out SNPs below this value\n",
    "# Default to 10, with WES data in mind \n",
    "# But for WGS, setting it to 2 may be fine considering the WGS may have low DP but the GQ filter should be good enough\n",
    "parameter: DP_snp = 10\n",
    "# Sample level QC - genotype quality (GQ) of specific sample. This measure tells you how confident we are that the genotype we assigned to a particular sample is correct\n",
    "parameter: GQ = 20\n",
    "# Sample level QC - read depth (DP) to filter out indels below this value\n",
    "parameter: DP_indel = 10\n",
    "# Allele balance for snps\n",
    "parameter: AB_snp = 0.15\n",
    "# Allele balance for indels\n",
    "parameter: AB_indel = 0.2\n",
    "# HWE filter, default to 0.0 which means no HWE filter is applied\n",
    "parameter: hwe_filter = 0.0\n",
    "output: f\"{_input:nn}.bcftools_qc.vcf.gz\"\n",
    "task: trunk_workers = 1, trunk_size=job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    # Initial filtering based on depth and genotype quality for SNPs and INDELs\n",
    "    bcftools filter -S . -e \\\n",
    "    '(TYPE=\"SNP\" & (FMT/DP)<${DP_snp} & (FMT/GQ)<${GQ}) | \n",
    "     (TYPE=\"INDEL\" & (FMT/DP)<${DP_indel} & (FMT/GQ)<${GQ})' ${_input} | \\\n",
    "     \n",
    "    # Further filtering to retain only variants that are PASS and have at least one non-reference allele \n",
    "    bcftools view -c1 | \\\n",
    "    bcftools view -f PASS | \\\n",
    "\n",
    "    # Filter based on genotype (hom/het) and allelic balance for SNPs and INDELs\n",
    "    bcftools filter -i \\\n",
    "    'GT=\"hom\" | \n",
    "     TYPE=\"snp\" & GT=\"het\" & (FORMAT/AD[*:1])/(FORMAT/AD[*:0] + FORMAT/AD[*:1]) >= ${AB_snp} | \n",
    "     TYPE=\"indel\" & GT=\"het\" & (FORMAT/AD[*:1])/(FORMAT/AD[*:0] + FORMAT/AD[*:1]) >= ${AB_indel}' | \\\n",
    "\n",
    "    # Filter based on missingness and Hardy-Weinberg equilibrium \n",
    "    bcftools filter -i 'F_MISSING<${geno_filter} & HWE>${hwe_filter}' \\\n",
    "    -Oz --threads ${numThreads} -o ${_output}\n",
    "\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    for i in ${_output} ; do \n",
    "        # Capture file metadata\n",
    "        output_info=\"$i\"\n",
    "        output_size=$(ls -lh \"$i\" | awk '{print $5}')\n",
    "        output_rows=$(zcat \"$i\" | wc -l)\n",
    "        output_column=$(zcat \"$i\" | grep -v \"##\" | head -1 | wc -w)\n",
    "        output_header_row=$(zcat \"$i\" | grep \"##\" | wc -l)\n",
    "        output_preview=$(zcat \"$i\" | grep -v \"##\" | head | cut -f 1-11)\n",
    "\n",
    "        # Write captured information to the stdout file\n",
    "        printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %d\\noutput_column: %d\\noutput_header_row: %d\\noutput_preview:\\n%s\\n\" \\\n",
    "            \"$output_info\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_header_row\" \"$output_preview\" >> ${_output:n}.stdout\n",
    "    done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-legislature",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[qc_3 (genotype data summary statistics)]\n",
    "input: output_from('qc_1'), output_from('qc_2'), group_by = 1\n",
    "output: f\"{cwd}/{_input:bnn}.novel_variant_sumstats\", \n",
    "        f\"{cwd}/{_input:bnn}.known_variant_sumstats\", \n",
    "        f\"{cwd}/{_input:bnn}.novel_variant.snipsift_tstv\",\n",
    "        f\"{cwd}/{_input:bnn}.known_variant.snipsift_tstv\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint=entrypoint\n",
    "    # Define the path to the SnpSift tool\n",
    "    SnpSift=\"/opt/snpEff/SnpSift.jar\"\n",
    "\n",
    "    # Compute statistics for novel variants (RSID is missing)\n",
    "    bcftools stats -i 'RSID=\".\"' -v ${_input} > ${_output[0]}\n",
    "\n",
    "    # Compute statistics for known variants (RSID is present)\n",
    "    bcftools stats -i 'RSID!=\".\"' -v ${_input} > ${_output[1]}\n",
    "\n",
    "    # Compute TS/TV for novel variants\n",
    "    bcftools filter -i 'RSID=\".\"' ${_input} | \\\n",
    "        java -jar \"$SnpSift\" tstv - > ${_output[2]}\n",
    "\n",
    "    # Compute TS/TV for known variants\n",
    "    bcftools filter -i 'RSID!=\".\"' ${_input} | \\\n",
    "        java -jar \"$SnpSift\" tstv - > ${_output[3]}\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    for i in ${_output} ; do \n",
    "        # Capture file metadata\n",
    "        output_info=\"$i\"\n",
    "        output_size=$(ls -lh \"$i\" | awk '{print $5}')\n",
    "        output_rows=$(zcat \"$i\" | wc -l)\n",
    "        output_column=$(zcat \"$i\" | grep -v \"##\" | head -1 | wc -w)\n",
    "        output_header_row=$(zcat \"$i\" | grep \"##\" | wc -l)\n",
    "        output_preview=$(zcat \"$i\" | grep -v \"##\" | head | cut -f 1-11)\n",
    "\n",
    "        # Write captured information to the stdout file\n",
    "        printf \"output_info: %s\\noutput_size: %s\\noutput_rows: %d\\noutput_column: %d\\noutput_header_row: %d\\noutput_preview:\\n%s\\n\" \\\n",
    "            \"$output_info\" \"$output_size\" \"$output_rows\" \"$output_column\" \"$output_header_row\" \"$output_preview\" >> ${_output:n}.stdout\n",
    "    done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
