{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# eQTL summary statistics formatting\n",
    "\n",
    "This workflow converts `fastqtl` eQTL analysis summary statistics text output to formats more friendly to R analysis. In particular:\n",
    "\n",
    "1. It converts single study results to HDF5 format grouped by genes.\n",
    "2. It combines multiple studies into one single HDF5 file. In the context of GTEx each study is result from one tissue.\n",
    "3. For MASH analysis in particular, it extracts from the complete data a subset of results to compute data driven MASH prior covariance, and to fit the MASH mixture model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input data\n",
    "\n",
    "### A list of summary statistics\n",
    "\n",
    "Summary statistics from `fasteqtl`-like output are in text format, one row per gene-snp pair. Columns in our example data-set are:\n",
    "\n",
    "```\n",
    "gene_id \n",
    "variant_id      \n",
    "tss_distance    \n",
    "ma_samples      \n",
    "ma_count        \n",
    "maf     \n",
    "pval_nominal    \n",
    "slope   \n",
    "slope_se\n",
    "```\n",
    "\n",
    "This is format of GTEx version 8 summary statistics data. Each analysis (\"tissue\" for GTEx) has a separate text file. Additionally there are support files of gene transcription start site coordinates, and SNP coordinates.\n",
    "\n",
    "The workflow takes a list of summary statistics file names, eg, `data/fastqtl/FastQTLSumStats.list` (can be configured) that has the contents:\n",
    "\n",
    "```\n",
    "Tissue_1.fastqtl.gz\n",
    "Tissue_2.fastqtl.gz\n",
    "...\n",
    "```\n",
    "\n",
    "The first two columns of these files have to be `gene_id` and `variant_id` (column name does not matter). For other contents we only need columns $\\hat{\\beta}$, $\\text{SE}(\\hat{\\beta})$ and p-value for the summary statistics. In the `fastqtl` output format above it is columns `8, 9, 7`. If your summary statistics file has a different format you can use `--cols` parameter to pass the proper column numbers. There are two ways to do it:\n",
    "\n",
    "1. Specify 3 numbers for  $\\hat{\\beta}$, $\\text{SE}(\\hat{\\beta})$ and p-value\n",
    "2. Specify 2 numbers for  $\\hat{\\beta}$ and p-value. $\\text{SE}(\\hat{\\beta})$ will then be computed on the fly assuming that the test is two-sided test and that $\\hat{\\beta}/\\text{SE}(\\hat{\\beta})$ is standard normal.\n",
    "\n",
    "Current default is `--cols 8 9 7`. **Currently this pipeline only supports and requires summary statistics $\\hat{\\beta}$, $\\text{SE}(\\hat{\\beta})$ and p-value, not other quantities (eg t statistic)**.\n",
    "\n",
    "### A list of gene names (optional)\n",
    "\n",
    "To speed up merging multiple HDF5 files it helps to provide a list of gene names. Otherwise it takes too much time to figure them out from individual HDF5 files before merger can happen. The gene list has the contents like:\n",
    "\n",
    "```\n",
    "ENSG00000186092.4\n",
    "ENSG00000227232.5\n",
    "ENSG00000228463.9\n",
    "ENSG00000241860.6\n",
    "ENSG00000268903.1\n",
    "ENSG00000269981.1\n",
    "ENSG00000279457.4\n",
    "ENSG00000279928.2\n",
    "...\n",
    "```\n",
    "\n",
    "## Run analysis\n",
    "\n",
    "Under the same folder as this list file, you keep all these listed data files. Then you run:\n",
    "\n",
    "```\n",
    "JOB_OPTION=\"-j 8\"\n",
    "#JOB_OPTION=\"-q midway2 -c midway2.yml\"\n",
    "sos run workflows/fastqtl_to_mash.ipynb convert \\\n",
    "    --data-list data/fastqtl/FastQTLSumStats.list \\\n",
    "    --gene-list data/fastqtl/GTEx_genes.txt \\\n",
    "    $JOB_OPTION\n",
    "```\n",
    "to convert to HDF5 only, and \n",
    "\n",
    "```\n",
    "sos run workflows/fastqtl_to_mash.ipynb \\\n",
    "    --data-list data/fastqtl/FastQTLSumStats.list \\\n",
    "    --gene-list data/fastqtl/GTEx_genes.txt \\\n",
    "    $JOB_OPTION\n",
    "```\n",
    "\n",
    "to convert to HDF5 AND extract MASH input.\n",
    "\n",
    "### Note on `JOB_OPTION`\n",
    "\n",
    "With bash variable `JOB_OPTION` set to `-q midway2 -c midway2.yml`, each workflow is configured to run on UChicago RCC's midway2 cluster, via `task` option. \n",
    "When executed as is, it takes 33hrs to convert the summary statistics for GTEx data (Release V8).\n",
    "You might need to configure `task` differently for your computing environment if you are familiar with `SoS`.\n",
    "Otherwise, you can use the `JOB_OPTION` `-j 8` to use 8 CPU threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## More options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "Bash",
    "tags": [
     "report_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run fastqtl_to_mash.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  convert\n",
      "  default\n",
      "  document_it\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd fastqtl_to_mash_output (as path)\n",
      "                        Work directory / output directory\n",
      "  --data-list data/eQTL_summary_files.txt (as path)\n",
      "                        A text file containing data-set names\n",
      "  --gene-list . (as path)\n",
      "                        Optionally, a list of gene names.\n",
      "  --msg 'eQTL mapping summary statistics'\n",
      "                        Meta-info tag for HDF5 database\n",
      "  --maxsize 1000 (as int)\n",
      "                        maximum number of groups per HDF5 file\n",
      "  --cols 8 9 7 (as list)\n",
      "                        1-based indexing of 3 numbers indicating columns for\n",
      "                        betahat, se(betahat) and p-value, or 2 numbesr for\n",
      "                        betahat and p-value\n",
      "  --keep-ensg-version 0 (as int)\n",
      "  --common-suffix ''\n",
      "                        for GTEx data, parameter: common_suffix =\n",
      "                        \".allpairs.txt\"\n",
      "\n",
      "Sections\n",
      "  convert_0, default_0: Generate utility functions\n",
      "  convert_1, default_1: Convert summary stats gzip format to HDF5\n",
      "  convert_2, default_2: Merge single study data to multivariate data\n",
      "  default_3:            Extract data to fit MASH model\n",
      "    Workflow Options:\n",
      "      --random-per-gene 9 (as int)\n",
      "                        Number of random SNPs to draw per gene for fitting MASH\n",
      "                        mixture Use -1 to export all\n",
      "      --best-per-gene 1 (as int)\n",
      "                        Use 1 to extract strongest gene-snp pair Use 0 to avoid\n",
      "                        extracting the info\n",
      "  default_4:            Subset and split data, generate Z-score and save to RDS\n",
      "    Workflow Options:\n",
      "      --random-snp-size 0.5 (as float)\n",
      "                        Size of mash random SNPs. This specifies the size of\n",
      "                        `random` SNP set and the rest of random SNPs go into\n",
      "                        `random_test` SNP set. if input is float it means half\n",
      "                        random SNPs the input can also be a integer to use\n",
      "                        specified number.\n",
      "      --effects-list NULL (as path)\n",
      "                        A list of effect names (SNP names) to include in mash\n",
      "                        analysis.\n",
      "      --conditions-list NULL (as path)\n",
      "                        A list of condition names (tissue names) to include in\n",
      "                        mash analysis\n",
      "  document_it:          Export workflow to HTML document\n"
     ]
    }
   ],
   "source": [
    "sos run fastqtl_to_mash.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output data\n",
    "\n",
    "If you run the entire workflow, you should find under `./fastqtl_to_mash_output` (can be configured):\n",
    "\n",
    "- Study (tissue) specific HDF5 files of summary statistics\n",
    "- Merged HDF5 from multiple studies\n",
    "- \"*.portable.h5\" data extracted for MASH computations\n",
    "- \"*.mash.rds\" data in RDS format splitted to `strong`/`random` and optionally `random_test`, with Z-scores computed from p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory / output directory\n",
    "parameter: cwd = path('./fastqtl_to_mash_output')\n",
    "# A text file containing data-set names\n",
    "parameter: data_list = path(\"data/eQTL_summary_files.txt\")\n",
    "# Optionally, a list of gene names.\n",
    "parameter: gene_list = path()\n",
    "# Meta-info tag for HDF5 database\n",
    "parameter: msg = \"eQTL mapping summary statistics\"\n",
    "# maximum number of groups per HDF5 file\n",
    "parameter: maxsize = 1000\n",
    "# 1-based indexing of 3 numbers indicating columns for betahat, se(betahat) and p-value, \n",
    "# or 2 numbesr for betahat and p-value\n",
    "parameter: cols = [8, 9, 7]\n",
    "parameter: keep_ensg_version = 0\n",
    "# for GTEx data, parameter: common_suffix = \".allpairs.txt\"\n",
    "parameter: common_suffix = \"\"\n",
    "\n",
    "if len(cols) == 2:\n",
    "    cols.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Pipeline details\n",
    "\n",
    "### HDF5 utilities\n",
    "\n",
    "The HDF5 Python interface `pytables` is used to program the data format conversion workhorse. You can use command, eg, `pip install tables` to install `pytables` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Generate utility functions\n",
    "[utils: provides = '.sos/utils.py']\n",
    "depends: Py_Module('tables'), Py_Module('pandas>0.23.0')\n",
    "output: '.sos/utils.py'\n",
    "report: expand = \"${ }\", output = _output\n",
    "    import sys, os, re, copy\n",
    "    import numpy as np, pandas as pd, tables as tb\n",
    "    tb.parameters.MAX_GROUP_WIDTH = 51200\n",
    "    # tb.parameters.NODE_CACHE_SLOTS = -51200\n",
    "    # tb.parameters.METADATA_CACHE_SIZE = 1048576 * 100000\n",
    "    # tb.parameters.CHUNK_CACHE_SIZE = 2097152 * 100000\n",
    "    # tb.parameters.CHUNK_CACHE_NELMTS = 521\n",
    "    import scipy.stats as st\n",
    "\n",
    "    class Environment:\n",
    "        def __init__(self):\n",
    "            self.float = np.float32\n",
    "            self.duplicate_tag = '_duplicated_'\n",
    "            self.common_suffix = '${common_suffix}.h5'\n",
    "\n",
    "    env = Environment()\n",
    "\n",
    "    class TBData(dict):\n",
    "        def __init__(self, data, name, msg = None, root = '/', complib = 'bzip2'):\n",
    "            '''bzip2 may not be compatible with other hdf5 applications; but zlib is fine'''\n",
    "            self.__root = root.strip('/')\n",
    "            self.__group = name\n",
    "            self.__msg = msg\n",
    "            try:\n",
    "                if type(data) is dict:\n",
    "                    self.update(data)\n",
    "                elif type(data) is str:\n",
    "                    # is file name\n",
    "                    self.__load(tb.open_file(data))\n",
    "                else:\n",
    "                    # is file stream\n",
    "                    self.__load(data)\n",
    "            except tb.exceptions.NoSuchNodeError:\n",
    "                raise ValueError('Cannot find dataset {}!'.format(name))\n",
    "            self.tb_filters = tb.Filters(complevel = 9, complib=complib)\n",
    "\n",
    "        def sink(self, filename):\n",
    "            with tb.open_file(filename, 'a') as f:\n",
    "                if self.__root:\n",
    "                    try:\n",
    "                        f.create_group(\"/\", self.__root)\n",
    "                    except:\n",
    "                        pass\n",
    "                try:\n",
    "                    # there is existing data -- have to merge with current data\n",
    "                    # have to do this because the input file lines are not grouped by gene names!!\n",
    "                    # use try ... except to hopefully faster than if ... else\n",
    "                    # e.g., if not f.__contains__('/{}'.format(self.__group)) ... else ...\n",
    "                    for element in f.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                        if element.name != 'colnames':\n",
    "                            self[element.name] = np.concatenate((element[:], self[element.name]))\n",
    "                except tb.exceptions.NoSuchNodeError:\n",
    "                    f.create_group(\"/\" + self.__root, self.__group,\n",
    "                                   self.__msg if self.__msg else self.__group)\n",
    "                for key in self:\n",
    "                    self.__store_array(key, f)\n",
    "                f.flush()\n",
    "\n",
    "        def dump(self, table, output = False):\n",
    "            if output:\n",
    "                pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames']).to_csv(sys.stdout, na_rep = 'NA')\n",
    "                return None\n",
    "            else:\n",
    "                return pd.DataFrame({self['colnames'][i] : self[table][:,i] for i in range(len(self['colnames']))}, index = self['rownames'])\n",
    "\n",
    "        def __load(self, fstream):\n",
    "            try:\n",
    "                for element in fstream.list_nodes('/{}/{}'.format(self.__root, self.__group)):\n",
    "                    self[element.name] = element[:]\n",
    "                fstream.close()\n",
    "            except:\n",
    "                fstream.close()\n",
    "                raise\n",
    "\n",
    "        def __roll_back(self, group, name):\n",
    "            try:\n",
    "                n = getattr(group, name)\n",
    "                n._f_remove()\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        def __store_array(self, name, fstream):\n",
    "            if self.__root:\n",
    "                element = getattr(getattr(fstream.root, self.__root), self.__group)\n",
    "            else:\n",
    "                element = getattr(fstream.root, self.__group)\n",
    "            arr = self[name]\n",
    "            if type(arr) is list:\n",
    "                arr = np.array(arr)\n",
    "            self.__roll_back(element, name)\n",
    "            #\n",
    "            if arr.shape != (0,):\n",
    "                ds = fstream.create_carray(element, name, tb.Atom.from_dtype(arr.dtype), arr.shape,\n",
    "                                           filters = self.tb_filters)\n",
    "                ds[:] = arr\n",
    "\n",
    "    def get_tb_grps(filenames, group_name = None):\n",
    "        if len(filenames) == 0:\n",
    "            return []\n",
    "        if isinstance(filenames, str):\n",
    "            filenames = [filenames]\n",
    "        names = set()\n",
    "        for filename in filenames:\n",
    "            with tb.open_file(filename) as f:\n",
    "                names.update([node._v_name for node in (f.root if group_name is None else getattr(f.root, '{}'.format(group_name)))])\n",
    "        return sorted(names)\n",
    "\n",
    "    def get_se(bhat, pval):\n",
    "        if bhat < 0:\n",
    "            z = st.norm.ppf(pval / 2)\n",
    "        else:\n",
    "            z = st.norm.ppf(1 - pval / 2)\n",
    "        if z != 0:\n",
    "            se = bhat/z\n",
    "        else:\n",
    "            se = np.nan\n",
    "        return se\n",
    "\n",
    "    class SSData:\n",
    "        def __init__(self, header = False):\n",
    "            self.data = {'buffer':{'data':[], 'rownames':[]}, 'output':{}}\n",
    "            self.header = header\n",
    "            self.previous_name = self.current_name = None\n",
    "            self.min_line_len = max([${cols[0]}, ${cols[1]}, ${cols[2]}])\n",
    "\n",
    "        def parse(self, line, ensg_version = 0, include = set()):\n",
    "            # input line is snp, gene, beta, t, pval and optionally se(bhat)\n",
    "            if not line:\n",
    "                self.__reset()\n",
    "                self.current_name = None\n",
    "                return 1\n",
    "            if isinstance(line, bytes):\n",
    "                line = line.decode()\n",
    "            line = line.strip().split()\n",
    "            if self.header:\n",
    "                # header line has to be accounted for\n",
    "                self.header = False\n",
    "                return 0\n",
    "            #\n",
    "            line[0] = line[0].strip('\"')\n",
    "            if not line[0] in include and len(include)>0:\n",
    "                return -1\n",
    "            # the line is not long enough ... there is format issues\n",
    "            if len(line) < self.min_line_len:\n",
    "                return -1\n",
    "            #\n",
    "            if ensg_version == 0:\n",
    "                line[0] = line[0].rsplit('.',1)[0]\n",
    "            if self.previous_name is None:\n",
    "                self.previous_name = line[0]\n",
    "            self.current_name = line[0]\n",
    "            if self.current_name != self.previous_name:\n",
    "                self.__reset()\n",
    "            if ${cols[2]} == 0:\n",
    "                # the two numbers are bhat and p-value, we need to compute se(bhat)\n",
    "                self.data['buffer']['data'].append([line[${cols[0]-1}], str(get_se(float(line[${cols[0]-1}]), float(line[${cols[1]-1}]))), line[${cols[1]-1}]])\n",
    "            else:\n",
    "                self.data['buffer']['data'].append([line[${cols[0]-1}], line[${cols[1]-1}], line[${cols[2]-1}]])\n",
    "            self.data['buffer']['rownames'].append(self.__format_variant_id(line[1]))\n",
    "            return 0\n",
    "        \n",
    "        @staticmethod\n",
    "        def __format_variant_id(value):\n",
    "            value = value.strip('\"').lstrip('chr').split('_')\n",
    "            if len(value) > 4:\n",
    "                # keep it chr, pos, ref, alt\n",
    "                value = value[:4]\n",
    "            return '_'.join(value)\n",
    "\n",
    "        def __reset(self):\n",
    "            self.data['buffer']['data'] = np.array(self.data['buffer']['data'], dtype = env.float)\n",
    "            self.data['buffer']['rownames'] = np.array(self.data['buffer']['rownames'])\n",
    "            self.data['buffer']['colnames'] = np.array(['beta','se','pval'])\n",
    "            self.data['output'] = copy.deepcopy(self.data['buffer'])\n",
    "            self.data['buffer'] = {'data':[], 'rownames':[]}\n",
    "\n",
    "        def dump(self):\n",
    "            return self.data['output']\n",
    "\n",
    "    class DataMerger(TBData):\n",
    "        def __init__(self, files, name, msg = None):\n",
    "            TBData.__init__(self, {}, name, msg, complib = \"zlib\")\n",
    "            self.files = sorted(files)\n",
    "            self.__group = name\n",
    "\n",
    "        def merge(self):\n",
    "            data = {}\n",
    "            one_snp = None\n",
    "            failure_ct = 0\n",
    "            # Collect data\n",
    "            for item in self.files:\n",
    "                tissue = re.sub(r'{}$'.format(env.common_suffix), '', os.path.basename(item))\n",
    "                try:\n",
    "                    data[tissue] = TBData(item, self.__group)\n",
    "                    if one_snp is None: one_snp = data[tissue]['rownames'][0]\n",
    "                except ValueError:\n",
    "                    data[tissue] = {'data' : np.array([[np.nan, np.nan, np.nan]]), 'rownames': None}\n",
    "                    failure_ct += 1\n",
    "                # Fix row name\n",
    "                # Because in GTEx data file there are duplicated gene-snp pairs having different sumstats!!\n",
    "                if data[tissue]['rownames'] is not None:\n",
    "                    data[tissue]['rownames'] = self.__dedup(data[tissue]['rownames'], item)\n",
    "            if failure_ct == len(self.files):\n",
    "                return 1\n",
    "            # Merge data\n",
    "            for idx, item in enumerate(['beta','se','pval']):\n",
    "                self[item] = pd.concat([pd.DataFrame(\n",
    "                    {tissue : data[tissue]['data'][:,idx]},\n",
    "                    index = data[tissue]['rownames'] if data[tissue]['rownames'] is not None else [one_snp]\n",
    "                    ) for tissue in sorted(data.keys())], sort=True, axis = 1)\n",
    "                if 'rownames' not in self:\n",
    "                    self['rownames'] = np.array(self[item].index, dtype = str)\n",
    "                if 'colnames' not in self:\n",
    "                    self['colnames'] = np.array(self[item].columns.values.tolist(), dtype = str)\n",
    "                self[item] = np.array(self[item].values, dtype = env.float)\n",
    "            # np.savetxt(sys.stdout, self['pval'], fmt='%10.5f')\n",
    "            # print(self['rownames'])\n",
    "            # print(self['colnames'])\n",
    "            return 0\n",
    "\n",
    "        def __dedup(self, seq, filename):\n",
    "            seen = {}\n",
    "            dups = set()\n",
    "            def __is_seen(x, seen):\n",
    "                if x not in seen:\n",
    "                    seen[x] = 0\n",
    "                    return 0\n",
    "                else:\n",
    "                    seen[x] += 1\n",
    "                    dups.add(x)\n",
    "                    return 1\n",
    "            # Tag them\n",
    "            obs = [x if not __is_seen(x, seen) else '%s%s%s' % (x, env.duplicate_tag, seen[x]) for x in seq]\n",
    "            # Log them\n",
    "            if len(dups):\n",
    "                filename = os.path.splitext(filename)[0]\n",
    "                with open(filename + '.error', 'a') as f:\n",
    "                    for item in dups:\n",
    "                        f.write('{}:{} appeared {} times in {}\\n'.\\\n",
    "                                format(self.__group, item, seen[item] + 1, filename))\n",
    "            return obs\n",
    "\n",
    "    def get_gs_pairs(data, name, num = (1, 9), method = 'equal_space'):\n",
    "        '''choose gene-snp pairs from data, controlled by num = (a, b)\n",
    "        for the best gene-snp pair (a = 0 or 1), and b other random \n",
    "        gene-snp pairs'''\n",
    "\n",
    "        def random_sample(x, k):\n",
    "            if k < 0:\n",
    "                return sorted(x)\n",
    "            return sorted(set(np.random.choice(x, min(len(x), k))))\n",
    "\n",
    "        def equal_sample(x, k):\n",
    "            if len(x) < k or k < 0:\n",
    "                return sorted(x)\n",
    "            f = lambda m, n: [i*n//m + n//(2*m) for i in range(m)]\n",
    "            return sorted(set([x[i] for i in f(k, len(x))]))\n",
    "\n",
    "        output = {'colnames' : data['colnames']}\n",
    "        lp = data.dump('pval')\n",
    "        shat = data.dump('se')\n",
    "        # pval cannot be nan or zero (in fastqtl output, Shat nan has pval zero)\n",
    "        lp = lp[np.all(np.isfinite(lp), axis=1) & np.all(np.isfinite(shat), axis=1)]\n",
    "        #\n",
    "        if lp.empty and num[0] > 0:\n",
    "            return None\n",
    "        # Find strong SNP-gene pair\n",
    "        lp = -np.log10(lp)\n",
    "        rowidx = np.where(data['rownames'] == lp.max(axis=1).idxmax())[0][0]\n",
    "        if num[0] != 0:\n",
    "            output['strong_rownames'] = ['%s_%s' % (name, data['rownames'][rowidx].decode())]\n",
    "            output['strong'] = {}\n",
    "            for k in ['beta', 'pval', 'se']:\n",
    "                output['strong'][k] = data[k][rowidx, :]\n",
    "        if num[1] != 0:\n",
    "            all_randomidxes = [y for y, x in enumerate(data['rownames']) if x in lp.index and y != rowidx]\n",
    "            sample_randomidxes = random_sample(all_randomidxes, num[1]) if method == 'random' else equal_sample(all_randomidxes, num[1])\n",
    "            output['random_rownames'] = ['%s_%s' % (name, data['rownames'][x].decode()) for x in sample_randomidxes]\n",
    "            output['random'] = {}\n",
    "            for k in ['beta', 'pval', 'se']:\n",
    "                output['random'][k] = data[k][sample_randomidxes, :]\n",
    "        if not 'strong' in output and not 'random' in output:\n",
    "            output = None\n",
    "        return output\n",
    "\n",
    "    def merge_tmp_h5(output, verbose = 0):\n",
    "        from glob import glob\n",
    "        tmpfiles = list(glob(output + \"_*.tmp\"))\n",
    "        if os.path.isfile(output+'.h5'):\n",
    "            os.remove(output+'.h5')\n",
    "        for item in sorted(tmpfiles):\n",
    "            for name in get_tb_grps(item):\n",
    "                cmd = 'h5copy -i {0} -o {2} -s \"/{1}\" -d \"/{1}\"'.format(item, name, output+'.h5')\n",
    "                if verbose:\n",
    "                    print(cmd)\n",
    "                os.system(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Convert to HDF5 format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "####  Per study (tissue) conversion\n",
    "\n",
    "For per study conversion I use `bzip2` compression method and `float32` to achieve higher compression rate. This workflow step will generate one HDF5 file per summary statistics file. `h5copy` program can be installed via `conda install -c conda-forge hdf5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert summary stats gzip format to HDF5\n",
    "[convert_1, default_1]\n",
    "depends: '.sos/utils.py', executable(\"h5copy\")\n",
    "fail_if(not data_list.is_file(), msg = 'Need data list file!')\n",
    "data_files = set(get_output(f\"awk '{{print $1}}' {data_list:e}\").strip().split('\\n'))\n",
    "fail_if(len(data_files) == 0, msg = 'Need input data files!')\n",
    "import os\n",
    "input: [f'{data_list:d}/{x}' if os.path.isfile(f'{data_list:d}/{x}') else x for x in data_files], group_by = 1\n",
    "output: f'{cwd:a}/{_input:bn}.h5'\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp\n",
    "    \n",
    "python: expand = \"${ }\", input = _depends[0]\n",
    "    import warnings\n",
    "    import gzip\n",
    "    gene_names = set([x.strip() for x in open(${gene_list:r}).readlines() if x.strip()] if ${gene_list.is_file()} else [])\n",
    "    ssp = SSData(header = True)\n",
    "    group_counts = 0\n",
    "    with gzip.open(${_input:r}) as f:\n",
    "        while True:\n",
    "            line = f.readline()\n",
    "            quit = ssp.parse(line, ${keep_ensg_version}, include = gene_names)\n",
    "            if quit == -1:\n",
    "                continue\n",
    "            if ssp.current_name != ssp.previous_name:\n",
    "                group_counts += 1\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "                    data = TBData(ssp.dump(), ssp.previous_name, \"${msg}\")\n",
    "                    data.sink(\"${_output:n}_%i.tmp\" % (np.ceil(group_counts / ${maxsize})) \n",
    "                                if ${maxsize} > 0 else ${_output:r})\n",
    "                ssp.previous_name = ssp.current_name\n",
    "            if quit:\n",
    "                break\n",
    "    if ${maxsize} > 0:\n",
    "        merge_tmp_h5(${_output:nr})\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Merge per study (tissue) HDF5 to one HDF5\n",
    "\n",
    "This step creates tables for each summary statistic per gene from multiple studies. Rows are effects (SNPs), columns are conditions (tissue names). This time I use `zlib` compression for better compatibility with other HDF5 routines (eg `rhdf5`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Merge single study data to multivariate data\n",
    "[convert_2, default_2]\n",
    "depends: '.sos/utils.py', executable(\"h5copy\")\n",
    "input: group_by = 'all'\n",
    "output: f'{cwd:a}/{data_list:bn}.h5'\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "\n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp\n",
    "\n",
    "python: expand = '${ }', input = _depends[0]\n",
    "    import warnings\n",
    "    if ${gene_list.is_file()}:\n",
    "        gene_names = [x.strip() for x in open(${gene_list:r}).readlines() if x.strip()]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${_input:r,}])\n",
    "    if ${keep_ensg_version} == 0:\n",
    "        gene_names = [os.path.splitext(x)[0] for x in gene_names]\n",
    "    failure_ct = 0\n",
    "    for idx, item in enumerate(gene_names):\n",
    "        ssm = DataMerger([${_input:r,}], item, \"${msg}\")\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "            if ssm.merge() == 0:\n",
    "                ssm.sink(\"${_output:n}_%i.tmp\" % (np.ceil((idx + 1.0) / ${maxsize})) \n",
    "                        if ${maxsize} > 0 else ${_output:r})\n",
    "            else:\n",
    "                failure_ct += 1\n",
    "    with open(\"${_output:n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups merged!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))\n",
    "    if ${maxsize} > 0:\n",
    "        merge_tmp_h5(${_output:nr})\n",
    "            \n",
    "bash: expand = True, workdir = cwd\n",
    "    rm -f {_output:n}_*.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Extract data for MASH model\n",
    "\n",
    "We need to extract the \"top\" signals based on single tissue analysis to compute MASH priors, as well as some random SNP sets to fit MASH mixture model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Extract data to fit MASH model\n",
    "[default_3]\n",
    "depends: '.sos/utils.py'\n",
    "# Number of random SNPs to draw per gene for fitting MASH mixture\n",
    "# Use -1 to export all\n",
    "parameter: random_per_gene = 9\n",
    "# Use 1 to extract strongest gene-snp pair\n",
    "# Use 0 to avoid extracting the info\n",
    "parameter: best_per_gene = 1\n",
    "output: f\"{_input:n}.portable.h5\"\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "\n",
    "python: expand = \"${ }\", input = _depends[0]\n",
    "    import warnings\n",
    "    if ${gene_list.is_file()}:\n",
    "        gene_names = [x.strip() for x in open(${gene_list:r}).readlines() if x.strip()]\n",
    "    else:\n",
    "        gene_names = get_tb_grps([${_input:r,}])\n",
    "    if ${keep_ensg_version} == 0:\n",
    "        gene_names = [os.path.splitext(x)[0] for x in gene_names]\n",
    "    output = dict()\n",
    "    output['random'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    output['strong'] = {'colnames': None, 'rownames': [], 'beta': None, 'se': None, 'pval': None}\n",
    "    failure_ct = 0\n",
    "    for idx, name in enumerate(gene_names):\n",
    "        # extract the best gene-snp pair or some random gene-snp pairs\n",
    "\n",
    "        try:\n",
    "            data = TBData(${_input:r}, name)\n",
    "            res = get_gs_pairs(data, name, (${best_per_gene}, ${random_per_gene}))\n",
    "        except ValueError:\n",
    "            res = None\n",
    "        #\n",
    "        if res is None:\n",
    "            failure_ct += 1\n",
    "            continue\n",
    "        for k in output:\n",
    "            if not k in res:\n",
    "                continue\n",
    "            for kk in output[k]:\n",
    "                if kk == 'rownames':\n",
    "                    if output[k]['rownames'] is None:\n",
    "                        output[k]['rownames'] = res[\"{}_rownames\".format(k)]\n",
    "                    else:\n",
    "                        output[k]['rownames'].extend(res[\"{}_rownames\".format(k)])\n",
    "                elif kk == 'colnames':\n",
    "                    if output[k]['colnames'] is None:\n",
    "                        output[k]['colnames'] = res['colnames']\n",
    "                else:\n",
    "                    output[k][kk] = np.vstack((output[k][kk], res[k][kk])) if output[k][kk] is not None else res[k][kk]\n",
    "    #\n",
    "    if failure_ct < len(gene_names):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category = tb.FlavorWarning)\n",
    "            for k in output:\n",
    "                TBData(dict(output[k]), k, msg = \"%s, %s gene-snp pair\" % (\"${msg}\", k), complib = 'zlib').sink(${_output:r})\n",
    "    with open(\"${_output:n}.log\", 'w') as f:\n",
    "        f.write(\"%s out of %s groups extracted!\\n\" % (len(gene_names) - failure_ct, len(gene_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Compute Z score and save to RDS\n",
    "\n",
    "Here I also implemented two command options to subset data:\n",
    "\n",
    "- `--effects-list`: a list of effect names (row names) to include from output MASH data-set. This option was used in MASH paper to remove SNPs in LD with each other.\n",
    "- `--conditions-list`: a list of conditions (column names) to include in output MASH data-set. Conditions not in this list are excluded from output. This option was used in MASH paper to focus analysis on brain / no-brain tissues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Subset and split data, generate Z-score and save to RDS\n",
    "[default_4]\n",
    "# Size of mash random SNPs. \n",
    "# This specifies the size of `random` SNP set \n",
    "# and the rest of random SNPs go into `random_test` SNP set.\n",
    "# if input is float it means half random SNPs\n",
    "# the input can also be a integer to use specified number.\n",
    "parameter: random_snp_size = 0.5\n",
    "# A list of effect names (SNP names) to include in mash analysis.\n",
    "parameter: effects_list = path('NULL')\n",
    "# A list of condition names (tissue names) to include in mash analysis\n",
    "parameter: conditions_list = path('NULL')\n",
    "\n",
    "depends: R_library('rhdf5')\n",
    "output: f\"{_input:n}.mash.rds\" if not str(_input).endswith('.portable.h5') else f\"{_input:nn}.mash.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "\n",
    "R: expand = \"${ }\"\n",
    "    ConvertP2Z <- function(pval, beta) {\n",
    "      z <- abs(qnorm(pval / 2))\n",
    "      z[which(beta < 0)] <- -1 * z[which(beta < 0)]\n",
    "      return(z)\n",
    "    }\n",
    "\n",
    "    handle_nan_b = function(x) {\n",
    "          x[which(is.nan(x) | is.na(x))] = 0\n",
    "          return(x)\n",
    "    }\n",
    "\n",
    "    handle_nan_s = function(x) {\n",
    "          x[which(is.nan(x) | is.infinite(x) | is.na(x) | x == 0)] = 1E3\n",
    "          return(x)\n",
    "    }\n",
    "  \n",
    "    handle_nan_z = function(z, b, s) {\n",
    "          z[which(is.nan(s) | is.infinite(s) | is.na(s) | s == 0)] = 0\n",
    "          z[which(is.nan(b) | is.na(b) | b == 0)] = 0\n",
    "          z[which(is.nan(z) | is.na(z))] = 0\n",
    "          return(z)\n",
    "    }\n",
    "  \n",
    "    GetSS <- function(table, db) {\n",
    "      dat <- rhdf5::h5read(db, table)\n",
    "      dat$\"z\" <- ConvertP2Z(dat$\"pval\", dat$\"beta\")\n",
    "      for (name in c(\"beta\", \"se\", \"pval\", \"z\")) {\n",
    "        dat[[name]] <- as.matrix(t(dat[[name]]))\n",
    "        colnames(dat[[name]]) <- dat$colnames\n",
    "        rownames(dat[[name]]) <- dat$rownames\n",
    "      }\n",
    "      dat$colnames <- dat$rownames <- NULL\n",
    "      dat[['z']] = handle_nan_z(dat[['z']], dat[['beta']], dat[['se']])\n",
    "      dat[['se']] = handle_nan_s(dat[['se']])\n",
    "      dat[['beta']] = handle_nan_b(dat[['beta']])\n",
    "      return(dat)\n",
    "    }\n",
    "  \n",
    "    SplitTrainTest <- function(dat, table) {\n",
    "        # load data\n",
    "        strong = dat$strong[[table]]\n",
    "        random = dat$random[[table]]\n",
    "        # select rows to keep\n",
    "        num_train = ${random_snp_size}\n",
    "        if (num_train < 1)\n",
    "          num_train = nrow(random) * num_train\n",
    "        num_train = as.integer(num_train)\n",
    "        if (num_train > nrow(random)) {\n",
    "            num_train = nrow(random)\n",
    "        }\n",
    "        train = random[1:num_train,]\n",
    "        if (num_train == nrow(random)) {\n",
    "            test = NULL\n",
    "        } else {\n",
    "            test = random[(num_train+1):nrow(random),]\n",
    "        }\n",
    "        if (${effects_list:r} != 'NULL') {\n",
    "            pout = scan(${effects_list:ar}, what=\"character\", sep=NULL)\n",
    "            strong = strong[(rownames(strong) %in% pout),]\n",
    "            train = train[(rownames(train) %in% pout),]\n",
    "            if (!is.null(test))\n",
    "                test = test[(rownames(test) %in% pout),]                                         \n",
    "        }\n",
    "        if (${conditions_list:r} != 'NULL') {\n",
    "            rout = scan(${conditions_list:ar}, what=\"character\", sep=NULL)\n",
    "            strong = strong[,(colnames(strong) %in% rout),drop=F]\n",
    "            train = train[,(colnames(train) %in% rout),drop=F]\n",
    "            if (!is.null(test))\n",
    "                test = test[,(colnames(test) %in% rout),drop=F]\n",
    "        } \n",
    "\n",
    "        return(list(random = train,\n",
    "               random.test = test,\n",
    "               strong = strong))\n",
    "    }\n",
    "      \n",
    "    SS_data = list(strong = GetSS('strong', ${_input:r}), random = GetSS('random', ${_input:r}))\n",
    "    ztable = SplitTrainTest(SS_data, \"z\")\n",
    "    btable = SplitTrainTest(SS_data, \"beta\")\n",
    "    stable = SplitTrainTest(SS_data, \"se\")\n",
    "    # save output\n",
    "    saveRDS(list(random.z = ztable$random,\n",
    "                 random.test.z = ztable$random.test,\n",
    "                 strong.z = ztable$strong,\n",
    "                 random.b = btable$random,\n",
    "                 random.test.b = btable$random.test,\n",
    "                 strong.b = btable$strong,\n",
    "                 random.s = stable$random,\n",
    "                 random.test.s = stable$random.test,\n",
    "                 strong.s = stable$strong), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The output is an R list of data matrices:\n",
    "\n",
    "```\n",
    "> dat = readRDS('FastQTLSumStats.mash.rds')\n",
    "> names(dat)\n",
    "[1] \"random.z\"      \"random.test.z\" \"strong.z\"      \"random.b\"     \n",
    "[5] \"random.test.b\" \"strong.b\"      \"random.s\"      \"random.test.s\"\n",
    "[9] \"strong.s\"\n",
    "```\n",
    "\n",
    "`*.z`, `*.b` and `*.s` stands for z-scores, $\\hat{\\beta}$ and $\\text{SE}(\\hat{\\beta})$ (standard error). `strong` is data of the \"strongest\" gene-SNP pair; `random` is a random subset of gene-SNP pairs. They are used to find pattern of sharing in strong signals and learning mixture weights. Additionally `random.test.*` is a seperate random data-set that can be used to assess fit / overfit of the MASH mixture model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Export workflow documentation to HTML format\n",
    "\n",
    "```\n",
    "sos run workflow/fastqtl_to_mash document_it\n",
    "```\n",
    "\n",
    "will export the narrative in this workflow notebook to HTML format as a documentation file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Export workflow to HTML document\n",
    "[document_it]\n",
    "input: [item for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 1\n",
    "output: [f'{cwd:a}/{item:bn}.html' for item in paths(sys.argv) if item.suffix == '.ipynb'], group_by = 2\n",
    "bash: expand = True, stderr = False\n",
    "  sos convert {_input} {_output} --template sos-report-toc"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "default_kernel": "SoS",
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
