{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "heated-collins",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Fine-mapping with SuSiE RSS model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-thriller",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook take a list of LD reference files and a list of sumstat files from various association studies ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-sullivan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-digit",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "\n",
    "I. **GWAS Summary Statistics Files**\n",
    "- **Input**: Vector of files for one or more GWAS studies.\n",
    "- **Format**: \n",
    "  - Tab-delimited files.\n",
    "  - First 4 columns: `chrom`, `pos`, `A1`, `A2`\n",
    "  - Additional columns can be loaded using column mapping file see below  \n",
    "- **Column Mapping files (optional)**:\n",
    "  - Optional YAML file for custom column mapping.\n",
    "  - Required columns: `chrom`, `pos`, `A1`, `A2`, either `z` or (`beta` and `se`), `n_sample`, `n_case`, `n_control`. Note: You can only provide `n_sample` or (`n_case` & `n_control`, together for case control study), fill with 0 if do not provide them. If none of these are known, fill `n_sample`, `n_case`, `n_control` with 0.\n",
    "  - Optional columns: `var_y`.\n",
    "\n",
    "II. **GWAS Summary Statistics Meta-File**: this is optional and helpful when there are lots of GWAS data to process via the same command\n",
    "- **Columns**: `study_id`, chromosome number, path to summary statistics file, optional path to column mapping file.\n",
    "- **Note**: Chromosome number `0` indicates a genome-wide file.\n",
    "\n",
    "eg: `gwas_meta.tsv`\n",
    "\n",
    "```\n",
    "study_id\tchrom\tfile_path\tcolumn_mapping_file\tn_sample\tn_case\tn_control\n",
    "study1\t0\tBellenguez.tsv\tgwas.yml\t0\t0\t0\n",
    "study2\t1\thg38.txt\tJansen.yml\t0    313213    31233\n",
    "```\n",
    "\n",
    "If both summary stats file (I) and meta data file (II) are specified we will take the union of the two.\n",
    "\n",
    "eg. `column_mapping.yml` left: standard name. Right: original column name. (do not add space before and after \":\")\n",
    "\n",
    "```\n",
    "chrom:chromosome\n",
    "pos:base_pair_location\n",
    "A1:effect_allele\n",
    "A2:other_allele\n",
    "beta:beta\n",
    "se:standard_error\n",
    "pvalue:p_value\n",
    "maf:maf\n",
    "n_case:n_cases\n",
    "n_control:n_controls\n",
    "n_sample:n\n",
    "```\n",
    "\n",
    "\n",
    "III. **LD Reference Metadata File**\n",
    "- **Format**: Single TSV file.\n",
    "- **Contents**:\n",
    "  - Columns: `#chrom`, `start`, `end`, path to the LD matrix, genomic build.\n",
    "  - LD matrix path format: comma-separated, first entry is the LD matrix, second is the bim file.\n",
    "- **Documentation**: Refer to our LD reference preparation document for detailed information (Tosin pending update).\n",
    "\n",
    "IV. For analyzing specific genomic regions, you can specify them using the `--region-names` option in the 'chr:start-end' format, where multiple regions are accepted. Alternatively, you may provide a file containing a list of regions through the `--region-list` option, also adhering to the 'chr:start-end' format. When both `--region-names` and `--region-list` are provided, union of these options will be used to analyze. In cases where neither option is specified, the analysis defaults to encompass all regions specified in the LD reference metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-argument",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-lobby",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Each rds file is the finemapping result of one LD block. Including 10 elements in each rds file.\n",
    "1. single_effect_regression: Assume only one causal variant, simple regression.\n",
    "2. noqc: Susie finemapping, the sumstat is only allele_qced.\n",
    "3. qc_impute: Susie finemapping, the summary statistics are QCed (suspecious outliers of z scores removed) and imputed (all outliers and missing variants from reference panel).\n",
    "4. qc_only: Susie finemapping, the summary statistics are only QCed .\n",
    "5. conditional_regression_noqc: Bayesian conditional regression, original sumstat\n",
    "6. conditional_regression_qc_only: Bayesian conditional regression, sumstat QCed\n",
    "7. conditional_regression_qc_impute: Bayesian conditional regression, sumstat QCed and imputed.\n",
    "8. sumstats_allele_qc_only: a table of sumstat after allele qc (check each variant to see if ref/alt are alipped, and correct them)\n",
    "9. sumstats_qc_impute: a table of sumstat after qc and imputation.\n",
    "10. sumstats_qc_impute: a table of sumstat after qc and imputation, bad quality imputation variants removed by empirical thresholds. (the actual input of all qc_impute results)\n",
    "\n",
    "Each rds file is accompanied by 2 tsv files reporting elements 8 and 9 in tsv format for the sake of convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-groove",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-biodiversity",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run xqtl-pipeline/pipeline/SuSiE_RSS.ipynb SuSiE_RSS \\\n",
    "    --ld-meta-data ADSP_R4_EUR.LD.list \\\n",
    "    --gwas-meta-data AD_sumstat_list.txt \\\n",
    "    --impute \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-lecture",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output/\")\n",
    "parameter: gwas_meta_data = path()\n",
    "parameter: ld_meta_data = path()\n",
    "parameter: gwas_name = []\n",
    "parameter: gwas_data = []\n",
    "parameter: column_mapping = []\n",
    "parameter: region_list = path()\n",
    "parameter: region_name = []\n",
    "parameter: container = ''\n",
    "parameter: skip_regions = []\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: job_size = 5\n",
    "parameter: walltime = \"10h\"\n",
    "parameter: mem = \"16G\"\n",
    "parameter: numThreads = 1\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "import os\n",
    "if (not os.path.isfile(region_list)) and len(region_name) == 0:\n",
    "    region_list = ld_meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-limit",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "def file_exists(file_path, relative_path=None):\n",
    "    \"\"\"Check if a file exists at the given path or relative to a specified path.\"\"\"\n",
    "    if os.path.exists(file_path) and os.path.isfile(file_path):\n",
    "        return True\n",
    "    elif relative_path:\n",
    "        relative_file_path = os.path.join(relative_path, file_path)\n",
    "        return os.path.exists(relative_file_path) and os.path.isfile(relative_file_path)\n",
    "    return False\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def parse_region(region):\n",
    "    \"\"\"Parse a region string in 'chr:start-end' format into a list [chr, start, end].\"\"\"\n",
    "    chrom, rest = region.split(':')\n",
    "    start, end = rest.split('-')\n",
    "    return [int(chrom), int(start), int(end)]\n",
    "\n",
    "def extract_regional_data(gwas_meta_data, gwas_name, gwas_data, column_mapping, region_name=None, region_list=None):\n",
    "    \"\"\"\n",
    "    Extracts data from GWAS metadata files and additional GWAS data provided. \n",
    "    Optionally filters data based on specified regions.\n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - gwas_name (list): Vector of GWAS study names.\n",
    "    - gwas_data (list): Vector of GWAS data.\n",
    "    - column_mapping (list, optional): Vector of column mapping files.\n",
    "    - region_name (list, optional): List of region names in 'chr:start-end' format.\n",
    "    - region_list (str, optional): File path to a file containing regions.\n",
    "\n",
    "    Returns:\n",
    "    - GWAS Dictionary: Maps study IDs to a list containing chromosome number, \n",
    "      GWAS file path, and optional column mapping file path.\n",
    "    - Region Dictionary: Maps region names to lists [chr, start, end].\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If any specified file path does not exist.\n",
    "    - ValueError: If required columns are missing in the input files or vector lengths mismatch.\n",
    "    \"\"\"\n",
    "    # Check vector lengths\n",
    "    if len(gwas_name) != len(gwas_data):\n",
    "        raise ValueError(\"gwas_name and gwas_data must be of equal length\")\n",
    "    \n",
    "    if len(column_mapping) > 0 and len(column_mapping) != len(gwas_name):\n",
    "        raise ValueError(\"If column_mapping is provided, it must be of the same length as gwas_name and gwas_data\")\n",
    "\n",
    "    # Required columns for GWAS file type\n",
    "    required_gwas_columns = ['study_id', 'chrom', 'file_path']\n",
    "\n",
    "    # Base directory of the metadata files\n",
    "    gwas_base_dir = os.path.dirname(gwas_meta_data)\n",
    "    \n",
    "    # Reading the GWAS metadata file\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_gwas_columns)\n",
    "    gwas_dict = OrderedDict()\n",
    "\n",
    "    # Process additional GWAS data from vectors\n",
    "    for name, data, mapping in zip(gwas_name, gwas_data, column_mapping or [None]*len(gwas_name)):\n",
    "        gwas_dict[name] = {0: [data, mapping]}\n",
    "\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_path = row['file_path']\n",
    "        mapping_file = row.get('column_mapping_file')\n",
    "        n_sample = row.get('n_sample')\n",
    "        n_case = row.get('n_case')\n",
    "        n_control = row.get('n_control')\n",
    "\n",
    "        # Check if the file and optional mapping file exist\n",
    "        if not file_exists(file_path, gwas_base_dir) or (mapping_file and not file_exists(mapping_file, gwas_base_dir)):\n",
    "            raise FileNotFoundError(f\"File {file_path} not found for {row['study_id']}\")\n",
    "        \n",
    "        # Adjust paths if necessary\n",
    "        file_path = file_path if file_exists(file_path) else os.path.join(gwas_base_dir, file_path)\n",
    "        if mapping_file:\n",
    "            mapping_file = mapping_file if file_exists(mapping_file) else os.path.join(gwas_base_dir, mapping_file)\n",
    "        \n",
    "        # Create or update the entry for the study_id\n",
    "        if row['study_id'] not in gwas_dict:\n",
    "            gwas_dict[row['study_id']] = {}\n",
    "\n",
    "        # Expand chrom 0 to chrom 1-22 or use the specified chrom\n",
    "        chrom_range = range(1, 23) if row['chrom'] == 0 else [row['chrom']]\n",
    "        for chrom in chrom_range:\n",
    "            if chrom in gwas_dict[row['study_id']]:\n",
    "                existing_entry = gwas_dict[row['study_id']][chrom]\n",
    "                raise ValueError(f\"Duplicate chromosome specification for study_id {row['study_id']}, chrom {chrom}. \"\n",
    "                                 f\"Conflicting entries: {existing_entry} and {[file_path, mapping_file]}\")\n",
    "            gwas_dict[row['study_id']][chrom] = [file_path, mapping_file, n_sample, n_case, n_control]\n",
    "\n",
    "    # Process region_list and region_name\n",
    "    region_dict = dict()\n",
    "    if region_list and os.path.isfile(region_list):\n",
    "        with open(region_list, 'r') as file:\n",
    "            for line in file:\n",
    "                # Skip empty lines\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                if line.startswith(\"#\"):\n",
    "                    continue\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) == 1:\n",
    "                    region = parse_region(parts[0])\n",
    "                elif len(parts) >= 3:\n",
    "                    region = [int(parts[0].replace(\"chr\", \"\")), int(parts[1]), int(parts[2])]\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid region format in region_list\")\n",
    "                \n",
    "                region_dict[f\"{region[0]}:{region[1]}_{region[2]}\"] = region\n",
    "                \n",
    "    if region_name:\n",
    "        for region in region_name:\n",
    "            parsed_region = parse_region(region)\n",
    "            region_key = f\"{parsed_region[0]}:{parsed_region[1]}-{parsed_region[2]}\"\n",
    "            if region_key not in region_dict:\n",
    "                region_dict[region_key] = parsed_region\n",
    "\n",
    "    return gwas_dict, region_dict\n",
    "\n",
    "gwas_dict, region_dict = extract_regional_data(gwas_meta_data, gwas_name, gwas_data, column_mapping, region_name, region_list)\n",
    "regional_data = dict([(\"GWAS\", gwas_dict), (\"regions\", region_dict)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "durable-strip",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_1]\n",
    "parameter: L = 5\n",
    "parameter: max_L = 10\n",
    "# If available the column that indicates sample size within the sumstats\n",
    "# filtering threshold for raiss imputation\n",
    "parameter: rcond = 0.01\n",
    "parameter: R2_threshold = 0.6\n",
    "parameter: l_step = 5\n",
    "parameter: pip_cutoff = 0.025\n",
    "parameter: minimum_ld = 5\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "# Whether to impute the sumstat for all the snp in LD but not in sumstat.\n",
    "parameter: impute = False \n",
    "parameter: QC = False\n",
    "parameter: bayesian_conditional_analysis = False\n",
    "depends: sos_variable(\"regional_data\")\n",
    "regions = list(regional_data['regions'].keys())\n",
    "input: for_each = \"regions\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{_regions.replace(\":\", \"_\")}.susie_rss.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(susieR)\n",
    "    library(pecotmr)\n",
    "    library(dplyr)\n",
    "    library(data.table)\n",
    "    skip_region = c(${', '.join(['\"{}\"'.format(item) for item in skip_regions])})\n",
    "    studies = c(${', '.join(['\"{}\"'.format(item) for item in regional_data[\"GWAS\"].keys()])})\n",
    "    sumstat_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][0] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    column_file_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][1] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_samples = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][2] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_cases = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][3] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_controls = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][4] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    LD_data = load_LD_matrix(\"${ld_meta_data}\", '${\"chr%s:%s-%s\" % (regional_data['regions'][_regions][0], regional_data['regions'][_regions][1], regional_data['regions'][_regions][2])}')\n",
    "    run_rss_pipeline <- function(sumstat_path, column_file_path, LD_data, n_sample, n_case, n_control, skip_region) {\n",
    "      rss_input = get_rss_input(sumstat_path = sumstat_path, column_file_path = column_file_path, n_sample = n_sample, n_case = n_case, n_control = n_control)\n",
    "        sumstats = rss_input$sumstats\n",
    "        n = rss_input$n\n",
    "        var_y = rss_input$var_y \n",
    "        input_processed = rss_input_preprocess(sumstats = sumstats, LD_data = LD_data, skip_region = skip_region)\n",
    "        L = ${L} \n",
    "        final_result = susie_rss_pipeline(input_processed, R = LD_data$combined_LD_matrix, ref_panel = LD_data$ref_panel, n = n, L = L,\n",
    "            var_y = var_y, QC = ${\"TRUE\" if QC else \"FALSE\"},\n",
    "            impute = ${\"TRUE\" if impute else \"FALSE\"},\n",
    "            bayesian_conditional_analysis = ${\"TRUE\" if bayesian_conditional_analysis else \"FALSE\"}, \n",
    "            rcond = ${rcond}, R2_threshold = ${R2_threshold}, minimum_ld = ${minimum_ld}, max_L = ${max_L}, l_step = ${l_step},\n",
    "            coverage = ${coverage[0]}, secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}),  signal_cutoff = ${pip_cutoff})\n",
    "        final_result$sumstats_allele_qc_only = input_processed\n",
    "        return(final_result)\n",
    "    }\n",
    "  \n",
    "    res = setNames(replicate(length(studies), list(), simplify = FALSE), studies)\n",
    "    for (r in 1:length(res)) {\n",
    "        tryCatch({\n",
    "        res[[r]] = run_rss_pipeline(sumstat_path = sumstat_paths[r], column_file_path = column_file_paths[r], \n",
    "                                        LD_data = LD_data, n_sample = as.numeric(n_samples[r]), n_case = as.numeric(n_cases[r]), \n",
    "                                          n_control = as.numeric(n_controls[r]), skip_region = skip_region)\n",
    "        write.table(res[[r]]$sumstats_allele_qc_only, sub(\"studyname\",studies[r],\"${_output:n}.studyname.sumstats_allele_QCed.tsv\"), sep = \"\\t\", col.names=TRUE, row.names=FALSE, quote=FALSE)\n",
    "        if(${\"TRUE\" if QC else \"FALSE\"} & ${\"TRUE\" if impute else \"FALSE\"}){\n",
    "          write.table(res[[r]]$sumstats_qc_impute, sub(\"studyname\",studies[r],\"${_output:n}.studyname.sumstats_QC_imputed.tsv\"), sep = \"\\t\", col.names=TRUE, row.names=FALSE, quote=FALSE)\n",
    "        }\n",
    "      }, error = function(e) {\n",
    "        res[[r]] = NULL\n",
    "        cat(paste(\"Error processing file \", studies[r], \": \", conditionMessage(e), \"\\n\"))}\n",
    "    )}\n",
    "    saveRDS(res, file = \"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb191d-782d-4ace-8a6d-09779653effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[SuSiE_eQTL_RSS_1]\n",
    "parameter: L = 5\n",
    "parameter: max_L = 10\n",
    "# If available the column that indicates sample size within the sumstats\n",
    "# filtering threshold for raiss imputation\n",
    "parameter: rcond = 0.01\n",
    "parameter: R2_threshold = 0.6\n",
    "parameter: l_step = 5\n",
    "parameter: pip_cutoff = 0.025\n",
    "parameter: minimum_ld = 5\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "# Whether to impute the sumstat for all the snp in LD but not in sumstat.\n",
    "parameter: impute = False \n",
    "parameter: QC = False\n",
    "parameter: bayesian_conditional_analysis = False\n",
    "depends: sos_variable(\"regional_data\")\n",
    "regions = list(regional_data['regions'].keys())\n",
    "input: for_each = \"regions\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{_regions.replace(\":\", \"_\")}.susie_rss.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(susieR)\n",
    "    library(pecotmr)\n",
    "    library(dplyr)\n",
    "    library(data.table)\n",
    "    skip_region = c(${', '.join(['\"{}\"'.format(item) for item in skip_regions])})\n",
    "    studies = c(${', '.join(['\"{}\"'.format(item) for item in regional_data[\"GWAS\"].keys()])})\n",
    "    sumstat_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][0] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    column_file_paths = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][1] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_samples = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][2] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_cases = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][3] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    n_controls = c(${paths([regional_data['GWAS'][x][regional_data['regions'][_regions][0]][4] for x in regional_data[\"GWAS\"].keys()]):r,})\n",
    "    LD_data = load_LD_matrix(\"${ld_meta_data}\", '${\"chr%s:%s-%s\" % (regional_data['regions'][_regions][0], regional_data['regions'][_regions][1], regional_data['regions'][_regions][2])}')\n",
    "    run_rss_pipeline <- function(sumstat_path, column_file_path, LD_data, n_sample, n_case, n_control, skip_region) {\n",
    "      rss_input = get_rss_input(sumstat_path = sumstat_path, column_file_path = column_file_path, n_sample = n_sample, n_case = n_case, n_control = n_control)\n",
    "        sumstats = rss_input$sumstats\n",
    "        n = rss_input$n\n",
    "        var_y = rss_input$var_y \n",
    "        input_processed = rss_input_preprocess(sumstats = sumstats, LD_data = LD_data, skip_region = skip_region)\n",
    "        L = ${L} \n",
    "        final_result = susie_rss_pipeline(input_processed, R = LD_data$combined_LD_matrix, ref_panel = LD_data$ref_panel, n = n, L = L,\n",
    "            var_y = var_y, QC = ${\"TRUE\" if QC else \"FALSE\"},\n",
    "            impute = ${\"TRUE\" if impute else \"FALSE\"},\n",
    "            bayesian_conditional_analysis = ${\"TRUE\" if bayesian_conditional_analysis else \"FALSE\"}, \n",
    "            rcond = ${rcond}, R2_threshold = ${R2_threshold}, minimum_ld = ${minimum_ld}, max_L = ${max_L}, l_step = ${l_step},\n",
    "            coverage = ${coverage[0]}, secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}),  signal_cutoff = ${pip_cutoff})\n",
    "        final_result$sumstats_allele_qc_only = input_processed\n",
    "        return(final_result)\n",
    "    }\n",
    "  \n",
    "    res = setNames(replicate(length(studies), list(), simplify = FALSE), studies)\n",
    "    for (r in 1:length(res)) {\n",
    "        tryCatch({\n",
    "        res[[r]] = run_rss_pipeline(sumstat_path = sumstat_paths[r], column_file_path = column_file_paths[r], \n",
    "                                        LD_data = LD_data, n_sample = as.numeric(n_samples[r]), n_case = as.numeric(n_cases[r]), \n",
    "                                          n_control = as.numeric(n_controls[r]), skip_region = skip_region)\n",
    "        write.table(res[[r]]$sumstats_allele_qc_only, sub(\"studyname\",studies[r],\"${_output:n}.studyname.sumstats_allele_QCed.tsv\"), sep = \"\\t\", col.names=TRUE, row.names=FALSE, quote=FALSE)\n",
    "        if(${\"TRUE\" if QC else \"FALSE\"} & ${\"TRUE\" if impute else \"FALSE\"}){\n",
    "          write.table(res[[r]]$sumstats_qc_impute, sub(\"studyname\",studies[r],\"${_output:n}.studyname.sumstats_QC_imputed.tsv\"), sep = \"\\t\", col.names=TRUE, row.names=FALSE, quote=FALSE)\n",
    "        }\n",
    "      }, error = function(e) {\n",
    "        res[[r]] = NULL\n",
    "        cat(paste(\"Error processing file \", studies[r], \": \", conditionMessage(e), \"\\n\"))}\n",
    "    )}\n",
    "    saveRDS(res, file = \"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joined-antarctica",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_RSS_2]\n",
    "output: pip_plot = f\"{cwd}/{_input:bn}.png\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = '12h', mem = '20G', cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand = \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', entrypoint = entrypoint\n",
    "    res = readRDS(${_input:r})\n",
    "    png(${_output[0]:r}, width = 14, height=6, unit='in', res=300)\n",
    "    par(mfrow=c(1,2))\n",
    "    susieR::susie_plot(res, y= \"PIP\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\")\n",
    "    susieR::susie_plot(res, y= \"z\", pos=list(attr='pos',start=res$pos[1],end=res$pos[length(res$pos)]), add_legend=T, xlab=\"position\", ylab=\"-log10(p)\")\n",
    "    dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-newport",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SuSiE_slides_1]\n",
    "input: group_by = 'all'\n",
    "output: analysis_summary = f'{cwd}/{sumstats_path:bnn}.analysis_summary.md', variants_csv = f'{cwd}/{sumstats_path:bnn}.variants.csv'\n",
    "R: container=container, expand = \"${ }\", entrypoint = entrypoint\n",
    "    # Define the theme string\n",
    "    theme <- '---\n",
    "    theme: base-theme\n",
    "    style: |\n",
    "     p {\n",
    "       font-size: 24px;\n",
    "       height: 900px;\n",
    "       margin-top:1cm;\n",
    "      }\n",
    "      img {\n",
    "        height: 70%;\n",
    "        display: block;\n",
    "        margin-left: auto;\n",
    "        margin-right: auto;\n",
    "      }\n",
    "      body {\n",
    "       margin-top: auto;\n",
    "       margin-bottom: auto;\n",
    "       font-family: verdana;\n",
    "      }\n",
    "    ---    \n",
    "    '\n",
    "    text <- \"\"\n",
    "    sep <- '\\n\\n---\\n'\n",
    "\n",
    "    inp <- strsplit(\"${_input:r}\", \" \")[[1]]\n",
    "    inp <- sapply(inp, function(x) paste(head(strsplit(x, \"\\\\.\")[[1]], -1), collapse = \".\"))\n",
    "\n",
    "    r <- unique(strsplit(\"${_input:bn}\", \" \")[[1]])\n",
    "\n",
    "    num_csets <- numeric()\n",
    "    region_info <- character()\n",
    "\n",
    "    variant_info <- list()\n",
    "\n",
    "    for (reg_i in seq_along(unique(inp))) {\n",
    "\n",
    "      rid <- unlist(strsplit(r[reg_i], '\\\\.'))[1]\n",
    "\n",
    "      text_temp <- \"\"\n",
    "      text_temp <- paste0(text_temp, \"#\\n\\n SuSiE RSS \", r[reg_i], \" \\n\")\n",
    "      text_temp <- paste0(text_temp, \"![](\", r[reg_i], \".png)\", sep, \" \\n \\n\")\n",
    "\n",
    "      rd <- readRDS(substr(each, 2, nchar(each)) + \".rds\")\n",
    "\n",
    "      # find the number of cs in the current region\n",
    "      if (is.null(rd$sets$cs)) {\n",
    "        num_csets <- c(num_csets, 0)\n",
    "      } else {\n",
    "        num_csets <- c(num_csets, length(rd$sets$cs))\n",
    "      }\n",
    "      cat(num_csets, \"\\n\")\n",
    "\n",
    "      # this will store the indices of all variants that cross the threshold\n",
    "      ind_p <- which(rd$pip >= ${pip_cutoff})\n",
    "      sumvars <- 0\n",
    "\n",
    "      # if we have at least one cs in the current region\n",
    "      if (num_csets[reg_i] > 0) {\n",
    "        tbl_header <- \"| chr number | pos at highest pip | ref | alt | region id | cs | highest pip |  \\n| --- | --- | --- | --- | --- | --- | --- |  \\n\"\n",
    "\n",
    "        table <- \"\"\n",
    "\n",
    "        sumpips <- 0\n",
    "\n",
    "        for (cset in names(rd$sets$cs)) {\n",
    "          print(cset)\n",
    "\n",
    "          # if we have many variants in the cs\n",
    "          if (length(rd$sets$cs[[cset]]) > 1) {\n",
    "            highestpip <- max(rd$pip[rd$sets$cs[[cset]]])\n",
    "            poswhighestpip <- which.max(rd$pip[rd$sets$cs[[cset]]])\n",
    "\n",
    "            # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "            ind_p <- setdiff(ind_p, rd$sets$cs[[cset]])\n",
    "\n",
    "            # append variant info\n",
    "            i <- poswhighestpip\n",
    "            variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, cset, rd$pip[i])\n",
    "\n",
    "            table <- paste0(table, \"| \", rd$chr[i], \" | \", rd$pos[i], \" | \", rd$ref[i], \" | \", rd$alt[i], \" | \", rid, \" | \", cset, \" | \", sprintf(\"%.2f\", rd$pip[i]), \" |  \\n\")\n",
    "\n",
    "            sumpips <- sumpips + sum(rd$pip[rd$sets$cs[[cset]]])\n",
    "            sumvars <- sumvars + length(rd$sets$cs[[cset]])\n",
    "          } else { # if we have only one variant in the cs\n",
    "            i <- rd$sets$cs[[cset]]\n",
    "\n",
    "            # we make sure that ind_p only stores the variants that aren't in any cs\n",
    "            ind_p <- setdiff(ind_p, i)\n",
    "\n",
    "            # append variant info\n",
    "            variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, cset, rd$pip[i])\n",
    "\n",
    "            table <- paste0(table, \"| \", rd$chr[i], \" | \", rd$pos[i], \" | \", rd$ref[i], \" | \", rd$alt[i], \" | \", rid, \" | \", cset, \" | \", sprintf(\"%.2f\", rd$pip[i]), \" |  \\n\")\n",
    "\n",
    "            sumpips <- sumpips + rd$pip[i]\n",
    "            sumvars <- sumvars + 1\n",
    "          }\n",
    "        }\n",
    "\n",
    "        text_temp <- paste0(text_temp, \"- Total number of variants: \", length(rd$pip), \"\\n\")\n",
    "        text_temp <- paste0(text_temp, \"- Expected number of causal variants: \", sprintf(\"%.2f\", sumpips), \"\\n\")\n",
    "        text_temp <- paste0(text_temp, \"- Number of variants with PIP > \", ${pip_cutoff}, \" and not in any CS: \", length(ind_p), \"\\n\\n\")\n",
    "        text_temp <- paste0(text_temp, tbl_header, table, sep)\n",
    "\n",
    "        if (num_csets[reg_i] > 1) {\n",
    "          text_temp <- paste0(text_temp, \"#### CORR: Correlation between CS | OLAP: Overlap between CS\\n\")\n",
    "\n",
    "          cs <- names(rd$sets$cs)\n",
    "\n",
    "          corrheader <- \"|  |\"\n",
    "          corrbreak <- \"| --- |\"\n",
    "\n",
    "          for (i in cs) {\n",
    "            corrheader <- paste0(corrheader, \" CORR \", i, \" |\")\n",
    "            corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "          }\n",
    "\n",
    "          corrheader <- paste0(corrheader, \"  |\")\n",
    "          corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "\n",
    "          for (i in cs) {\n",
    "            corrheader <- paste0(corrheader, \" OLAP \", i, \" |\")\n",
    "            corrbreak <- paste0(corrbreak, \" --- |\")\n",
    "          }\n",
    "\n",
    "          corrheader <- paste0(corrheader, \"\\n\")\n",
    "          corrbreak <- paste0(corrbreak, \"\\n\")\n",
    "\n",
    "          body <- \"\"\n",
    "\n",
    "          for (en in seq_along(cs)) {\n",
    "            i <- cs[en]\n",
    "            body <- paste0(body, \"| \", i, \" |\")\n",
    "            for (j in rd$cscorr[[en]]) {\n",
    "              body <- paste0(body, \" \", sprintf(\"%.2f\", j), \" |\")\n",
    "            }\n",
    "            body <- paste0(body, \"  |\")\n",
    "            for (j in names(rd$sets$cs)) {\n",
    "              body <- paste0(body, \" \", length(intersect(rd$sets$cs[[i]], rd$sets$cs[[j]])), \" |\")\n",
    "            }\n",
    "            body <- paste0(body, \"\\n\")\n",
    "          }\n",
    "\n",
    "          text_temp <- paste0(text_temp, corrheader, corrbreak, body, sep)\n",
    "        }\n",
    "\n",
    "        region_info <- c(region_info, text_temp)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    f <- file(${_output[\"analysis_summary\"]:r}, \"w\")\n",
    "    writeLines(paste0(theme, text), f)\n",
    "    close(f)\n",
    "\n",
    "    for (i in ind_p) {\n",
    "      # append variant info\n",
    "      variant_info[[length(variant_info) + 1]] <- list(rd$chr[i], rd$pos[i], rd$ref[i], rd$alt[i], rid, \"None\", rd$pip[i])\n",
    "    }\n",
    "\n",
    "    df <- do.call(rbind, variant_info)\n",
    "    colnames(df) <- c(\"chr\", \"pos\", \"ref\", \"alt\", \"rid\", \"cs\", \"pip\")\n",
    "    write.table(df, ${_output[\"variants_csv\"]:r}, sep = \"\\t\", row.names = TRUE, col.names = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-optics",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Generate analysis report: HTML file, and optionally PPTX file\n",
    "[SuSiE_slides_2]\n",
    "output: f\"{_input['analysis_summary']:n}.html\"\n",
    "sh: container=container_marp, expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint = entrypoint\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:a} \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files\n",
    "    node /opt/marp/.cli/marp-cli.js ${_input['analysis_summary']} -o ${_output:an}.pptx \\\n",
    "        --title '${region_file:bnn} fine mapping analysis' \\\n",
    "        --allow-local-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
