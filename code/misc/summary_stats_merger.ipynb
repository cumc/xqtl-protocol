{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "played-nicholas",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Summary statistics merger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710fc526-6783-4005-b1b4-c50077170565",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a30f7f-95ec-4121-b277-a14e4a18ede8",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- 1.To merge multiple summary statistic files to new summary statistic files with common SNPs\n",
    "- 2.To deal with allele flip and reserve issues in the process of merging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae072396-9dd6-4916-b5b7-dd5113955990",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Notes\n",
    " - 1. If there are duplicated `indels` in the summary statistics, they will be removed. For example, two SNPs at 10000 on chr1. one's `A0` is `T`, and `A1` is `TC`. Whereas the other one's `A0` is `TC`, and `A1` is `T`. Both of them will be removed. More about `indels` issues(https://github.com/statgenetics/UKBB_GWAS_dev/issues/81#issuecomment-1015556800).\n",
    " - 2. If duplicated `chr:pos` (GWAS) or `gene:chr:pos` (TWAS) exist, run a recursive match for each pair of them between two summary statistic files (`query`(each of inputs) and `subject` (target file)). \n",
    " - 3. under the same `chr:pos` or `gene:chr:pos`, The variants' `A0` and `A1` are matched by exact, flip, reverse, or flip+reverse models. Only one of them is `True`, the variant in two files are matched. If they are matched by flip or flip+reverse, the sign of `query`'s `STAT` will be inversed. And the `query`'s `A0` and `A1` will be the same as the `subject`'s `A0` and `A1`.       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc489ee3-b68d-4f72-89ff-6dde1f36bbf5",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Pre-requisites\n",
    "\n",
    "Make sure you install the pre-requisited before running this notebook:\n",
    "\n",
    "```\n",
    "pip install cugg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f795fea6-7083-4404-947f-e3b9cb1e7b6c",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "- `--cwd`, the path of working directory\n",
    "- `--yml_path`, the path of yaml file\n",
    "- `--keep-ambiguous`, boolean. default False. if add --keep-ambiguous parameter, keep ambiguous alleles which can not be decided from flip or reverse, such as A/T or C/G. Otherwise, remove them. \n",
    "- `--intersect`, boolean. default False. if add --intersect parameter, output intersect SNPs in all input files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76e3b9b-7ffa-43fb-b369-743c90334d32",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### The format of the input yaml file "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b2f5c9-7e17-41db-8884-83783b87c89a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For GWAS summary statistics: `ID` is `CHR,POS,A0,A1`, which can be used as a unique label for each variant.\n",
    "\n",
    "```\n",
    "INPUT:\n",
    "  - ./data/testflip/*.gz:\n",
    "        ID: CHR,POS,A0,A1\n",
    "        CHR: CHR\n",
    "        POS: POS\n",
    "        A0: REF\n",
    "        A1: ALT\n",
    "        SNP: SNP\n",
    "        STAT: BETA\n",
    "        SE: SE\n",
    "        P: P\n",
    "  - ./data/testflip/flip/snps500_flip.regenie.snp_stats.gz:\n",
    "  \n",
    "TARGET: \n",
    "  - ./data/testflip/snps500.regenie.snp_stats.gz:\n",
    "        ID: CHR,POS,A0,A1\n",
    "        CHR: CHR\n",
    "        POS: POS\n",
    "        A0: REF\n",
    "        A1: ALT\n",
    "        SNP: SNP\n",
    "        STAT: BETA\n",
    "        SE: SE\n",
    "        P: P\n",
    "OUTPUT: data/testflip/output/\n",
    "```\n",
    "\n",
    "For TWAS summary statistics: `ID` is `GENE,CHR,POS,A0,A1`, which add the `GENE` name because a variant can be made association with multiple genes. \n",
    "\n",
    "```\n",
    "INPUT:\n",
    "  - data/twas/*.txt:\n",
    "        ID: GENE,CHR,POS,A0,A1\n",
    "        CHR: chrom\n",
    "        POS: pos\n",
    "        A0: ref\n",
    "        A1: alt\n",
    "        SNP: variant_id\n",
    "        GENE: gene\n",
    "        STAT: beta\n",
    "        SE: se\n",
    "        P: pval\n",
    " \n",
    "  \n",
    "TARGET: \n",
    "  - data/twas/DLPFC.chr6.mol_phe.cis_long_table.reformated.txt:\n",
    "        ID: GENE,CHR,POS,A0,A1\n",
    "        CHR: chrom\n",
    "        POS: pos\n",
    "        A0: ref\n",
    "        A1: alt\n",
    "        SNP: variant_id\n",
    "        GENE: gene\n",
    "        STAT: beta\n",
    "        SE: se\n",
    "        P: pval\n",
    "OUTPUT: ../data/twas/output/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a08c1-c7a4-4272-bc0a-08f34a9da381",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "There are three parts in the input yaml file.\n",
    "- INPUT\n",
    "   - A list of yml file, as the output from yml_generator, each yml file documents a set of input\n",
    "       - the input summary statistic files with the column names in below. \n",
    "       - the input files can be from multiple directory and from different format. The input paths must follow the rules related to Unix shell. the format is to pair the column names with keys (CHR, POS, A0, A1, SNP, STAT, SE, P). if not provided, the column names of the input file will be considered as the default keys.\n",
    "       - The input summary statistic file cannot have duplicated chr:pos\n",
    "       - The input summary statstic file cannot have # in its header\n",
    "       -`ID` in yml is the rule to generate a unique identifier for each SNP, the content of ID shall be a combination of CHR, POS, A0, A1,SNP .etc but not the actual column names. ID can not take existing id columns in the original file.\n",
    "- TARGET\n",
    "   - the target file is a reference summary statistic file or a file with chr, pos, a0, a1 columns at least, which the other files compare with.\n",
    "- OUTPUT\n",
    "   - the path of an output directory for new summary statistic files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c9eb3-55d8-4f32-8cc3-67105cfe4831",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "new summary statistic files with common SNPs in all input files. the sign of statistics has been corrected to make it consistent in different data.\n",
    "   - for each input sumstat file, a qced version will be generated.\n",
    "   - The generated sumstat files will have header as \\\"CHR  ,   POS  ,   A0   ,   A1    ,  SNP   ,  STAT ,   SE    ,  P\\\" regardless of input header\n",
    "   - The generated sumstat files will be in gz format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e130b371-4d06-4b81-af53-40018e26ce9a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Memory usage\n",
    "For merging two sumstat with ~85000 rows and of size of ~5MB, 1 GB of memory is needed \n",
    "\n",
    "For merging two sumstat with ~2000000 rows and of size of ~1 GB, at least 50 GB of memory is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046bfe8a-dcf4-47d8-b4c7-9f582003d222",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab31ad-7374-4231-ae04-a9444c438833",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "sos run ./summary_stats_merger.ipynb --cwd data --yml_list data/yml_list.txt --keep-ambiguous --intersect\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-rainbow",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory where output will be saved to\n",
    "parameter: cwd = path\n",
    "## path to a list of yml file , with columns #chr and dir\n",
    "parameter: yml_list = path\n",
    "import pandas as pd\n",
    "yml_path = pd.read_csv(yml_list,sep = \"\\t\").values.tolist()\n",
    "#if add --keep-ambiguous parameter, keep ambiguous alleles which can not be decided from flip or reverse, such as A/T or C/G. Otherwise, remove them.\n",
    "parameter: keep_ambiguous = False\n",
    "# if add --intersect parameter, output intersect SNPs in all input files.\n",
    "parameter: intersect = False\n",
    "# Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "parameter: numThreads = 1\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Walltime \n",
    "parameter: walltime = '5h'\n",
    "parameter: mem = '3G'\n",
    "# The directory of the output sumstat\n",
    "parameter: sumstat_list = path\n",
    "sumstat_path = pd.read_csv(sumstat_list,sep = \"\\t\").drop(columns=\"#chr\").values.tolist()\n",
    "name = pd.read_csv(sumstat_list,sep = \"\\t\").drop(columns=\"#chr\").columns.values.tolist()\n",
    "## Whether to rename the Chr name.\n",
    "parameter: remame = False\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3003c566-e58f-401d-8e33-5847af8d8241",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88cdb2f7-f129-4e9f-af44-06bf51357c7a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_1 (export utils script)]\n",
    "depends: Py_Module('cugg')\n",
    "output: f'{cwd:a}/utils.py'\n",
    "report: expand = '${ }', output=f'{cwd:a}/utils.py'\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from cugg.sumstat import read_sumstat\n",
    "    from cugg.utils import *\n",
    "    ## To be added in cugg packages\n",
    "    ## Running functions\n",
    "    def unify_sumstat(yml,keep_ambiguous,intersect):\n",
    "        #parse yaml\n",
    "        yml = load_yaml(yml)\n",
    "        input_dict = parse_input(yml['INPUT'])\n",
    "        target_dict = parse_input(yml['TARGET'])\n",
    "        output_path = yml['OUTPUT']\n",
    "        input_dict[list(target_dict.keys())[0]] = list(target_dict.values())[0]\n",
    "        lst_sumstats_file = [ os.path.basename(i) for i in input_dict.keys()]\n",
    "        print('Total number of sumstats: ',len(lst_sumstats_file))\n",
    "        if len(set(lst_sumstats_file))<len(lst_sumstats_file):\n",
    "            raise Exception(\"There are duplicated names in {}\".format(lst_sumstats_file))\n",
    "        #read all sumstats\n",
    "        print(input_dict)\n",
    "        lst_sumstats = {os.path.basename(i):read_sumstat(i,j) for i,j in input_dict.items()}\n",
    "        nqs = []\n",
    "        #Readin the reference target file\n",
    "        subject = pd.read_csv(list(target_dict.keys())[0],sep = \"\\t\")\n",
    "        for query in lst_sumstats.values():\n",
    "            #check duplicated indels and remove them.\n",
    "            query = check_indels(query)\n",
    "            #under the same chr:pos or gene:chr:pos. match A0 and A1 by exact, flip, reverse, or flip+reverse.\n",
    "            #if duplicated chr_pos or gene_chr_pos exist, run a recursive match for each pair of them between query and subject.\n",
    "            nq,_ = snps_match(query,subject,keep_ambiguous)\n",
    "            nqs.append(nq)\n",
    "        if intersect:\n",
    "            #get common snps\n",
    "            common_snps = set.intersection(*[set(nq.SNP) for nq in nqs])\n",
    "            print('Total number of common SNPs: ',len(common_snps))\n",
    "            #write out new sumstats\n",
    "            for output_sumstats,nq,name in zip(lst_sumstats_file,nqs,Theme):\n",
    "                sumstats = nq[nq.SNP.isin(common_snps)]\n",
    "                sumstats.to_csv(os.path.join(output_path, output_sumstats), sep = \"\\t\", header = True, index = False)\n",
    "        else:\n",
    "            for output_sumstats,nq,name in zip(lst_sumstats_file,nqs,Theme):\n",
    "                #output match SNPs with target SNPs.\n",
    "                sumstats,header = ss_2_vcf(nq,name)\n",
    "                sumstats.to_csv(os.path.join(output_path, output_sumstats), sep = \"\\t\", header = True, index = False)\n",
    "        print('All are done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e63b670-435a-44ea-91a4-4a754dd2fe2a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[default_2 (unify sumstats)]\n",
    "depends: f'{cwd:a}/utils.py'\n",
    "input: for_each = \"yml_path\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = '${ }', input = f'{cwd:a}/utils.py', stderr = f'{cwd:a}/output.stderr', stdout = f'{cwd:a}/output.stdout'\n",
    "    yml = \"${_yml_path[1]}\"\n",
    "    keep_ambiguous = ${keep_ambiguous}\n",
    "    intersect = ${intersect}\n",
    "    print(yml, keep_ambiguous,intersect)\n",
    "    unify_sumstat(yml, keep_ambiguous,intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d06c550-fe3b-4b4d-9c37-9c62d1f781eb",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sumstat_to_vcf_1]\n",
    "input: for_each = \"sumstat_path\"\n",
    "output: [f'{path(x):an}.vcf' for x in _sumstat_path]\n",
    "python: expand = '${ }', stderr = f'{cwd:a}/output.stderr', stdout = f'{cwd:a}/output.stdout'\n",
    "    from cugg.sumstat import ss_2_vcf\n",
    "    import pandas as pd\n",
    "    def ss_2_vcf(ss_df,name = \"name\"):\n",
    "        ## Geno field\n",
    "        df = pd.DataFrame()\n",
    "        df[['#CHROM', 'POS', 'ID', 'REF', 'ALT']] = ss_df[['CHR', 'POS', 'SNP', 'A0', 'A1']].sort_values(['CHR', 'POS'])\n",
    "        ## Info field(Empty)\n",
    "        df['QUAL'] = \".\"\n",
    "        df['FILTER'] = \"PASS\"\n",
    "        df['INFO'] = \".\"\n",
    "        fix_header = [\"SNP\",\"A1\",\"A0\",\"POS\",\"CHR\",\"STAT\",\"SE\",\"P\"]\n",
    "        header_list = []\n",
    "        if \"GENE\" in ss_df.columns:\n",
    "            df['INFO'] = \"GENE = \" + ss_df[\"GENE\"]\n",
    "            fix_header = [\"GENE\",\"SNP\",\"A1\",\"A0\",\"POS\",\"CHR\",\"STAT\",\"SE\",\"P\"]\n",
    "            header_list = ['##INFO=<ID=GENE,Number=A,Type=String,Description=\"The name of genes']\n",
    "        ### Fix headers\n",
    "        import time\n",
    "        header = '##fileformat=VCFv4.2\\n' + \\\n",
    "        '##FILTER=<ID=PASS,Description=\"All filters passed\">\\n' + \\\n",
    "        f'##fileDate={time.strftime(\"%Y%m%d\",time.localtime())}\\n'+ \\\n",
    "        '##FORMAT=<ID=STAT,Number=A,Type=Float,Description=\"Effect size estimate relative to the alternative allele\">\\n' + \\\n",
    "        '##FORMAT=<ID=SE,Number=A,Type=Float,Description=\"Standard error of effect size estimate\">\\n' + \\\n",
    "        '##FORMAT=<ID=P,Number=A,Type=Float,Description=\"The Pvalue corresponding to ES\">\\n' \n",
    "        ### Customized Field headers\n",
    "        for x in ss_df.columns:\n",
    "            if x not in fix_header:\n",
    "                Prefix = f'##FORMAT=<ID={x},Number=A,Type='\n",
    "                Type = str(type(test[x][0])).replace(\"<class \\'\",\"\").replace(\"'>\",\"\").replace(\"numpy.\",\"\").replace(\"64\",\"\").capitalize()\n",
    "                Surfix = f',Description=\"Customized Field {x}'\n",
    "                header_list.append(Prefix+Type+Surfix)\n",
    "        ## format and sample field\n",
    "        df['FORMAT'] = \":\".join([\"STAT\",\"SE\",\"P\"] + ss_df.drop(fix_header,axis = 1).columns.values.tolist())\n",
    "        df[f'{name}'] = ss_df['STAT'].astype(str) + \":\" + ss_df['SE'].astype(str) + \":\" + ss_df['P'].astype(str) + ss_df.drop(fix_header,axis = 1).astype(str).apply(\":\".join,axis = 1)\n",
    "        ## Rearrangment\n",
    "        df = df[['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO','FORMAT',f'{name}']]\n",
    "        # Add headers\n",
    "        header = header + \"\\n\".join(header_list)\n",
    "    return df,header\n",
    "\n",
    "    sumstat_path_list = ${_sumstat_path}\n",
    "    name = ${name}\n",
    "    for x,y in zip(sumstat_path_list,name):\n",
    "        sumstats = pd.read_csv(x,\"\\t\")\n",
    "        sumstats,header = ss_2_vcf(sumstats,y)\n",
    "    with open(f'{path(x):an}.vcf', 'w') as f:\n",
    "        f.write(header)\n",
    "    sumstats.to_csv(f'{path(x):an}.vcf', sep = \"\\t\", header = True, index = False,method = \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330cf48e-dc4d-4299-9999-eabfde81e77f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sumstat_to_vcf_2]\n",
    "output: f'{cwd}/{_input[0]:bn}.merged.vcf.gz'\n",
    "bash: expand = '${ }', stderr = f'{cwd:a}/output.stderr', stdout = f'{cwd:a}/output.stdout'\n",
    "    for i in ${_input:r}; do\n",
    "    bgzip -f $i \n",
    "    tabix -p vcf -f  $i.gz; done\n",
    "    bcftools merge ${_input:r} --force-samples  -Oz -o ${_output:a}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
