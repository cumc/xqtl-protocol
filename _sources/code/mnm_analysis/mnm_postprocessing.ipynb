{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legislative-cylinder",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Integrative Analysis Output Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-robertson",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook is to post-process the susie results into different text file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-jurisdiction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Extracting susie results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-routine",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_to_tsv \\\n",
    "    --cwd output/test --rds_path `ls output/test/cache/*rds | head ` --region-list <(head -50  ./dlpfc_region_list) --container containers/stephenslab.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-speed",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Extracting susie_rss results for ADGWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-dayton",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_to_tsv \\\n",
    "    --cwd output/ADGWAS_finemapping_extracted/Bellenguez/ --rds_path `ls GWAS_Finemapping_Results/Bellenguez/ADGWAS2022*rds ` \\\n",
    "    --region-list ~/1300_hg38_EUR_LD_blocks_orig.tsv \\\n",
    "    --container containers/stephenslab.sif \n",
    "\n",
    "sos run pipeline/SuSiE_post_processing.ipynb susie_tsv_collapse \\\n",
    "    --cwd output/ADGWAS_finemapping_extracted --tsv_path `ls output/ADGWAS_finemapping_extracted/*lbf.tsv` \\\n",
    "    --container containers/stephenslab.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-index",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Extracting fsusie results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-broadcast",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_haQTL_pos --rds_path `ls output/f_susie_tad_haQTL_pos/cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-remark",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos_selected/ --rds_path `ls output/f_susie_tad_meQTL_pos_selected//cache/*1204*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-embassy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos_2/ --rds_path `ls output/f_susie_tad_meQTL_pos_2//cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-rwanda",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos/ --rds_path `ls output/f_susie_tad_meQTL_pos//cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-retailer",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_haQTL_pos_check_pure_2 --rds_path `ls output/f_susie_tad_haQTL_pos_check_pure_2/cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-shepherd",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos_2/ --rds_path `ls output/f_susie_tad_meQTL_pos_2//cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-measure",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Plotting the pip plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-secretary",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_pip_landscape_plot \\\n",
    "    --cwd output/test/ --plot_list plot_recipe --annot_tibble ~/Annotatr_builtin_annotation_tibble.tsv -s force &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-blackjack",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_upsetR_plot \\\n",
    "    --cwd output/test/ --plot_list plot_recipe_1  -s force &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-drilling",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The required input for this step is a tab-delimited plot_recipe file that specifies the path to each of the variant.tsv files generated from this module. Each column represents a molecular phenotype, and each row indicates the files that share common variants. Since one TAD may correspond to multiple genes, additional eQTL are permitted. If there are additional molecular phenotypes or ADGWAS datasets, additional columns can be appended.\n",
    "\n",
    "The built-in Annotatr_builtin_annotation_tibble.tsv can be downloaded from [synapse](https://www.synapse.org/#!Synapse:syn51198526), please download it and specify the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nuclear-chancellor",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haQTL\tmQTL\teQTL\teQTL\tADGWAS\n",
      "/mnt/vast/hpc/csg/molecular_phenotype_calling/QTL_fine_mapping/output/f_susie_tad_meQTL_pos//meQTL.yuqi_mqtl.tad100.uni_Fsusie.mixture_normal_per_scale.variant.tsv\t/mnt/vast/hpc/csg/molecular_phenotype_calling/QTL_fine_mapping/output/f_susie_tad_haQTL_pos//haQTL.rosmap_haqtl.tad100.uni_Fsusie.mixture_normal_per_scale.variant.tsv\t/mnt/vast/hpc/csg/molecular_phenotype_calling/eqtl//output/susie_per_gene_tad//demo.ENSG00000117322.unisusie.fit.variant.tsv\t/mnt/vast/hpc/csg/molecular_phenotype_calling/eqtl//output/susie_per_gene_tad//demo.ENSG00000203710.unisusie.fit.variant.tsv\t/mnt/vast/hpc/csg/xqtl_workflow_testing/susie_rss/output/ADGWAS_finemapping_extracted/Bellenguez/ADGWAS2022.chr1.sumstat.chr1_205972031_208461272.unisusie_rss.fit.variant.tsv\n"
     ]
    }
   ],
   "source": [
    "cat /mnt/vast/hpc/csg/molecular_phenotype_calling/QTL_fine_mapping/plot_recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe78e94-87f7-4de7-b46d-fae572aa1194",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Exporting cis_analysis susie_twas results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c5448-ef4d-4e09-8a55-f87aabf8da1c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "meta file is produced by:\n",
    "```\n",
    "get_condition <- function(conditions, Author, qtl_type){\n",
    "    strings <- c()\n",
    "    for(condition in conditions){\n",
    "        string =  paste(condition, Author, qtl_type, sep = \"_\")  \n",
    "        string = paste(unique(unlist(strsplit(string, \"_\"))), collapse = \"_\")\n",
    "        strings <- c(strings, string)\n",
    "    }\n",
    "    return(strings)\n",
    "}\n",
    "\n",
    "raw_name<- c(\"Mic\",\"Ast\",\"Oli\",\"OPC\",\"Exc\",\"Inh\",\"DLPFC\",\"PCC\",\"AC\")\n",
    "raw_name_kellis<- c(\"Mic_Kellis\",\"Ast_Kellis\",\"Oli_Kellis\",\"OPC_Kellis\",\"Exc_Kellis\",\"Inh_Kellis\",\"Ast.10\",\"Mic.12\",\"Mic.13\")\n",
    "\n",
    "dejager_name <- get_condition(raw_name, \"De_Jager\",\"eQTL\")\n",
    "kellis_name <- get_condition(raw_name_kellis, \"Kellis\",\"eQTL\")\n",
    "eQTL_meta <- data.frame(raw_name = c(raw_name, raw_name_kellis), new_name = c(dejager_name, kellis_name))\n",
    "write_delim(eQTL_meta, \"/mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/eQTL_meta.tsv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd09e57-ee13-4822-a4a7-8a3fc640f999",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_name new_name\n",
      "Mic Mic_De_Jager_eQTL\n",
      "Ast Ast_De_Jager_eQTL\n",
      "Oli Oli_De_Jager_eQTL\n",
      "OPC OPC_De_Jager_eQTL\n",
      "Exc Exc_De_Jager_eQTL\n",
      "Inh Inh_De_Jager_eQTL\n",
      "DLPFC DLPFC_De_Jager_eQTL\n",
      "PCC PCC_De_Jager_eQTL\n",
      "AC AC_De_Jager_eQTL\n"
     ]
    }
   ],
   "source": [
    "head /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/eQTL_meta.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137961b5-1c6c-4c4b-b29a-a27643d5b1d1",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#susie\n",
    "sos run /home/rf2872/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  cis_results_export    \\\n",
    "    --region_file /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/TADB_enhanced_cis.protein_coding.bed   \\\n",
    "    --file_path  /mnt/vast/hpc/homes/rf2872/aws/rds_files   \\\n",
    "    --name demo_susie   \\\n",
    "    --suffix univariate_susie_twas_weights.rds   \\\n",
    "    --prefix  MiGA_eQTL KNIGHT_pQTL  \\\n",
    "    --min_corr 0.8 \\\n",
    "    --geno_ref /mnt/vast/hpc/csg/rf2872/data/Fungen_xqtl/geno_align/Fungen_xQTL.ROSMAP_NIA_WGS.ROSMAP_NIA_WGS.MSBB_WGS_ADSP_hg38.MiGA.MAP_Brain-xQTL_Gwas_geno_0.STARNET.aligned.bim.gz  \\\n",
    "    --context-meta /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/context_meta.tsv  \\\n",
    "    --cwd demo_susie \\\n",
    "    --step1_only #optional to keep cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f9f93-82d8-4093-aff0-4843758a7dd2",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#fsusie\n",
    "sos run /home/rf2872/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  cis_results_export    \\\n",
    "    --region_file /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/extended_TADB.bed  \\\n",
    "    --file_path  /mnt/vast/hpc/homes/rf2872/aws/rds_files     \\\n",
    "    --name demo_fsusie   \\\n",
    "    --suffix fsusie_mixture_normal_top_pc_weights.rds    \\\n",
    "    --prefix  ROSMAP_mQTL ROSMAP_haQTL  \\\n",
    "    --min_corr 0.8  \\\n",
    "    --geno_ref /mnt/vast/hpc/csg/rf2872/data/Fungen_xqtl/geno_align/Fungen_xQTL.ROSMAP_NIA_WGS.ROSMAP_NIA_WGS.MSBB_WGS_ADSP_hg38.MiGA.MAP_Brain-xQTL_Gwas_geno_0.STARNET.aligned.bim.gz  \\\n",
    "    --context-meta /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/context_meta.tsv   \\\n",
    "    --cwd demo_fsusie \\\n",
    "    --fsusie \\\n",
    "    --step1_only #optional to keep cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74eb199a-a490-41af-acf3-1b483f6cba11",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#meta QTL\n",
    "sos run /home/rf2872/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  cis_results_export      \\\n",
    "    --region_file /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/hg38_1362_blocks.bed     \\\n",
    "    --file_path  /mnt/vast/hpc/homes/rf2872/aws/rds_files       \\\n",
    "    --name demo_metaQTL     \\\n",
    "    --suffix univariate_susie_twas_weights.rds        \\\n",
    "    --prefix  ROSMAP_metaQTL  \\\n",
    "    --min_corr 0.8   \\\n",
    "    --geno_ref /mnt/vast/hpc/csg/rf2872/data/Fungen_xqtl/geno_align/Fungen_xQTL.ROSMAP_NIA_WGS.ROSMAP_NIA_WGS.MSBB_WGS_ADSP_hg38.MiGA.MAP_Brain-xQTL_Gwas_geno_0.STARNET.aligned.bim.gz    \\\n",
    "    --context-meta /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/context_meta.tsv    \\\n",
    "    --cwd demo_metaQTL \\\n",
    "    --metaQTL \\\n",
    "    --step1_only #optional to keep cache file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc253a4-eac3-4408-9f63-da73ffed62d0",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# multi gene mvsusie\n",
    "    sos run /data/interactive_analysis/rf2872/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  cis_results_export  \\\n",
    "    --region_file  /data/xqtl-analysis/resource/TADB_sliding_window.bed   \\\n",
    "    --file_path /data/analysis_result/mvsusie_multi_gene_test/multi_gene/    \\\n",
    "    --name ROSMAP_DLPFC_SSU   \\\n",
    "    --suffix mnm.rds   \\\n",
    "    --prefix  ROSMAP_multi_gene \\\n",
    "    --min_corr 0.8 \\\n",
    "    --geno_ref  /data/analysis_result/geno_align/Fungen_xQTL.ROSMAP_NIA_WGS.ROSMAP_NIA_WGS.MSBB_WGS_ADSP_hg38.MiGA.MAP_Brain-xQTL_Gwas_geno_0.STARNET.aligned.bim.gz \\\n",
    "    --cwd demo_multigene \\\n",
    "    --context-meta /data/xqtl-analysis/resource/context_meta.tsv \\\n",
    "    --multi_gene \\\n",
    "    --step1_only #optional to keep cache file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3b184-552a-4ca0-b11d-0e56516ed6ff",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Export gwas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6b547-b86a-4014-9377-07bbad828d62",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    " sos run /home/rf2872/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  gwas_results_export      \\\n",
    "    --region_file /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/hg38_1362_blocks.bed     \\\n",
    "    --file_path  /home/hs3393/RSS_QC/GWAS_finemapping_Apr9/univariate_rss      \\\n",
    "    --name demo_gwas   \\\n",
    "    --suffix  univariate_susie_rss.rds       \\\n",
    "    --prefix  RSS_QC_RAISS_imputed   \\\n",
    "    --min_corr 0.8    \\\n",
    "    --geno_ref /mnt/vast/hpc/csg/rf2872/data/Fungen_xqtl/geno_align/Fungen_xQTL.ROSMAP_NIA_WGS.ROSMAP_NIA_WGS.MSBB_WGS_ADSP_hg38.MiGA.MAP_Brain-xQTL_Gwas_geno_0.STARNET.aligned.bim.gz    \\\n",
    "    --context-meta /mnt/vast/hpc/homes/rf2872/codes/xqtl-analysis/resource/context_meta.tsv    \\\n",
    "    --cwd demo_gwas \\\n",
    "    --gwas \\\n",
    "    --region-name chr9_19882538_22992379  \\#optional \n",
    "    --step1_only #optional "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab5c55-2668-4a99-8ad8-1d28c33a5ca8",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### combine seperate meta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa6916a-6f8e-458b-aca0-8a33fe708a5b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run /home/rf2872/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb   combine_export_meta \\\n",
    "   --cache_path demo_susie_back/demo_susie_cache \\\n",
    "   --output_file test_combine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bcc90-94ef-4d9b-b3eb-57abaacfd5c9",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Overlapped gwas data and eQTL data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efd99d-cd04-4f8d-9d2b-418b67df5d3e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run /home/rf2872/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb   overlap_qtl_gwas \\\n",
    "    --name demo_overlap \\\n",
    "    --qtl_meta_path /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/demo_susie/demo_susie.cis_results_db.tsv \\\n",
    "    --gwas_meta_path /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/demo_gwas/demo_gwas.block_results_db.tsv \\\n",
    "    --cwd demo_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-fields",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import glob\n",
    "import pandas as pd\n",
    "# A region list file documenting the chr_pos_ref_alt of a susie_object\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: name = \"demo\"\n",
    "\n",
    "## Path to work directory where output locates\n",
    "## Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 50\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"96h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"6G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 2\n",
    "parameter: windows = 1000000\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "genetic-campbell",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_to_tsv_1]\n",
    "# Input\n",
    "# For complete susie, region_list or tad_list, for susie_rss , LD region list \n",
    "parameter: region_list = path\n",
    "region_tbl = pd.read_csv(region_list,sep = \"\\t\")\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "output: f\"{cwd}/{_input:bn}.variant.tsv\",f\"{cwd}/{_input:bn}.lbf.tsv\",f\"{cwd}/{_input:bn}.effect.tsv\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"susieR\")\n",
    "    extract_lbf = function(susie_obj){\n",
    "    \n",
    "    if(\"variants\" %in% names(susie_obj) ){\n",
    "    ss_bf = tibble(snps = susie_obj$variants, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    }\n",
    "      else \n",
    "      {\n",
    "    ss_bf = tibble(snps = susie_obj$variable_name, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${_input:bnnn}\")\n",
    "     }\n",
    "    \n",
    "    ss_bf = ss_bf%>%cbind(susie_obj$lbf_variable%>%t)%>%as_tibble()\n",
    "    \n",
    "    return(ss_bf)\n",
    "    }\n",
    "    \n",
    "    extract_variants_pip = function(susie_obj,region_list){\n",
    "    susie_tb = tibble( variants =  names(susie_obj$pip)[which( susie_obj$pip >= 0)],\n",
    "                           snps_index = which(( susie_obj$pip >= 0))) %>%\n",
    "        mutate(chromosome = map_chr(variants, ~read.table(text = .x, sep = \":\")$V1%>%str_replace(\"chr\",\"\") ),\n",
    "                position  = map_chr(variants, ~read.table(text = .x, sep = \":\")$V2  ),\n",
    "                ref = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V2  ),\n",
    "                alt = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V3  ),\n",
    "                position  = map_dbl(position , ~read.table(text = .x, sep = \"_\",as.is = T)$V1  )\n",
    "                             )\n",
    "      susie_tb = susie_tb%>%mutate(cs_order =(map(susie_tb$snps_index , ~tryCatch(which(pmap(list( a= susie_obj$sets$cs) , function(a) .x %in% a )%>%unlist()), error = function(e) return(0) )  ))%>%as.character%>%str_replace(\"integer\\\\(0\\\\)\",\"0\"),\n",
    "                         cs_id = map_chr(cs_order,~ifelse(.x ==\"0\", \"None\" ,names(susie_obj$sets$cs)[.x%>%str_split(\":\")%>%unlist%>%as.numeric] ) ),\n",
    "                         log10_base_factor = map_chr(snps_index,~paste0( susie_obj$lbf_variable[,.x],  collapse = \";\")),\n",
    "                         pip = susie_obj$pip,\n",
    "                         posterior_mean = coef.susie(susie_obj)[-1],\n",
    "                         posterior_sd = susie_get_posterior_sd(susie_obj),\n",
    "                         z = posterior_mean/posterior_sd)\n",
    "    \n",
    "          susie_tb =  susie_tb%>%mutate(  molecular_trait_id = region_list$molecular_trait_id,\n",
    "                             finemapped_region_start = region_list$finemapped_region_start,\n",
    "                             finemapped_region_end = region_list$finemapped_region_end)\n",
    "          return(susie_tb)    }\n",
    "          \n",
    "        \n",
    "\n",
    "     extract_effect_pip = function(susie_obj,region_list,susie_tb){\n",
    "      result_tb =  tibble(phenotype = susie_obj$name,\n",
    "        V = susie_obj$V,effect_id = paste0(\"L\",1:length(V) ) ,\n",
    "        cs_log10bf = susie_obj$lbf)\n",
    "        if(is.null(susie_obj$sets$cs)){\n",
    "            cs_min_r2 = cs_avg_r2 =  coverage =  0 \n",
    "            cs = \"None\"} else {         cs = map_chr(susie_obj$sets$cs[result_tb$effect_id],~susie_tb$variants[.x]%>%paste0(collapse = \";\"))\n",
    "        coverage = map(result_tb$effect_id, ~susie_obj$sets$coverage[which(names(susie_obj$sets$cs) == .x )])%>%as.numeric%>%replace_na(0)\n",
    "        cs_min_r2  = (susie_obj$sets$purity[result_tb$effect_id,1])%>%as.numeric%>%replace_na(0)  \n",
    "        cs_avg_r2  = (susie_obj$sets$purity[result_tb$effect_id,2])%>%as.numeric%>%replace_na(0) }\n",
    "        result_tb = result_tb%>%mutate(cs_min_r2 = cs_min_r2,cs_avg_r2 = cs_avg_r2 ,coverage = coverage%>%unlist,cs = cs )            \n",
    "      return(result_tb)\n",
    "      }\n",
    "       \n",
    "  \n",
    "    susie_obj = readRDS(\"${_input:a}\")\n",
    "    if(\"variants\" %in% names(susie_obj) ){susie_obj$variants = susie_obj$variants%>%str_replace(\"_\",\":\")}\n",
    "    if(is.null(names(susie_obj$pip ))){names(susie_obj$pip) = susie_obj$variants}\n",
    "    lbf = extract_lbf(susie_obj)\n",
    "    region_list = read_delim(\"${region_list}\",\"\\t\")\n",
    "    if(ncol(region_list) == 3 ){   region_list =  region_list%>%mutate(`#chr` = `#chr`%>%str_remove_all(\" \") , ID = paste0(`#chr`,\"_\",start,\"_\",end) ) } # LD_list \n",
    "    if(region_list$start[1] - region_list$end[1]  == -1 ){ \n",
    "        region_list = region_list%>%mutate( start = start - ${windows} ,end = start +${windows}) # region_list for fix cis windows  \n",
    "          } \n",
    "      if(\"gene_id\" %in% colnames(region_list)){region_list = region_list%>%mutate(ID = gene_id)  } # region_list for gene\n",
    "    region_list = region_list%>%select(molecular_trait_id = ID, chromosome  = `#chr`,finemapped_region_start = start ,finemapped_region_end = end)  # Formatting\n",
    "    region_list = region_list%>%filter(molecular_trait_id == \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    variants_pip = extract_variants_pip( susie_obj , region_list)\n",
    "    effect_pip = extract_effect_pip( susie_obj , region_list,variants_pip)\n",
    "    lbf%>%write_delim(\"${_output[1]}\",\"\\t\")\n",
    "    variants_pip%>%write_delim(\"${_output[0]}\",\"\\t\")\n",
    "    effect_pip%>%write_delim(\"${_output[2]}\",\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-florist",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sQTL_susie_to_tsv_1]\n",
    "# Input\n",
    "# For complete susie, region_list or tad_list, for susie_rss , LD region list \n",
    "parameter: region_list = path\n",
    "region_tbl = pd.read_csv(region_list,sep = \"\\t\")\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "input_name=f\"{_input:bn}\"\n",
    "input_name=input_name.replace('*', 'N') # \"*\" in leafcutter2 would be ignored in shell and cause error \n",
    "output: f\"{cwd}/{input_name}.variant.tsv\",f\"{cwd}/{input_name}.lbf.tsv\",f\"{cwd}/{input_name}.effect.tsv\"\n",
    "tags = f'{step_name}_{_output[0]:bn}'\n",
    "tags = tags.replace(':', '_').replace('+', 'ps') # also for other symbols in tag id \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = tags\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"susieR\")\n",
    "    extract_lbf = function(susie_obj){\n",
    "    \n",
    "    if(\"variants\" %in% names(susie_obj) ){\n",
    "    ss_bf = tibble(snps = susie_obj$variants, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    }\n",
    "      else \n",
    "      {\n",
    "    ss_bf = tibble(snps = susie_obj$variable_name, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${_input:bnnn}\")\n",
    "     }\n",
    "    \n",
    "    ss_bf = ss_bf%>%cbind(susie_obj$lbf_variable%>%t)%>%as_tibble()\n",
    "    \n",
    "    return(ss_bf)\n",
    "    }\n",
    "    \n",
    "    extract_variants_pip = function(susie_obj,region_list){\n",
    "    susie_tb = tibble( variants =  names(susie_obj$pip)[which( susie_obj$pip >= 0)],\n",
    "                           snps_index = which(( susie_obj$pip >= 0))) %>%\n",
    "        mutate(chromosome = map_chr(variants, ~read.table(text = .x, sep = \":\")$V1%>%str_replace(\"chr\",\"\") ),\n",
    "                position  = map_chr(variants, ~read.table(text = .x, sep = \":\")$V2  ),\n",
    "                ref = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V2  ),\n",
    "                alt = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V3  ),\n",
    "                position  = map_dbl(position , ~read.table(text = .x, sep = \"_\",as.is = T)$V1  )\n",
    "                             )\n",
    "      susie_tb = susie_tb%>%mutate(cs_order =(map(susie_tb$snps_index , ~tryCatch(which(pmap(list( a= susie_obj$sets$cs) , function(a) .x %in% a )%>%unlist()), error = function(e) return(0) )  ))%>%as.character%>%str_replace(\"integer\\\\(0\\\\)\",\"0\"),\n",
    "                         cs_id = map_chr(cs_order,~ifelse(.x ==\"0\", \"None\" ,names(susie_obj$sets$cs)[.x%>%str_split(\":\")%>%unlist%>%as.numeric] ) ),\n",
    "                         log10_base_factor = map_chr(snps_index,~paste0( susie_obj$lbf_variable[,.x],  collapse = \";\")),\n",
    "                         pip = susie_obj$pip,\n",
    "                         posterior_mean = coef.susie(susie_obj)[-1],\n",
    "                         posterior_sd = susie_get_posterior_sd(susie_obj),\n",
    "                         z = posterior_mean/posterior_sd)\n",
    "    \n",
    "          susie_tb =  susie_tb%>%mutate(  molecular_trait_id = region_list$molecular_trait_id,\n",
    "                             finemapped_region_start = region_list$finemapped_region_start,\n",
    "                             finemapped_region_end = region_list$finemapped_region_end)\n",
    "          return(susie_tb)    }\n",
    "          \n",
    "        \n",
    "\n",
    "     extract_effect_pip = function(susie_obj,region_list,susie_tb){\n",
    "      result_tb =  tibble(phenotype = susie_obj$name,\n",
    "        V = susie_obj$V,effect_id = paste0(\"L\",1:length(V) ) ,\n",
    "        cs_log10bf = susie_obj$lbf)\n",
    "        if(is.null(susie_obj$sets$cs)){\n",
    "            cs_min_r2 = cs_avg_r2 =  coverage =  0 \n",
    "            cs = \"None\"} else {         cs = map_chr(susie_obj$sets$cs[result_tb$effect_id],~susie_tb$variants[.x]%>%paste0(collapse = \";\"))\n",
    "        coverage = map(result_tb$effect_id, ~susie_obj$sets$coverage[which(names(susie_obj$sets$cs) == .x )])%>%as.numeric%>%replace_na(0)\n",
    "        cs_min_r2  = (susie_obj$sets$purity[result_tb$effect_id,1])%>%as.numeric%>%replace_na(0)  \n",
    "        cs_avg_r2  = (susie_obj$sets$purity[result_tb$effect_id,2])%>%as.numeric%>%replace_na(0) }\n",
    "        result_tb = result_tb%>%mutate(cs_min_r2 = cs_min_r2,cs_avg_r2 = cs_avg_r2 ,coverage = coverage%>%unlist,cs = cs )            \n",
    "      return(result_tb)\n",
    "      }\n",
    "       \n",
    "  \n",
    "    susie_obj = readRDS(\"${_input:a}\")\n",
    "    if(\"variants\" %in% names(susie_obj) ){susie_obj$variants = susie_obj$variants%>%str_replace(\"_\",\":\")}\n",
    "    if(is.null(names(susie_obj$pip ))){names(susie_obj$pip) = susie_obj$variants}\n",
    "    lbf = extract_lbf(susie_obj)\n",
    "    region_list = read_delim(\"${region_list}\",\"\\t\")\n",
    "    if(ncol(region_list) == 3 ){   region_list =  region_list%>%mutate(`#chr` = `#chr`%>%str_remove_all(\" \") , ID = paste0(`#chr`,\"_\",start,\"_\",end) ) } # LD_list \n",
    "    if(region_list$start[1] - region_list$end[1]  == -1 ){ \n",
    "        region_list = region_list%>%mutate( start = start - ${windows} ,end = start +${windows}) # region_list for fix cis windows  \n",
    "          } \n",
    "      if(\"gene_id\" %in% colnames(region_list)){region_list = region_list%>%mutate(ID = gene_id)  } # region_list for gene\n",
    "    region_list = region_list%>%select(molecular_trait_id = ID, chromosome  = `#chr`,finemapped_region_start = start ,finemapped_region_end = end)  # Formatting\n",
    "    mole_id = \"${f'{_input:br}'.split('.')[-4]}\"%>%gsub(\"_N:\",\"_*:\",.)#for sQTL\n",
    "    region_list = region_list%>%filter(molecular_trait_id == mole_id)\n",
    "    variants_pip = extract_variants_pip( susie_obj , region_list)\n",
    "    effect_pip = extract_effect_pip( susie_obj , region_list,variants_pip)\n",
    "    lbf%>%write_delim(\"${_output[1]}\",\"\\t\")\n",
    "    variants_pip%>%write_delim(\"${_output[0]}\",\"\\t\")\n",
    "    effect_pip%>%write_delim(\"${_output[2]}\",\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-wichita",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fsusie_to_tsv_1]\n",
    "# Input\n",
    "# For complete susie, region_list or tad_list, for susie_rss , LD region list \n",
    "parameter: region_list = path\n",
    "region_tbl = pd.read_csv(region_list,sep = \"\\t\")\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "output: f\"{cwd}/{_input:bn}.variant.tsv\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"susieR\")\n",
    "\n",
    "    extract_variants_pip = function(susie_obj,region_list){\n",
    "        susie_tb = tibble( variants =  names(susie_obj$csd_X),\n",
    "                               snps_index = which(( susie_obj$pip >= 0))) %>%\n",
    "            mutate(chromosome = map_chr(variants, ~read.table(text = .x, sep = \":\")$V1%>%str_replace(\"chr\",\"\") ),\n",
    "                    position  = map_chr(variants, ~read.table(text = .x, sep = \":\")$V2  ),\n",
    "                    ref = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V2  ),\n",
    "                    alt = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V3  ),\n",
    "                    position  = map_dbl(position , ~read.table(text = .x, sep = \"_\",as.is = T)$V1  )\n",
    "                                 )\n",
    "          susie_tb = susie_tb%>%mutate(cs_order =(map(susie_tb$snps_index , ~tryCatch(which(pmap(list( a= susie_obj$cs) , function(a) .x %in% a )%>%unlist()), error = function(e) return(0) )  ))%>%as.character%>%str_replace(\"integer\\\\(0\\\\)\",\"0\"),\n",
    "                             pip = susie_obj$pip)\n",
    "          susie_tb =  susie_tb%>%mutate(  molecular_trait_id = region_list$tad_index,\n",
    "                                 finemapped_region_start = region_list$start,\n",
    "                                 finemapped_region_end = region_list$end)\n",
    "          if(\"purity\" %in% names(susie_obj)){\n",
    "              susie_tb = susie_tb%>%mutate(purity = map_dbl(susie_tb$cs_order, ~ifelse(.x%>%as.numeric > 0, susie_obj$purity[[as.numeric(.x)]], NA ) ), is_dummy = as.numeric(purity < 0.5)  )\n",
    "              }\n",
    "    susie_tb = susie_tb%>%mutate(effect_peak_pos = map_dbl(cs_order, ~ifelse(.x%>%as.numeric > 0, susie_obj$outing_grid[which(abs(susie_obj$fitted_func[[as.numeric(.x)]]) == max(abs(susie_obj$fitted_func[[as.numeric(.x)]])))] , NA ) )) \n",
    "    susie_tb_lbf = cbind(susie_tb%>%select(molecular_trait_id,variants,cs_order),Reduce(cbind, susie_obj$lBF)%>%as.tibble%>%`colnames<-`(1:length(susie_obj$lBF)))\n",
    "          return(list(susie_tb, susie_tb_lbf))    }\n",
    "    susie_obj = readRDS(\"${_input:a}\")\n",
    "    region_list = read_delim(\"${region_list}\",\"\\t\")\n",
    "    region_list = region_list%>%filter(tad_index == \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    variants_pip = extract_variants_pip( susie_obj , region_list)[[1]]\n",
    "    variants_lbf = extract_variants_pip( susie_obj , region_list)[[2]]\n",
    "    print(paste0(\"fsusie run time is \", round(susie_obj$runtime[[3]]/60),\"min\"))\n",
    "    variants_pip%>%write_delim(\"${_output}\",\"\\t\")\n",
    "    variants_pip%>%write_delim(\"${_output:nn}.lbf.tsv\",\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-extent",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[*_to_tsv_2]\n",
    "parameter: name = f'{_input[0]:b}'.split(\".\")[0]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{cwd}/{name}.all_variants.tsv\"\n",
    "bash: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    head -1 ${_input[0]} > ${_output}\n",
    "    cat ${_input[0]:d}/*variant.tsv | grep -v cs_order >> ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-festival",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_tsv_collapse]\n",
    "parameter: tsv_path = paths # TSV needs have the name ends with  *.chr1_2_3.unisusie(_rss).lbf.tsv\n",
    "tsv_list  = pd.DataFrame({\"lbf_path\" : [str(x) for x in tsv_path]})\n",
    "chromosome = list(set([f'{x.split(\".\")[-5].split(\"_\")[0].replace(\"chr\",\"\")}'  for x in tsv_list.lbf_path ])) ## Add chr if there is no chr prefix. This is to accomodata chr XY and M\n",
    "input: tsv_path, for_each = \"chromosome\"\n",
    "output: f'{cwd}/{_input[0]:bnnnnnnn}.chr{_chromosome}.unisusie_rss.lbf.tsv'\n",
    "bash: expand = '${ }', stdout = f\"{_output}.stdout\", stderr = f\"{_output}.stderr\", container = container, entrypoint = entrypoint\n",
    "        head -1 ${_input[0]} > ${_output}\n",
    "        cat ${_input[0]:d}/*.chr${_chromosome}_*lbf.tsv | grep -v cs_index >> ${_output}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-praise",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_pip_landscape_plot]\n",
    "parameter: plot_list = path\n",
    "parameter: annot_tibble = path(\"~/Annotatr_builtin_annotation_tibble.tsv\")\n",
    "import pandas as pd\n",
    "plot_list  = pd.read_csv(plot_list,sep = \"\\t\")\n",
    "file_type = plot_list.columns.values.tolist()\n",
    "file_type = [x.split(\".\")[0] for x in file_type ]\n",
    "plot_list = plot_list.to_dict(\"records\")\n",
    "input: plot_list, group_by = len(file_type)\n",
    "output: f'{cwd}/{\"_\".join(file_type)}.{str(_input[0]).split(\".\")[-5]}.pip_landscape_plot.rds',f'{cwd}/{\"_\".join(file_type)}.{str(_input[0]).split(\".\")[-5]}.pip_landscape_plot.pdf'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    color = c(\"black\", \"dodgerblue2\", \"green4\", \"#6A3D9A\", \n",
    "          \"#FF7F00\", \"gold1\", \"skyblue2\", \"#FB9A99\", \"palegreen2\",\n",
    "          \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\",\n",
    "          \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\", \"green1\", \n",
    "          \"yellow4\", \"yellow3\",\"darkorange4\",\"brown\",\"navyblue\",\"#FF0000\",\n",
    "          \"darkgreen\",\"#FFFF00\",\"purple\",\"#00FF00\",\"pink\",\"#0000FF\",\n",
    "          \"orange\",\"#FF00FF\",\"cyan\",\"#00FFFF\",\"#FFFFFF\")\n",
    "    extract_table = function(variant_df,type){ \n",
    "    if(\"purity\" %in% colnames(variant_df) ){\n",
    "      variant_df$purity[is.na(variant_df$purity)] = 0\n",
    "      variant_df[abs(variant_df$purity) < 0.5,7] = 0\n",
    "      }\n",
    "     variant_df = variant_df%>%mutate(CS = (cs_order%>%as.factor%>%as.numeric-1)%>%as.factor)%>%\n",
    "          select( y = pip ,snp = variants,pos = position , CS, molecular_trait_id)%>%mutate(molecular_trait_id = paste0(type,\"_\",molecular_trait_id ) )\n",
    "    return(variant_df)\n",
    "    }\n",
    "    plot_recipe = tibble( type =  c('${\"','\".join(file_type) }'), path = c(${_input:r,}))\n",
    "    plot_list = map2(plot_recipe$type,plot_recipe$path, ~read_delim(.y, guess_max = 10000000)%>%extract_table(.x) )\n",
    "    plot_df = Reduce(rbind,plot_list)\n",
    "    plot_range = (plot_df%>%group_by(molecular_trait_id)%>%summarize(start = (min(pos)), end = (max(pos)))%>%mutate(start = median(start),end = median(end)))[1,c(2,3)]%>%as.matrix\n",
    "    plot_chr = (plot_df$snp[1]%>%stringr::str_split(\":\"))[[1]][1]\n",
    "    plot_df = plot_df%>%mutate(Shared = as.logical(map(snp, ~(plot_df%>%filter( snp ==.x ,   CS%>%as.numeric !=  1 )%>%nrow()) > 1  )))\n",
    "    pip_plot <- plot_df%>%ggplot2::ggplot(aes(y = y, x = pos,\n",
    "                                  col =  CS, shape = Shared )) + facet_grid(molecular_trait_id ~.)+\n",
    "      geom_point(size = 7) +\n",
    "      scale_color_manual(\"CS\",values = color) +\n",
    "      theme(axis.ticks.x = element_blank()) +\n",
    "     ylab(\"Posterior Inclusion Probability (PIP)\")+xlim(plot_range)+\n",
    "      theme(axis.ticks.x = element_blank()) +\n",
    "            theme(strip.text.y.right = element_text(angle = 0))+\n",
    "            xlab(\"\") + \n",
    "            theme(text = element_text(size = 30))+ggtitle(\"Overview of fine-mapping\")\n",
    "  \n",
    "    annot = read_delim(\"${annot_tibble}\")\n",
    "    annot = annot%>%filter(seqnames == plot_chr, start > plot_range[1], end < plot_range[2])\n",
    "    annot_plot   = annot%>%filter(!type%in%c(\"hg38_genes_introns\",\"hg38_genes_1to5kb\"))%>%\n",
    "                        ggplot(aes())+\n",
    "                        geom_segment( aes(x = start,xend = end, y = \"Regulartory Element\", yend = \"Regulartory Element\", color = type ), linewidth =10)+\n",
    "                        ylab(\"\")+xlab(\"\")+xlim(plot_range)+theme(axis.text.x=element_blank(),text = element_text(size = 20))+scale_color_brewer(palette=\"Dark2\")\n",
    "    gene_plot = annot%>%filter(type%in%c(\"hg38_genes_1to5kb\"))%>%group_by(symbol)%>%\n",
    "                            summarise(start = min(start), end = max(end))%>%na.omit%>%\n",
    "                            ggplot(aes())+geom_segment( aes(x = start,xend = end, y = \"Gene\", yend = \"Gene\", color = symbol ), linewidth =10)+\n",
    "                            geom_label(aes(x = (start+end)/2,y = \"Gene\", label = symbol ),size = 5)+ylab(\"\")+xlab(\"POS\")+\n",
    "                            theme(legend.position=\"none\")+theme(text = element_text(size = 20))+xlim(plot_range)\n",
    "      \n",
    "    list(pip_plot,plot_df,annot_plot,gene_plot)%>%saveRDS(\"${_output[0]}\")\n",
    "    cowplot::plot_grid(plotlist = list(pip_plot,annot_plot,gene_plot),ncol = 1, align = \"v\",axis = \"tlbr\",rel_heights = c(8,1,1))%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width = 30, height = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "juvenile-leone",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_upsetR_plot]\n",
    "parameter: plot_list = path\n",
    "import pandas as pd\n",
    "plot_list  = pd.read_csv(plot_list, sep = \"\\t\")\n",
    "file_type = plot_list.columns.values.tolist()\n",
    "file_type = [x.split(\".\")[0] for x in file_type ]\n",
    "plot_list = plot_list.to_dict(\"records\")\n",
    "input: plot_list\n",
    "output: f'{cwd}/{\"_\".join(file_type)}.UpSetR.rds',f'{cwd}/{\"_\".join(file_type)}.UpSetR.pdf'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"UpSetR\")\n",
    "    library(\"ComplexUpset\")\n",
    "    plot_recipe = tibble( type =  c('${\"','\".join(file_type) }'), path = c(${_input:r,}))\n",
    "    plot_list = map2(plot_recipe$type,plot_recipe$path, ~read_delim(.y, guess_max = 10000000)%>%mutate(cs = cs_order != 0 )%>%filter(cs > 0)%>%select(variants,cs)%>%`colnames<-`(c(\"variants\",.x))%>%distinct() )\n",
    "    cs_sharing = Reduce(full_join,plot_list)\n",
    "    cs_upsetR_sharing = cs_sharing\n",
    "    cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]%>%mutate_all(as.numeric)-> cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]\n",
    "    a = upset(cs_upsetR_sharing%>%as.data.frame,intersect = colnames(cs_upsetR_sharing[2:ncol(cs_upsetR_sharing)]),\n",
    "      keep_empty_groups = F,\n",
    "          base_annotations=list(`Intersection size` = intersection_size( bar_number_threshold = 1, position = position_dodge(0.5), width = 0.3 ,text = list(size = 5)   )  ) ,\n",
    "              themes=upset_default_themes(axis.text=element_text(size=30))     ,\n",
    "              min_degree = 1)\n",
    "    \n",
    "    a%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width=18.5, height=10.5)\n",
    "    list(cs_upsetR_sharing)%>%saveRDS(\"${_output[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-prompt",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_upsetR_cs_plot]\n",
    "parameter: plot_list = path\n",
    "import pandas as pd\n",
    "plot_list  = pd.read_csv(plot_list, sep = \"\\t\")\n",
    "file_type = plot_list.columns.values.tolist()\n",
    "file_type = [x.split(\".\")[0] for x in file_type ]\n",
    "plot_list = plot_list.to_dict(\"records\")\n",
    "parameter: trait_to_select =  1 \n",
    "input: plot_list\n",
    "output: f'{cwd}/{\"_\".join(file_type)}.UpSetR_{file_type[trait_to_select-1]}_cs.rds',f'{cwd}/{\"_\".join(file_type)}.UpSetR_{file_type[trait_to_select-1]}_cs.pdf'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"UpSetR\")\n",
    "    library(\"ComplexUpset\")\n",
    "\n",
    "    cs_sharing_identifer = function(upsetR_input,df){\n",
    "    inner_join(upsetR_input, df%>%select(variants,molecular_trait_id, cs_order)%>%filter(cs_order != 0))%>%select(-variants)-> dfL_CS_sharing\n",
    "    dfL_CS_sharing[is.na(dfL_CS_sharing)] = FALSE\n",
    "    dfL_CS_sharing = dfL_CS_sharing%>%group_by(molecular_trait_id,cs_order)%>%summarize(across(everything(), list(mean))   )\n",
    "    dfL_CS_sharing = dfL_CS_sharing%>%mutate(across(colnames(dfL_CS_sharing)[3:ncol(dfL_CS_sharing)], ~.x != 0  ))%>%`colnames<-`(c(\"molecular_trait_id\",\"cs_order\",colnames(cs_sharing)[2:ncol(cs_sharing)]))\n",
    "    }\n",
    "  \n",
    "  \n",
    "    plot_recipe = tibble( type =  c('${\"','\".join(file_type) }'), path = c(${_input:r,}))\n",
    "    plot_list = map2(plot_recipe$type,plot_recipe$path, ~read_delim(.y, guess_max = 10000000)%>%mutate(cs = cs_order != 0 )%>%filter(cs > 0)%>%select(variants,cs)%>%`colnames<-`(c(\"variants\",.x))%>%distinct() )\n",
    "    cs_sharing = Reduce(full_join,plot_list)\n",
    "    cs_upsetR_sharing = cs_sharing\n",
    "    cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]%>%mutate_all(as.numeric)-> cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]\n",
    "\n",
    "    df = read_delim(plot_recipe$path[[${trait_to_select}]]) \n",
    "    \n",
    "    cs_sharing_df = cs_sharing_identifer(cs_upsetR_sharing,df)\n",
    "\n",
    "    a = upset(cs_sharing_df%>%as.data.frame,intersect = colnames(cs_sharing_df[3:ncol(cs_sharing_df)]),\n",
    "          keep_empty_groups = F,\n",
    "          base_annotations=list(`Intersection size` = intersection_size( bar_number_threshold = 1, position = position_dodge(0.5), width = 0.3 ,text = list(size = 8)   )  ),\n",
    "          themes=upset_default_themes(axis.text=element_text(size=30)),\n",
    "          set_size = F  ,  min_degree = 1,wrap = T) + ggtitle( paste0(plot_recipe$type[[${trait_to_select}]],'CS shared with other phenotypes') )   + theme(plot.title = element_text(size = 40, face = \"bold\"))\n",
    "  \n",
    "    a%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width=18.5, height=10.5)\n",
    "    list(cs_sharing_df)%>%saveRDS(\"${_output[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-apartment",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[tmp_annotatation_of_snps_1]\n",
    "parameter: SNP_list = path\n",
    "parameter: annotation_rds = path\n",
    "input: SNP_list\n",
    "output: f'{cwd}/{_input:b}.annotated.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output}.stdout\", stderr = f\"{_output}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"purrr\")\n",
    "    library(\"stringr\")\n",
    "    sharing_snp = readRDS(\"${_input}\")\n",
    "    sharing_snp_fsusie = sharing_snp[[1]]%>%filter(haQTL == 1 | mQTL == 1)\n",
    "    sharing_snp_fsusie = sharing_snp_fsusie%>%mutate(X1 =  read.table(text = sharing_snp_fsusie$variants, sep = \":\")$V1, X2 = read.table(text = read.table(text = sharing_snp_fsusie$variants, sep = \":\")$V2 , sep = \"_\")$V1  )\n",
    "    sharing_snp_fsusie = sharing_snp_fsusie%>%select(variants,chr = X1, pos = X2)\n",
    "    annotation = readRDS(\"${annotation_rds}\")\n",
    "    print(\"data loaded\")\n",
    "    result = sharing_snp_fsusie%>%mutate(annot = map2( chr,pos , ~ annotation%>%filter(X1 == .x, X2 <= .y, X3 >= .y)%>%pull(X5)))%>%mutate(annot = map_chr(annot, ~paste0(.x ,collapse = \",\")) )\n",
    "    print(\"snp annotated\")\n",
    "    result%>%saveRDS(\"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-transport",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[tmp_annotatation_of_snps_2]\n",
    "parameter: SNP_list = path\n",
    "parameter: annotation_rds = path\n",
    "output: f'{cwd}/{_input:b}.annotated_rev.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output}.stdout\", stderr = f\"{_output}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"purrr\")\n",
    "    library(\"stringr\")\n",
    "    result = readRDS(${_input:r})\n",
    "    result_rev = tibble(annot = unique(annotation$X5))%>%mutate(variants = map(annot, ~  result%>%filter( str_detect(annot,.x))%>%pull(variants)) )%>%mutate( variants = map_chr(variants,~paste0(.x ,collapse = \",\"))  )\n",
    "    result_rev%>%saveRDS(\"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-workplace",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fsusie_extract_effect]\n",
    "parameter: rds_path = paths\n",
    "parameter: annot_tibble = path(\"~/Annotatr_builtin_annotation_tibble.tsv\")\n",
    "input: rds_path, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.estimated_effect.tsv',f'{cwd}/{_input:bn}.estimated_effect.pdf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    color = c(\"black\", \"dodgerblue2\", \"green4\", \"#6A3D9A\", \n",
    "          \"#FF7F00\", \"gold1\", \"skyblue2\", \"#FB9A99\", \"palegreen2\",\n",
    "          \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\",\n",
    "          \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\", \"green1\", \n",
    "          \"yellow4\", \"yellow3\",\"darkorange4\",\"brown\",\"navyblue\",\"#FF0000\",\n",
    "          \"darkgreen\",\"#FFFF00\",\"purple\",\"#00FF00\",\"pink\",\"#0000FF\",\n",
    "          \"orange\",\"#FF00FF\",\"cyan\",\"#00FFFF\",\"#FFFFFF\")\n",
    "    \n",
    "    effect_extract = function(fsusie){\n",
    "        plot_df = fsusie$fitted_func%>%as_tibble(.name_repair = \"universal\")%>%mutate(pos =  fsusie$outing_grid)%>%`colnames<-`(c(paste0(\"Effect_\",1:length(fsusie$cs)),\"pos\"))%>%mutate(`#chr` = str_split(names(fsusie$csd_X)[[1]],\":\")[[1]][[1]] )%>%select(`#chr`, pos, everything())\n",
    "        plot=  plot_df%>%pivot_longer(cols = 3:ncol(plot_df),names_to = \"effect\", values_to = \"values\"   ) %>%ggplot(aes(x = pos, y = values,color = effect),linewidth = 7)+\n",
    "        geom_line()+ylab(\"Estimated Effect\") + xlab(\"POS\")+facet_grid(effect~. )+\n",
    "            scale_color_manual(\"Credible set\",values = color[2:length(color)])+geom_line(aes(y = 0), color = \"black\")\n",
    "            theme(strip.text.y.right = element_text(angle = 0))+\n",
    "            xlab(\"\") + \n",
    "            ylab(\"Estimated Effect\")+ \n",
    "            theme(text = element_text(size = 50))+\n",
    "            ggtitle(paste0( \"Estimated effect for ${f'{_input:br}'.split('.')[-4]}\"))\n",
    "        return(list(plot_df,plot))\n",
    "        }\n",
    "    susie_obj =  readRDS(\"${_input}\")\n",
    "    output = effect_extract(susie_obj)\n",
    "    effect_tbl = output[[1]]\n",
    "    annot = read_delim(\"${annot_tibble}\")\n",
    "    annot = annot%>%filter(seqnames == (effect_tbl$`#chr`)[[1]], start > min(effect_tbl$pos), end < max(effect_tbl$pos))\n",
    "    plot_range = c(min(effect_tbl$pos),  max(effect_tbl$pos))\n",
    "    annot_plot   = annot%>%filter(!type%in%c(\"hg38_genes_introns\",\"hg38_genes_1to5kb\"))%>%\n",
    "                        ggplot(aes())+\n",
    "                        geom_segment( aes(x = start,xend = end, y = \"Regulartory Element\", yend = \"Regulartory Element\", color = type ), linewidth =10)+\n",
    "                        ylab(\"\")+xlab(\"\")+xlim(plot_range)+theme(axis.text.x=element_blank(),text = element_text(size = 20))+scale_color_brewer(palette=\"Dark2\")\n",
    "    gene_plot = annot%>%filter(type%in%c(\"hg38_genes_1to5kb\"))%>%group_by(symbol)%>%\n",
    "                            summarise(start = min(start), end = max(end))%>%na.omit%>%\n",
    "                            ggplot(aes())+geom_segment( aes(x = start,xend = end, y = \"Gene\", yend = \"Gene\", color = symbol ), linewidth =10)+\n",
    "                            geom_label(aes(x = (start+end)/2,y = \"Gene\", label = symbol ),size = 5)+ylab(\"\")+xlab(\"POS\")+\n",
    "                            theme(legend.position=\"none\")+theme(text = element_text(size = 20))+xlim(plot_range)\n",
    "    cowplot::plot_grid(plotlist = list(output[[2]]),ncol = 1, align = \"v\",axis = \"tlbr\",rel_heights = c(8))%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width = 30, height = 30)\n",
    "    effect_tbl%>%write_delim(\"${_output[0]}\",\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-disabled",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fsusie_affected_region]\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.affected_region.tsv',f'{cwd}/{_input:bn}.affected_region.pdf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(susiF.alpha)\n",
    "    library(ashr)\n",
    "    library(wavethresh)\n",
    "    color = c(\"black\", \"dodgerblue2\", \"green4\", \"#6A3D9A\", \n",
    "          \"#FF7F00\", \"gold1\", \"skyblue2\", \"#FB9A99\", \"palegreen2\",\n",
    "          \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\",\n",
    "          \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\", \"green1\", \n",
    "          \"yellow4\", \"yellow3\",\"darkorange4\",\"brown\",\"navyblue\",\"#FF0000\",\n",
    "          \"darkgreen\",\"#FFFF00\",\"purple\",\"#00FF00\",\"pink\",\"#0000FF\",\n",
    "          \"orange\",\"#FF00FF\",\"cyan\",\"#00FFFF\",\"#FFFFFF\")\n",
    "    ## Define Function\n",
    "    update_cal_credible_band2 <- function(susiF.obj )\n",
    "    {\n",
    "    \n",
    "    \n",
    "    \n",
    "      if(sum( is.na(unlist(susiF.obj$alpha))))\n",
    "      {\n",
    "        stop(\"Error: some alpha value not updated, please update alpha value first\")\n",
    "      }\n",
    "      temp <- wavethresh::wd(rep(0, susiF.obj$n_wac))\n",
    "    \n",
    "    \n",
    "      for ( l in 1:susiF.obj$L)\n",
    "      {\n",
    "        Smat <-  susiF.obj$fitted_wc2[[l]]\n",
    "        W1   <- ((wavethresh::GenW(n=  ncol(Smat )  , filter.number = 10, family = \"DaubLeAsymm\")))\n",
    "        tt   <- diag( W1%*%diag(c(susiF.obj$alpha[[l]]%*%Smat ))%*% t(W1 ))\n",
    "    \n",
    "        up                       <-  susiF.obj$fitted_func[[l]]+ 3*sqrt(tt)\n",
    "        low                      <-  susiF.obj$fitted_func[[l]]- 3*sqrt(tt)\n",
    "        susiF.obj$cred_band[[l]] <- rbind(up, low)\n",
    "      }\n",
    "    \n",
    "    \n",
    "    \n",
    "      return(susiF.obj)\n",
    "    }\n",
    "    \n",
    "    affected_reg <- function( susiF.obj){\n",
    "      outing_grid <- susiF.obj$outing_grid\n",
    "    \n",
    "      reg <-  list()\n",
    "      h <- 1\n",
    "      for (   l in 1:length(susiF.obj$cs)){\n",
    "    \n",
    "        pos_up <-  which(susiF.obj$cred_band[[l]][1,]<0)\n",
    "        pos_low <- which(susiF.obj$cred_band[[l]][2,]>0)\n",
    "    \n",
    "    \n",
    "        reg_up <- split( pos_up,cumsum(c(1,diff( pos_up)!=1)))\n",
    "    \n",
    "        reg_low <- split( pos_low,cumsum(c(1,diff( pos_low)!=1)))\n",
    "        for( k in 1:length(reg_up)){\n",
    "          reg[[h]] <- c(l, outing_grid[reg_up[[k]][1]], outing_grid[reg_up[[k]][length(reg_up[[k]])]])\n",
    "    \n",
    "          h <- h+1\n",
    "        }\n",
    "        for( k in 1:length(reg_low )){\n",
    "          reg[[h]] <- c(l, outing_grid[reg_low [[k]][1]], outing_grid[reg_low [[k]][length(reg_low [[k]])]])\n",
    "    \n",
    "          h <- h+1\n",
    "        }\n",
    "    \n",
    "    \n",
    "      }\n",
    "      reg <-  do.call(rbind, reg)\n",
    "      colnames(reg) <- c(\"CS\", \"Start\",\"End\")\n",
    "      return(reg)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    susiF_obj =  readRDS(\"${_input}\")\n",
    "    susiF_obj = update_cal_credible_band2(susiF_obj)\n",
    "    affected_tbl = affected_reg(susiF_obj)\n",
    "    affected_tbl = affected_tbl%>%as_tibble%>%mutate(analysis = susiF_obj$name,\n",
    "                          chr = (names(susiF_obj$csd_X)[[1]]%>%stringr::str_split(\":\"))[[1]][[1]],\n",
    "                          molecular_trait_id =  \"${f'{_input:br}'.split('.')[-4]}\", \n",
    "                          purity  = purrr::map_dbl(CS,~susiF_obj$purity[[.x]] )  )\n",
    "    \n",
    "    affected_tbl%>%as_tibble%>%write_delim(\"${_output[0]}\",\"\\t\")\n",
    "    plt = plot_susiF(susiF_obj, cred.band = T)\n",
    "    plt%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width = 30, height = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-bangkok",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Into VCF format\n",
    "\n",
    "FIXME: These codes were moved from earlier workflows. To be cleaned up and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-montana",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#[uni_susie_2]\n",
    "input: group_with = \"genoFile\"\n",
    "output: f\"{_input:n}.vcf.bgz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:nn}.stdout\", stderr = f\"{_output:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "   ## Define create_vcf function\n",
    "           create_vcf = function (chrom, pos, nea, ea, snp = NULL, ea_af = NULL, effect = NULL, \n",
    "        se = NULL, pval = NULL, name = NULL,cs = NULL, pip = NULL) \n",
    "    {\n",
    "        stopifnot(length(chrom) == length(pos))\n",
    "        if (is.null(snp)) {\n",
    "            snp <- paste0(chrom, \":\", pos)\n",
    "        }\n",
    "        snp <- paste0(chrom, \":\", pos)\n",
    "        nsnp <- length(chrom)\n",
    "        gen <- list()\n",
    "        ## Setupt data content for each sample column\n",
    "        if (!is.null(ea_af)) \n",
    "            gen[[\"AF\"]] <- matrix(ea_af, nsnp)\n",
    "        if (!is.null(effect)) \n",
    "            gen[[\"ES\"]] <- matrix(effect, nsnp)\n",
    "        if (!is.null(se)) \n",
    "            gen[[\"SE\"]] <- matrix(se, nsnp)\n",
    "        if (!is.null(pval)) \n",
    "            gen[[\"LP\"]] <- matrix(-log10(pval), nsnp)\n",
    "        if (!is.null(cs)) \n",
    "            gen[[\"CS\"]] <- matrix(cs, nsnp)\n",
    "        if (!is.null(pip)) \n",
    "            gen[[\"PIP\"]] <- matrix(pip, nsnp)\n",
    "        gen <- S4Vectors::SimpleList(gen)\n",
    "        \n",
    "      ## Setup snps info for the fix columns\n",
    "        gr <- GenomicRanges::GRanges(chrom, IRanges::IRanges(start = pos, \n",
    "            end = pos + pmax(nchar(nea), nchar(ea)) - 1, names = snp))\n",
    "         coldata <- S4Vectors::DataFrame(Studies = name, row.names = name)\n",
    "    ## Setup header informations\n",
    "        hdr <- VariantAnnotation::VCFHeader(header = IRanges::DataFrameList(fileformat = S4Vectors::DataFrame(Value = \"VCFv4.2\", \n",
    "            row.names = \"fileformat\")), sample = name)\n",
    "        VariantAnnotation::geno(hdr) <- S4Vectors::DataFrame(Number = c(\"A\", \n",
    "            \"A\", \"A\", \"A\", \"A\", \"A\"), Type = c(\"Float\", \"Float\", \n",
    "            \"Float\", \"Float\", \"Float\", \"Float\"), Description = c(\"Effect size estimate relative to the alternative allele\", \n",
    "            \"Standard error of effect size estimate\", \"-log10 p-value for effect estimate\",  \n",
    "            \"Alternate allele frequency in the association study\",\n",
    "            \"The CS this variate are captured, 0 indicates not in any cs\", \"The posterior inclusion probability to a CS\"), \n",
    "            row.names = c(\"ES\", \"SE\", \"LP\", \"AF\", \"CS\", \"PIP\"))\n",
    "      ## Save only the meta information in the sample columns \n",
    "        VariantAnnotation::geno(hdr) <- subset(VariantAnnotation::geno(hdr), \n",
    "            rownames(VariantAnnotation::geno(hdr)) %in% names(gen))\n",
    "      ## Save VCF \n",
    "        vcf <- VariantAnnotation::VCF(rowRanges = gr, colData = coldata, \n",
    "            exptData = list(header = hdr), geno = gen)\n",
    "        VariantAnnotation::alt(vcf) <- Biostrings::DNAStringSetList(as.list(ea))\n",
    "        VariantAnnotation::ref(vcf) <- Biostrings::DNAStringSet(nea)\n",
    "      ## Add fixed values\n",
    "        VariantAnnotation::fixed(vcf)$FILTER <- \"PASS\"\n",
    "          return(sort(vcf))\n",
    "        }\n",
    "    library(\"susieR\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"stringr\")\n",
    "    \n",
    "    # Get list of cs snps\n",
    "    susie_list = readRDS(${_input:r})\n",
    "    susie_tb_ls = list()\n",
    "    for (i in 1:length(susie_list)){\n",
    "        susie_tb = tibble( snps =  names(susie_list[[1]]$pip)[which( susie_list[[i]]$pip >= 0)], snps_index = which(( susie_list[[i]]$pip >= 0))  )\n",
    "        susie_tb_ls[[i]]= susie_tb%>%mutate( cs = map(snps_index,~which( susie_list[[i]]$sets$cs %in% .x))%>%as.numeric%>%replace_na(0),\n",
    "                                 pip = map_dbl(snps_index,~( susie_list[[i]]$pip[.x])),\n",
    "                                 coef = map_dbl(snps_index,~(coef.susie( susie_list[[i]])[.x+1])))\n",
    "        }\n",
    "    if(length(susie_tb_ls) >= 2){ \n",
    "      for(i in 2:length(susie_tb_ls)){\n",
    "          susie_tb_ls[[i]] = full_join(susie_tb_ls[[i-1]],susie_tb_ls[[i]], by = \"snps\") \n",
    "        }\n",
    "    }\n",
    "    m = c(\"cs\",\"pip\",\"coef\")    \n",
    "    output = list()\n",
    "    for(i in m){\n",
    "    output[[i]] = susie_tb_ls[[length(susie_tb_ls)]]%>%select(contains(i))%>%as.matrix\n",
    "    }\n",
    "    snps_tb = susie_tb_ls[[length(susie_tb_ls)]]%>%mutate(\n",
    "                         chr = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = T)$V1),\n",
    "                         pos_alt_ref = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = TRUE)$V2),\n",
    "                         pos = map_dbl(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE)$V1),\n",
    "                         alt = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V2),\n",
    "                         ref = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V3))\n",
    "    \n",
    "    snps_tb = snps_tb%>%filter(str_detect(ref, \"[ACTG]\") & str_detect(alt, \"[ACTG]\"))\n",
    "    output_vcf = create_vcf(\n",
    "            chrom = snps_tb$chr,\n",
    "             pos = snps_tb$pos,\n",
    "             ea = snps_tb$alt,\n",
    "             nea = snps_tb$ref,\n",
    "             effect = snps_tb%>%select(contains(\"coef\"))%>%as.matrix ,\n",
    "             pip = snps_tb%>%select(contains(\"pip\"))%>%as.matrix,\n",
    "             cs = snps_tb%>%select(contains(\"cs\"))%>%as.matrix,\n",
    "             name = names(susie_list))\n",
    "    VariantAnnotation::writeVcf(output_vcf,${_output:nr},index = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-harvest",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mv_susie_2]\n",
    "input: group_with = \"genoFile\"\n",
    "output: f\"{_input:n}.vcf.bgz\"\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '2h', mem = '55G', cores = 1, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:nn}.stdout\", stderr = f\"{_output:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "   ## Define create_vcf function\n",
    "           create_vcf = function (chrom, pos, nea, ea, snp = NULL, ea_af = NULL, effect = NULL, \n",
    "        se = NULL, pval = NULL, name = NULL,cs = NULL, pip = NULL) \n",
    "    {\n",
    "        stopifnot(length(chrom) == length(pos))\n",
    "        if (is.null(snp)) {\n",
    "            snp <- paste0(chrom, \":\", pos)\n",
    "        }\n",
    "        snp <- paste0(chrom, \":\", pos)\n",
    "        nsnp <- length(chrom)\n",
    "        gen <- list()\n",
    "        ## Setupt data content for each sample column\n",
    "        if (!is.null(ea_af)) \n",
    "            gen[[\"AF\"]] <- matrix(ea_af, nsnp)\n",
    "        if (!is.null(effect)) \n",
    "            gen[[\"ES\"]] <- matrix(effect, nsnp)\n",
    "        if (!is.null(se)) \n",
    "            gen[[\"SE\"]] <- matrix(se, nsnp)\n",
    "        if (!is.null(pval)) \n",
    "            gen[[\"LP\"]] <- matrix(-log10(pval), nsnp)\n",
    "        if (!is.null(cs)) \n",
    "            gen[[\"CS\"]] <- matrix(cs, nsnp)\n",
    "        if (!is.null(pip)) \n",
    "            gen[[\"PIP\"]] <- matrix(pip, nsnp)\n",
    "        gen <- S4Vectors::SimpleList(gen)\n",
    "        \n",
    "      ## Setup snps info for the fix columns\n",
    "        gr <- GenomicRanges::GRanges(chrom, IRanges::IRanges(start = pos, \n",
    "            end = pos + pmax(nchar(nea), nchar(ea)) - 1, names = snp))\n",
    "         coldata <- S4Vectors::DataFrame(Studies = name, row.names = name)\n",
    "    ## Setup header informations\n",
    "        hdr <- VariantAnnotation::VCFHeader(header = IRanges::DataFrameList(fileformat = S4Vectors::DataFrame(Value = \"VCFv4.2\", \n",
    "            row.names = \"fileformat\")), sample = name)\n",
    "        VariantAnnotation::geno(hdr) <- S4Vectors::DataFrame(Number = c(\"A\", \n",
    "            \"A\", \"A\", \"A\", \"A\", \"A\"), Type = c(\"Float\", \"Float\", \n",
    "            \"Float\", \"Float\", \"Float\", \"Float\"), Description = c(\"Effect size estimate relative to the alternative allele\", \n",
    "            \"Standard error of effect size estimate\", \"-log10 p-value for effect estimate\",  \n",
    "            \"Alternate allele frequency in the association study\",\n",
    "            \"The CS this variate are captured, 0 indicates not in any cs\", \"The posterior inclusion probability to a CS\"), \n",
    "            row.names = c(\"ES\", \"SE\", \"LP\", \"AF\", \"CS\", \"PIP\"))\n",
    "      ## Save only the meta information in the sample columns \n",
    "        VariantAnnotation::geno(hdr) <- subset(VariantAnnotation::geno(hdr), \n",
    "            rownames(VariantAnnotation::geno(hdr)) %in% names(gen))\n",
    "      ## Save VCF \n",
    "        vcf <- VariantAnnotation::VCF(rowRanges = gr, colData = coldata, \n",
    "            exptData = list(header = hdr), geno = gen)\n",
    "        VariantAnnotation::alt(vcf) <- Biostrings::DNAStringSetList(as.list(ea))\n",
    "        VariantAnnotation::ref(vcf) <- Biostrings::DNAStringSet(nea)\n",
    "      ## Add fixed values\n",
    "        VariantAnnotation::fixed(vcf)$FILTER <- \"PASS\"\n",
    "          return(sort(vcf))\n",
    "        }\n",
    "    library(\"susieR\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    library(\"tidyr\")\n",
    "    \n",
    "    # Get list of cs snps\n",
    "    res = readRDS(${_input:r})\n",
    "    output_snps = tibble( snps = res$variable_name[which(res$pip >= 0)], snps_index = which((res$pip >= 0))  )\n",
    "    output_snps = output_snps%>%mutate( cs = map(snps_index,~which(res$sets$cs %in% .x))%>%as.numeric%>%replace_na(0),\n",
    "                             pip = map_dbl(snps_index,~(res$pip[.x])),\n",
    "                     chr = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = T)$V1),\n",
    "                     pos_alt_ref = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = TRUE)$V2),\n",
    "                     pos = map_dbl(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE)$V1),\n",
    "                     alt = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V2),\n",
    "                     ref = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V3))\n",
    "    \n",
    "    effect_mtr = res$coef[output_snps$snps_index+1]%>%as.matrix\n",
    "    colnames(effect_mtr) = \"${name}\"\n",
    "    rownames(effect_mtr) = output_snps$snps\n",
    "    cs_mtr = effect_mtr\n",
    "    for(i in 1:nrow(cs_mtr)) cs_mtr[i,] =  output_snps$cs[[i]]  \n",
    "    pip_mtr = effect_mtr\n",
    "    for(i in 1:nrow(pip_mtr)) pip_mtr[i,] =  output_snps$pip[[i]]  \n",
    "    \n",
    "    output_vcf = create_vcf(\n",
    "           chrom = output_snps$chr,\n",
    "            pos = output_snps$pos,\n",
    "            ea = output_snps$alt,\n",
    "            nea = output_snps$ref,\n",
    "            effect = effect_mtr ,\n",
    "            pip = pip_mtr,\n",
    "            cs = cs_mtr,\n",
    "            name = colnames(effect_mtr)\n",
    "              )\n",
    "    VariantAnnotation::writeVcf(output_vcf,${_output:nr},index = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd0a65-ac69-4106-b48c-463b6525a671",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Cis-window analysis Result consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a0d94c-b049-4f71-948e-9520e66ecba9",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Exporting cis susie_twas results\n",
    "[cis_results_export_1, gwas_results_export_1]\n",
    "# per chunk we process at most 200 datasets\n",
    "parameter: per_chunk = 200\n",
    "# Region list should have last column being region name. \n",
    "parameter:region_file=path()\n",
    "# the path stored original output files\n",
    "parameter:file_path=''\n",
    "# assuming orinal files are named as prefix.id.suffix\n",
    "# prefix of the original files (before id) to identify file.\n",
    "parameter:prefix=[]\n",
    "# suffix of the original files (after id) to identify file.\n",
    "parameter:suffix=str\n",
    "# if need to rename context, please load condition meta file\n",
    "parameter: condition_meta = path()\n",
    "# if the pip all variants in a cs < this threshold, then remove this cs\n",
    "parameter: pip_thres = 0.05\n",
    "# only keep top cs_size variants in one cs \n",
    "parameter: cs_size = 3\n",
    "# provide exported meta to filter the exported genes \n",
    "parameter: exported_file = path()\n",
    "# Optional: if a region list is provide the analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "# Otherwise, all regions with both genotype and phenotype files will be analyzed\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "# the (aligned) geno bim file to check allele flipping\n",
    "parameter: geno_ref = path()\n",
    "# context meta file to map renamed context backs\n",
    "parameter: context_meta = path()\n",
    "# default cretria in susie purity filtering, recommended to use 0.8 here \n",
    "parameter: min_corr = 0.5\n",
    "# set this parameter as `True` when exporting gwas data.\n",
    "parameter: gwas = False\n",
    "# set this parameter as `True` when exporting fsusie data.\n",
    "parameter: fsusie = False\n",
    "# set this parameter as `True` when exporting metaQTL data.\n",
    "parameter: metaQTL = False\n",
    "# set this parameter as `True` when exporting multi_gene data.\n",
    "parameter: multi_gene = False\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "region = pd.read_csv(region_file, sep='\\t', names=['chr', 'start', 'end', 'id'])\n",
    "\n",
    "region_ids = []\n",
    "# If region_list is provided, read the file and extract IDs\n",
    "if not region_list.is_dir():\n",
    "    if region_list.is_file():\n",
    "        region_list_df = pd.read_csv(region_list, sep='\\t', header=None, comment = \"#\")\n",
    "        region_ids = region_list_df.iloc[:, -1].unique()  # Extracting the last column for IDs\n",
    "    else:\n",
    "        raise ValueError(\"The region_list path provided is not a file.\")\n",
    "        \n",
    "if len(region_name) > 0:\n",
    "    region_ids = list(set(region_ids).union(set(region_name)))\n",
    "    \n",
    "if len(region_ids) > 0:\n",
    "    region = region[region['id'].isin(region_ids)]\n",
    "\n",
    "# Function to create list of formatted strings for each row\n",
    "def create_formatted_list(row):\n",
    "    if len(prefix) > 0:\n",
    "        formatted_list = [f\"{file_path}/{p}.{row['id']}.{suffix}\" for p in prefix]\n",
    "    else:\n",
    "        formatted_list = [f\"{file_path}/{row['id']}.{suffix}\"]  # GWAS data do not have prefix     \n",
    "    return formatted_list\n",
    "\n",
    "# Apply the function to each row\n",
    "region['original_data'] = region.apply(create_formatted_list, axis=1)\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "def filter_existing_paths(row):\n",
    "    existing_paths = [path for path in row if os.path.exists(path)]\n",
    "    return existing_paths\n",
    "\n",
    "def is_file_exported(paths, results_set):\n",
    "    for path in paths:\n",
    "        basename = os.path.basename(path)\n",
    "        if basename not in results_set:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "region['original_data'] = region['original_data'].apply(filter_existing_paths)\n",
    "region = region[region['original_data'].map(bool)]\n",
    "\n",
    "# if provided exported meta, check if the original data are all exported already, isfo skip them \n",
    "if not exported_file.is_dir():\n",
    "    if exported_file.is_file():\n",
    "        results = pd.read_csv(exported_file, sep='\\t')\n",
    "        results_set = set(results['original_data'])\n",
    "        results_set = {item.strip() for sub in results_set for item in sub.split(',')}\n",
    "        mask = region['original_data'].apply(lambda paths: not is_file_exported(paths, results_set))\n",
    "        region = region[mask]\n",
    "    else:\n",
    "        raise ValueError(\"The exported_file path provided is not a file.\")\n",
    "\n",
    "regional_data = {\n",
    "    'meta': [(row['chr'], row['start'], row['end'], row['id']) for _, row in region.iterrows()],\n",
    "    'data': [(row['original_data']) for _, row in region.iterrows()]\n",
    "}\n",
    "\n",
    "meta_info = regional_data['meta']\n",
    "stop_if(len(regional_data['data']) == 0, f' All files have been exported already')\n",
    "\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f\"{cwd}/{name}_cache/{name}_{_meta_info[3]}.tsv\"\n",
    "task: trunk_workers = job_size, walltime = walltime, trunk_size = job_size, mem = mem, cores = numThreads, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\",stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "    library(data.table)\n",
    "    library(susieR)\n",
    "    library(pecotmr)\n",
    "\n",
    "    # check top_loci existing or not\n",
    "    has_rows <- function(df) {\n",
    "      !is.null(df) && nrow(df) > 0\n",
    "    }\n",
    "\n",
    "    # function to allele flipping checking by mapping them to aligned genotype bim file\n",
    "    align_to_genoref <- function(var_list, geno_ref, region ){\n",
    "        geno_ref <- pecotmr:::tabix_region(file= geno_ref,\n",
    "                    region = region)\n",
    "        colnames(geno_ref) <- c('chr', 'pos', 'alt', 'ref')\n",
    "        geno_ref <- geno_ref %>% mutate(chr = gsub('chr','',chr))\n",
    "        var_list_df <- data.frame(chr = str_split(var_list,\":|_\",simplify = T)[,1] %>% gsub('chr','',.),\n",
    "            pos = str_split(var_list,\":|_\",simplify = T)[,2],\n",
    "            ref = str_split(var_list,\":|_\",simplify = T)[,3],\n",
    "            alt = str_split(var_list,\":|_\",simplify = T)[,4])\n",
    "        # merge_genotype_data from below cell\n",
    "        aligned_var_df <- merge_genotype_data(geno_ref, var_list_df, all=FALSE)\n",
    "        aligned_var <- aligned_var_df %>%\n",
    "          mutate(id = {\n",
    "            if (grepl(\":\", var_list[1])) {\n",
    "              if (grepl(\"_\", var_list[1])) {\n",
    "                paste(chr, paste(pos, ref, alt, sep = \"_\"),sep = ':')\n",
    "              } else {\n",
    "                paste(chr, pos, ref, alt, sep = \":\")\n",
    "              }\n",
    "            } else {\n",
    "              paste(chr, pos, ref, alt, sep = \"_\")\n",
    "            }\n",
    "          }) %>%\n",
    "          pull(id)\n",
    "        if (grepl(\"chr\", var_list[1]))  aligned_var <- paste0(\"chr\",aligned_var)\n",
    "        return(aligned_var)\n",
    "    }\n",
    "    \n",
    "    # function to map variant list to geno ref\n",
    "    merge_genotype_data <- function(df1, df2, all = TRUE) {\n",
    "      setDT(df1)\n",
    "      setDT(df2)\n",
    "      df1[, key := paste(chr, pos, pmin(alt, ref), pmax(alt, ref))]\n",
    "      df2[, key := paste(chr, pos, pmin(alt, ref), pmax(alt, ref))]\n",
    "      df2[df1, on = \"key\", flip := i.alt == ref & i.ref == alt, by = .EACHI]\n",
    "      df2[flip == TRUE, c(\"alt\", \"ref\") := .(ref, alt)]\n",
    "      if (all) {\n",
    "        df_combined <- unique(rbindlist(list(df1[, .(chr, pos, alt, ref)], df2[, .(chr, pos, alt, ref)])), by = c(\"chr\", \"pos\", \"alt\", \"ref\"))\n",
    "      } else {\n",
    "        df_combined <- df2[, .(chr, pos, alt, ref)]\n",
    "      }\n",
    "      return(df_combined)\n",
    "    }\n",
    "\n",
    "    # Function to filter credible sets with all variants PIP < 0.05 and update rows based on _min_corr suffix condition\n",
    "    update_and_filter_cs_ids <- function(dat_con, dat_susie, df) {\n",
    "      # Identify cs_coverage columns\n",
    "      cs_columns <- grep(\"^cs_coverage\", names(df), value = TRUE)\n",
    "    \n",
    "      # Flag rows with any cs_coverage > 0\n",
    "      df$cs_all_non_zero_orig <- rowSums(df[cs_columns] == 0) != length(cs_columns)\n",
    "    \n",
    "      # Iterate over cs_coverage columns and their unique CS IDs\n",
    "      for (cs_column in cs_columns) {\n",
    "        unique_cs_ids <- unique(df[[cs_column]])\n",
    "        # Update top loci based on minimum correlation and attainable coverage for each coverage column\n",
    "        df <- update_top_loci_cs_annotation(dat_con, dat_susie, top_loci_df = df, coverage_value = cs_column)\n",
    "    \n",
    "        for (cs_id in unique_cs_ids[unique_cs_ids > 0]) {\n",
    "          # Flag CS IDs where all PIPs < pip_thres (default 0.05)\n",
    "          pip_check <- df[df[[cs_column]] == cs_id, \"pip\"] < ${pip_thres}\n",
    "          # label the whole cluster if all pip < pip_thres (default 0.05)\n",
    "          if (all(pip_check, na.rm = TRUE)) {\n",
    "            df[[cs_column]] <- ifelse(df[[cs_column]] == cs_id, 0, df[[cs_column]])\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "      # Identify min corr cs_coverage columns\n",
    "      mincor_columns <- grep(\"_min_corr$\", names(df), value = TRUE) # Identify _mincor columns\n",
    "      attainable_columns <- grep(\"_attainable$\", names(df), value = TRUE) # Identify _mincor columns\n",
    "    \n",
    "      # Filter out rows where all cs_coverage columns are 0, all _mincor columns are 0,\n",
    "      # and cs_all_non_zero_orig is TRUE\n",
    "      df <- df %>%\n",
    "        filter(!(cs_all_non_zero_orig &\n",
    "                   rowSums(df[cs_columns] == 0) == length(cs_columns) &\n",
    "                   rowSums(df[mincor_columns] == 0) == length(mincor_columns)&\n",
    "                   rowSums(df[attainable_columns] == 0) == length(attainable_columns))) %>%\n",
    "        select(-cs_all_non_zero_orig) # Remove the temporary flag column\n",
    "      df <- df %>%\n",
    "        select(-starts_with(\"cs_coverage\"), all_of(cs_columns), all_of(mincor_columns), all_of(attainable_columns))\n",
    "      return(df)\n",
    "     }\n",
    "  \n",
    "  \n",
    "     # update top loci table\n",
    "     update_top_loci_cs_annotation <- function(dat_con, dat_susie, top_loci_df, coverage_value, threshold = ${min_corr}) {\n",
    "       # update susie obj based CS\n",
    "       if(coverage_value == 'cs_coverage_0.95') dat_susie_tmp <- dat_susie\n",
    "        else dat_susie_tmp <- dat_susie$sets_secondary[[gsub('cs_','',coverage_value)]]\n",
    "       #get purity res  \n",
    "       purity_res <- dat_susie_tmp$sets$purity \n",
    "       \n",
    "       not_pass_min_cs <- rownames(purity_res)[purity_res$min.abs.corr < threshold] %>%\n",
    "         gsub('L', '', .)\n",
    "       top_loci_df[[paste0( coverage_value, \"_min_corr\")]] <- top_loci_df[[coverage_value]]\n",
    "       if (length(not_pass_min_cs) > 0) {\n",
    "         top_loci_df[[paste0( coverage_value, \"_min_corr\")]][top_loci_df[[coverage_value]] %in% not_pass_min_cs] <- 0\n",
    "       }\n",
    "       # update attainable coverage based CS\n",
    "       extract_last_float <- function(str_val) {\n",
    "           parts <- strsplit(str_val, \"_\")[[1]]   \n",
    "           as.numeric(parts[length(parts)])\n",
    "       }\n",
    "       coverage <- extract_last_float(coverage_value)\n",
    "       # here we always use dat_susie_0.95 to get alpha table \n",
    "       attainable_cs <- susieR:::susie_get_cs_attainable(dat_susie, coverage = coverage)$cs\n",
    "       if(!is.null(attainable_cs)){\n",
    "           cs_info <- pecotmr:::get_cs_info(attainable_cs, 1:length(dat_susie$pip))\n",
    "           column_name <- paste0('cs_coverage_', coverage, '_attainable')\n",
    "           # build a new top loci  table manually based on annatainable res\n",
    "           top_loci_new_df <- data.frame(\n",
    "             variant_id = dat_con$variant_name[which(cs_info > 0)],\n",
    "             maf = NA,\n",
    "             pip = dat_susie$pip[which(cs_info > 0)],\n",
    "             setNames(list( cs_info[which(cs_info > 0)]), column_name)\n",
    "           )\n",
    "         if (${\"FALSE\" if fsusie or gwas else \"TRUE\"}) {\n",
    "            top_loci_new_df <- top_loci_new_df %>% mutate(betahat = dat_con$sumstats$betahat[which(cs_info > 0)], \n",
    "                                      sebetahat = dat_con$sumstats$sebetahat[which(cs_info > 0)])\n",
    "          }\n",
    "         if (${\"TRUE\" if gwas else \"FALSE\"}) { \n",
    "            top_loci_new_df <- top_loci_new_df %>% mutate(z = dat_con$sumstats$z[which(cs_info > 0)], )\n",
    "         }\n",
    "  \n",
    "         top_loci_df <- merge(top_loci_df, top_loci_new_df %>% select(-maf), all =T) %>%\n",
    "                              mutate(across(starts_with(\"cs_coverage\"), ~replace_na(., 0)))\n",
    "       }\n",
    "       return(top_loci_df)\n",
    "     }\n",
    "    \n",
    "    # function to decide run update_and_filter_cs_ids or not\n",
    "    process_top_loci <- function(dat_con, dat_susie) {\n",
    "      data_frame <- dat_con$top_loci\n",
    "      if (has_rows(data_frame)) {\n",
    "        return(update_and_filter_cs_ids(dat_con, dat_susie, data_frame))\n",
    "      } else {\n",
    "        return(data_frame)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # function to get pip with their variant name\n",
    "    get_pip_withname <- function(susie_obj){\n",
    "      pip = susie_obj$susie_result_trimmed$pip\n",
    "      if(!(is.null(pip)))  names(pip) = susie_obj$variant_names\n",
    "      return(pip)\n",
    "    }\n",
    "  \n",
    "    # add column in top loci to indicate if the variant is identified in susie_on_top_pc top loci table or not\n",
    "    annotate_susie <- function(obj, top_loci){\n",
    "        tryCatch({\n",
    "            df <- data.frame()\n",
    "    \n",
    "            results <- lapply(obj[['susie_on_top_pc']], function(x) {\n",
    "              x[['top_loci']]\n",
    "            })\n",
    "            results <- results[!sapply(results, is.null)]\n",
    "                              \n",
    "            df <- bind_rows(results)\n",
    "    \n",
    "            top_loci_others <-  df %>%\n",
    "              filter(rowSums(select(., starts_with('cs_coverage_'))) == 0)\n",
    "            top_loci_cs <- df %>%\n",
    "              filter(rowSums(select(., starts_with('cs_coverage_'))) > 0)\n",
    "    \n",
    "            susie_vars <- rbind(top_loci_others %>% filter(pip >= 0.05), top_loci_cs) %>% pull(variant_id)\n",
    "            if(length(susie_vars) > 0) susie_vars <- align_to_genoref(var_list = susie_vars, geno_ref = geno_ref, region = paste0(gsub(\"chr\",\"\",\"${_meta_info[0]}\"), \":\", \"${_meta_info[1]}\", \"-\", \"${_meta_info[2]}\"))\n",
    "    \n",
    "            susie_vars_cs <- top_loci_cs %>% pull(variant_id)\n",
    "            if(length(susie_vars_cs) > 0)  susie_vars_cs <- align_to_genoref(var_list = susie_vars_cs, geno_ref = geno_ref, region = paste0(gsub(\"chr\",\"\",\"${_meta_info[0]}\"), \":\", \"${_meta_info[1]}\", \"-\", \"${_meta_info[2]}\"))\n",
    "    \n",
    "            top_loci <- top_loci %>% mutate(annotated_susie = ifelse(variant_id %in% susie_vars, 1, 0),\n",
    "                               annotated_susie_cs = ifelse(variant_id %in% susie_vars_cs, 1, 0))\n",
    "    \n",
    "            return(top_loci)\n",
    "        }, error = function(e) {\n",
    "            warning(\"Error in annotate_susie: \", e$message)\n",
    "        })\n",
    "    }\n",
    "    \n",
    "    # function to match context values and add analysis_name\n",
    "    match_contexts <- function(df, context_meta) {\n",
    "      # Split context_meta's context column into separate rows, one for each comma-separated value\n",
    "      context_meta_expanded <- context_meta %>% \n",
    "        separate_rows(context, sep = \",\") %>%\n",
    "        mutate(context = trimws(context)) # Remove potential leading and trailing whitespace\n",
    "\n",
    "      # Loop through each context in df to find the most precise match in context_meta_expanded\n",
    "      df$super_context <- sapply(df$context, function(df_context) {\n",
    "        # Find all potential matches\n",
    "        potential_matches <- context_meta_expanded %>%\n",
    "          filter(str_detect(df_context, context)) %>%\n",
    "          arrange(desc(nchar(context))) # Sort potential matches by descending length of context\n",
    "\n",
    "        # Select the longest match as the most precise one\n",
    "        if(nrow(potential_matches) > 0) {\n",
    "          return(potential_matches$context[1])\n",
    "        } else {\n",
    "          return(NA) # Return NA if no match is found\n",
    "        }\n",
    "      })\n",
    "\n",
    "      return(df)\n",
    "    }\n",
    "    # Define a function to process context data and match contexts\n",
    "    process_and_match_contexts <- function(context_meta_path, cons_top_loci, res_minp) {\n",
    "      # Read context metadata\n",
    "      context_meta <- fread(context_meta_path)\n",
    "\n",
    "      # Extract super contexts\n",
    "      super_contexts <- cons_top_loci[[1]] %>% unlist %>% names %>% as.character\n",
    "      # Create a data frame of context and minimum p-values\n",
    "      context_df <- data.frame(context = super_contexts,\n",
    "                               min_p = res_minp[[1]][super_contexts] %>% unlist %>% as.numeric)\n",
    "      # Match contexts using the previously defined match_contexts function\n",
    "      context_df <- match_contexts(context_df, context_meta)\n",
    "      # Filter context_df by super_context\n",
    "      context_df_no_order <- context_df %>% filter(context == super_context)\n",
    "      context_df_with_order <- context_df %>% filter(context != super_context) \n",
    "\n",
    "      return(list(context_df_no_order= context_df_no_order,context_df_with_order = context_df_with_order ))\n",
    "    }\n",
    "\n",
    "    # Function to filter introns based on leafcutter2 cluster\n",
    "    process_lf2_cluster <- function(df){\n",
    "        df <- df%>%\n",
    "        mutate(cluster_id = str_extract(context, \"clu_\\\\d+\"), \n",
    "               category = str_extract(context, \"([^:]+)(?=:EN)\")) %>%\n",
    "        group_by(cluster_id) %>%\n",
    "        # filter the intron clusters with NE and IN events\n",
    "        mutate(cluster_status = ifelse(any(category %in% c(\"NE\", \"IN\")), \"no\", \"yes\")) %>%\n",
    "        filter(cluster_status == \"yes\") %>%\n",
    "        ungroup %>%\n",
    "        filter(!str_detect(context, \":PR:\")) %>%  #FIXME: only keep unproductive events in leafcutter2 results. \n",
    "        select(-cluster_status, -cluster_id, -category)\n",
    "        return(df)\n",
    "    }\n",
    "\n",
    "    # Function to combine the contexted with and without order \n",
    "    combine_order <- function(context_df_no_order, context_df_with_order){\n",
    "     context_df_with_order <- context_df_with_order %>%\n",
    "        group_by(super_context) %>%\n",
    "        summarise(min_p = min(min_p), .groups = 'keep') %>%\n",
    "        left_join(context_df_with_order, by = c(\"super_context\", \"min_p\" = \"min_p\"))\n",
    "\n",
    "      # Combine context_df_no_order and context_df_with_order contexts\n",
    "      cons_top_loci_minp <- c(context_df_no_order$context, context_df_with_order$context)\n",
    "        return(cons_top_loci_minp)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Process each path and collect results\n",
    "    orig_files = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input])})\n",
    "     # Extract info from each RDS file\n",
    "    results <- list()\n",
    "    gene = \"${_meta_info[3]}\"\n",
    "    geno_ref <- \"${geno_ref}\"\n",
    "\n",
    "    # Post Processing: Extracting info\n",
    "    # use for loop instead of apply to save memory\n",
    "    res <- res_sum <- res_minp <- cons_top_loci <- list()\n",
    "    for(i in seq_along(orig_files)) {\n",
    "      rds_path <- orig_files[i]\n",
    "       dat <- tryCatch({\n",
    "        readRDS(rds_path)\n",
    "      }, error = function(e) {\n",
    "        writeLines(rds_path %>% basename, gsub(\".tsv\",\"_error\",\"${_output}\"))\n",
    "        return(NULL) # \n",
    "      })\n",
    "\n",
    "      if(is.null(dat)) next\n",
    "  \n",
    "      #extract qtl type from susie rds file name, if we have set decent condtiton name, this could be removed \n",
    "      temp_list <- list() # Temporary list to store inner results\n",
    "      if(${\"TRUE\" if multi_gene else \"FALSE\"}) genes <- 'mnm_result' else genes <- names(dat)\n",
    "        for(id in genes){\n",
    "         dat_study <- dat[[id]]\n",
    "          temp_list <- list()\n",
    "          if(${\"TRUE\" if multi_gene else \"FALSE\"}) conditions <- dat_study[['condition_names']] %>% paste(., collapse = '_') else conditions <- names(dat_study)[sapply(dat_study, function(x) length(x) > 0)]\n",
    "  \n",
    "          if(length(conditions) > 0) {\n",
    "  \n",
    "            for(condition in conditions) {\n",
    "                if (${\"FALSE\" if multi_gene else \"TRUE\"}) dat_con <- dat_study[[condition]]${[['fsusie_summary']] if fsusie else ''} else dat_con <- dat_study${[['fsusie_summary']] if fsusie else ''}\n",
    "                if (${\"TRUE\" if gwas else \"FALSE\"}){\n",
    "                    method <- names(dat_study[[condition]])[names(dat_study[[condition]]) != 'rss_data_analyzed'] ## FIXME: this method is not showing in meta (not need if only one method). or we can use `context@method` in meta\n",
    "                    if(length(method) == 1) dat_con <- dat_con[[method]] else stop('more than 1 method, please check.')\n",
    "                }\n",
    "                dat_susie <- dat_con$susie_result_trimmed\n",
    "\n",
    "                # rename context names if needed\n",
    "                if(${\"TRUE\" if condition_meta.is_file() else \"FALSE\"}){\n",
    "                      meta <- suppressMessages(read_delim(\"${condition_meta}\", col_names = F))\n",
    "                      context <- meta %>% filter(X1 == condition) %>% pull(X2)\n",
    "                      if (length(context) == 0) {\n",
    "                        context <- condition\n",
    "                        message(\"No matching entries found. context has been set to the condition value.\")\n",
    "                      }\n",
    "                    } else {context <- condition}\n",
    "                  \n",
    "                # align variants to aligned geno\n",
    "                variant_ids <- c(dat_con$top_loci$variant_id, dat_con$variant_names, dat_con$preset_variants_result$top_loci$variant_id, dat_con$preset_variants_result$variant_names)\n",
    "                unique_variant_ids <- unique(variant_ids)\n",
    "                aligned_variant_ids <- align_to_genoref(unique_variant_ids, geno_ref, paste0(gsub(\"chr\",\"\",\"${_meta_info[0]}\"), \":\", \"${_meta_info[1]}\", \"-\", \"${_meta_info[2]}\"))\n",
    "                names(aligned_variant_ids) <- unique_variant_ids\n",
    "\n",
    "                # change beta or z in top loci and sumstats\n",
    "                top_loci_changed_indexes <- which(dat_con$top_loci$variant_id !=  aligned_variant_ids[dat_con$top_loci$variant_id] %>% as.character )\n",
    "                if(has_rows(dat_con$top_loci) & length(top_loci_changed_indexes) > 0) {\n",
    "                      if (${\"FALSE\" if gwas else \"TRUE\"}) {\n",
    "                          dat_con$top_loci$betahat[top_loci_changed_indexes] <- (-1)* dat_con$top_loci$betahat[top_loci_changed_indexes]\n",
    "                      } else {\n",
    "                          dat_con$top_loci$z[top_loci_changed_indexes] <- (-1)* dat_con$top_loci$z[top_loci_changed_indexes]\n",
    "                      }\n",
    "                }\n",
    "                all_changed_indexes <- which(dat_con$variant_names !=  aligned_variant_ids[dat_con$variant_names] %>% as.character )\n",
    "                if(length(all_changed_indexes) > 0) {\n",
    "                      if (${\"FALSE\" if gwas else \"TRUE\"}) {\n",
    "                          dat_con$sumstats$betahat[all_changed_indexes] <- (-1)* dat_con$sumstats$betahat[all_changed_indexes]\n",
    "                      } else {\n",
    "                           dat_con$sumstats$z[all_changed_indexes] <- (-1)* dat_con$sumstats$z[all_changed_indexes]\n",
    "                      }\n",
    "                }\n",
    "\n",
    "                # change variant names \n",
    "                if(has_rows(dat_con$top_loci)) dat_con$top_loci$variant_id <- aligned_variant_ids[dat_con$top_loci$variant_id] %>% as.character ###\n",
    "                dat_con$variant_names <- aligned_variant_ids[dat_con$variant_names] %>% as.character ###\n",
    "\n",
    "                \n",
    "                if (${\"FALSE\" if gwas else \"TRUE\"}) { \n",
    "                    res[[id]][[context]] <- list(\n",
    "                      top_loci = process_top_loci(dat_con, dat_susie),\n",
    "                      pip = get_pip_withname(dat_con)\n",
    "                    )\n",
    "                    # fsusie saved preset results in different layer\n",
    "                    if (${\"FALSE\" if fsusie else \"TRUE\"}) {\n",
    "                          if(has_rows(dat_con$preset_variants_result$top_loci)) dat_con$preset_variants_result$top_loci$variant_id <- aligned_variant_ids[dat_con$preset_variants_result$top_loci$variant_id] %>% as.character\n",
    "                          dat_con$preset_variants_result$variant_names <- aligned_variant_ids[dat_con$preset_variants_result$variant_names] %>% as.character\n",
    "                          # change beta in preset top loci\n",
    "                          preset_top_loci_changed_indexes <- which(dat_con$preset_variants_result$top_loci$variant_id !=  aligned_variant_ids[dat_con$preset_variants_result$top_loci$variant_id] %>% as.character )\n",
    "                          if(has_rows(dat_con$top_loci) & length(preset_top_loci_changed_indexes) > 0) dat_con$preset_variants_result$top_loci$betahat[preset_top_loci_changed_indexes] <- (-1)* dat_con$preset_variants_result$top_loci$betahat[preset_top_loci_changed_indexes]   \n",
    "\n",
    "                          res[[id]][[context]][['region_info']] = dat_con$region_info\n",
    "                          res[[id]][[context]][['CV_table']] = dat_con$twas_cv_result$performance\n",
    "                          res[[id]][[context]][['preset_top_loci']] = process_top_loci(dat_con$preset_variants_result, dat_con$preset_variants_result$susie_result_trimmed)\n",
    "                          res[[id]][[context]][['preset_pip']] = get_pip_withname(dat_con$preset_variants_result)\n",
    "                    } else {\n",
    "                          res[[id]][[context]][['region_info']] = dat_study[[condition]]$region_info\n",
    "                          res[[id]][[context]][['CV_table']] = dat_study[[condition]]$twas_cv_result$performance\n",
    "                          res[[id]][[context]][['top_loci']] = annotate_susie(dat_study[[condition]], res[[id]][[context]][['top_loci']])\n",
    "                          # the preset data in fsusie is actually from first PC and analyzed by susie, we'd like to remove them to avoid misleading\n",
    "                    } \n",
    "                # fsusie do not have sumstats or p value\n",
    "                if (${\"FALSE\" if fsusie or multi_gene else \"TRUE\"}) {\n",
    "                    res_sum[[id]][[context]] <- list(\n",
    "                      variant_names = dat_con$variant_names,\n",
    "                      sumstats = dat_con$sumstats\n",
    "                    )\n",
    "\n",
    "                    if (${\"FALSE\" if fsusie or multi_gene else \"TRUE\"}) res_minp[[id]][[context]] <- min(pecotmr:::wald_test_pval(dat_con$sumstats$betahat, dat_con$sumstats$sebetahat,  n = 1000)) ##assuming sample size is 1000\n",
    "                }\n",
    "  \n",
    "                } else {\n",
    "                    res[[id]][[context]][[method]] <- list(\n",
    "                      top_loci = process_top_loci(dat_con, dat_susie),\n",
    "                      pip = get_pip_withname(dat_con)\n",
    "                    )\n",
    "                    res_sum[[id]][[context]][[method]] <- list(\n",
    "                      variant_names = dat_con$variant_names,\n",
    "                      sumstats = dat_con$sumstats\n",
    "                    )\n",
    "\n",
    "                }\n",
    "                \n",
    "\n",
    "                if(has_rows(dat_con$top_loci) || has_rows(dat_con$preset_top_loci)) cons_top_loci[[id]][[context]] <- context else  cons_top_loci[[id]][[context]] <- NULL\n",
    "              }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    cons_top_loci <- cons_top_loci %>% compact()  # Use 'compact' to remove NULLs\n",
    "    cons_top_loci <- if(length(cons_top_loci) > 0) cons_top_loci else NA\n",
    "\n",
    "    combine_data = combine_data_sumstats = cons_top_loci_minp = ''\n",
    "    combine_data = paste0(\"${_output:add}\",\"/\",\"${name}\", \".\", ${'\"epigenomics_\"' if fsusie else '\"\"'}, ${ '\"metabolomics_\"' if metaQTL else '\"\"'}, gene, \".cis_results_db.export.rds\")\n",
    "    if (${\"FALSE\" if fsusie else \"TRUE\"}) combine_data_sumstats = gsub(\"export.rds$\", \"export_sumstats.rds\", combine_data)\n",
    "  \n",
    "    if (${\"TRUE\" if exported_file.is_file() else \"FALSE\"}){\n",
    "        if (file.exists(combine_data)) {\n",
    "          res_exp <- readRDS(combine_data)\n",
    "          res[[gene]] <- c(res[[gene]], res_exp[[gene]]) # this may need to change if tehre are multiple genes in one rds file. \n",
    "        }\n",
    "        if (file.exists(combine_data_sumstats)) {\n",
    "          res_sum_exp <- readRDS(combine_data_sumstats)\n",
    "          res_sum[[gene]] <- c(res_sum[[gene]], res_sum_exp[[gene]])\n",
    "        }\n",
    "    }\n",
    "    saveRDS(res, combine_data)\n",
    "    # only save sumstats results when NOT fsusie or multi gene mvsusie\n",
    "    if (${\"FALSE\" if fsusie or multi_gene else \"TRUE\"}) saveRDS(res_sum, combine_data_sumstats)\n",
    "  \n",
    "    # generate md5 for data transferring\n",
    "    system(paste(\"md5sum \",combine_data, \" > \", paste0(combine_data, \".md5\")))\n",
    "    if (${\"FALSE\" if fsusie or multi_gene else \"TRUE\"}) system(paste(\"md5sum \",combine_data_sumstats, \" > \", paste0(combine_data_sumstats, \".md5\")))\n",
    "  \n",
    "      \n",
    "    TSS <- tryCatch({dat_con$region_info$region_coord$start},  error = function(e) {return(NA)})\n",
    "    if (length(res) > 0) conditions = paste(names(res[[1]]), collapse = \",\") else conditions = ''\n",
    "  \n",
    "    # fsusie does not have sumstats or pvalue, do not need to run this\n",
    "\n",
    "    if (${\"FALSE\" if fsusie or gwas or multi_gene else \"TRUE\"})  {\n",
    "        context_map <- process_and_match_contexts('${context_meta}', cons_top_loci, res_minp)\n",
    "        context_map$context_df_with_order <- process_lf2_cluster(context_map$context_df_with_order)\n",
    "        cons_top_loci_minp <- combine_order(context_map$context_df_no_order, context_map$context_df_with_order)\n",
    "    }\n",
    "  \n",
    "    meta = data.frame(chr=\"${_meta_info[0]}\", start=\"${_meta_info[1]}\", end=\"${_meta_info[2]}\", region_id=\"${_meta_info[3]}\", TSS =  if(is.null(TSS)) NA else TSS, \n",
    "                      original_data = paste(basename(orig_files), collapse = \",\"), combined_data = basename(combine_data), combined_data_sumstats = basename(combine_data_sumstats), \n",
    "                      conditions = conditions, \n",
    "                      conditions_top_loci = if(length(cons_top_loci) > 0) cons_top_loci[[1]] %>% unlist %>% names %>% as.character %>% paste(., collapse = ',') else '',\n",
    "                      conditions_top_loci_minp = if(length(cons_top_loci_minp) > 0) cons_top_loci_minp %>% paste(., collapse = ',') else '')\n",
    "\n",
    "    write_delim(meta, \"${_output}\", delim = '\\t')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2540dd0-41e5-4141-9462-6008b541bffe",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cis_results_export_2, gwas_results_export_2]\n",
    "# set it as True if you only want run first step (running parallel) and combine them manually after all finished \n",
    "parameter: step1_only = False\n",
    "skip_if(step1_only)\n",
    "# provide exported meta to filter the exported genes \n",
    "parameter: exported_file = path()\n",
    "# optional: qtl or gwas, there is slightly different in qtl and gwas rds file\n",
    "parameter: gwas = False\n",
    "input: group_by = 'all'\n",
    "output: f\"{cwd}/{name}.{'block_results_db' if gwas else 'cis_results_db'}.tsv\" \n",
    "# stop_if(_input[0] not in locals().keys(), 'All files have been exported already') #FIXME should we remove to a separate file. sothat we can stop globally as above\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    if [ -e \"${_output:ad}/${name}_cache/\" ]; then\n",
    "        sed 's/^chr/#chr/' `ls ${_output:ad}/${name}_cache/*tsv |head -n1` | head -n 1 > ${_output:an}.temp\n",
    "        tail -n +2 -q ${_output:ad}/${name}_cache/*.tsv >> ${_output:an}.temp\n",
    "        error_files=$(find \"${_output:ad}/${name}_cache/\" -type f -name \"*_error\")\n",
    "\n",
    "        if [[ -n $error_files ]]; then\n",
    "            cat $error_files >> ${_output:an}.error_genes\n",
    "        else\n",
    "            echo \"No truncated files detected\"\n",
    "        fi\n",
    "    else\n",
    "        echo \"All files have been exported already\"\n",
    "    fi\n",
    "    \n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    if (file.exists(paste0(${_output:anr},\".temp\"))) {\n",
    "        library(tidyverse)\n",
    "        meta <- read_delim(paste0(${_output:anr},\".temp\"), delim = '\\t')\n",
    "\n",
    "        if (${\"TRUE\" if exported_file.is_file() else \"FALSE\"}){\n",
    "          exp_meta <- read_delim(${exported_file:r}, delim = '\\t')\n",
    "          meta <- bind_rows(meta, exp_meta) %>%\n",
    "              group_by(`#chr`, start, end, region_id, TSS) %>%\n",
    "              summarise(across(c(original_data, combined_data, combined_data_sumstats, conditions, conditions_top_loci), \n",
    "                               ~paste(unique(.), collapse = \",\")),\n",
    "                        .groups = 'drop')\n",
    "              }\n",
    "\n",
    "        write_delim(meta, ${_output:r}, delim = '\\t')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe6e3e8-5db5-4df8-afa4-c2f7d5c63631",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cis_results_export_3]\n",
    "# set it as True if you only want run first step (running parallel) and combine them manually after all finished \n",
    "parameter: step1_only = False\n",
    "\n",
    "skip_if(step1_only)\n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_input:n}.stderr', stdout = f'{_input:n}.stdout', entrypoint=entrypoint   \n",
    "    rm -rf \"${_input:ad}/${name}_cache/\"\n",
    "    rm -rf ${_input:an}.temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753185c2-0c82-4842-bd64-0c86152f2599",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## GWAS results consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a485f0-941b-4f17-95a7-f04da0e12d21",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#get union of step1\n",
    "#1200 blocks costed ~5mins with in one for loop\n",
    "[gwas_results_export_3]\n",
    "parameter: step1_only = False\n",
    "\n",
    "skip_if(step1_only)\n",
    "output: f\"{cwd}/{name}.union_export.tsv.gz\" \n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_input:n}.stderr', stdout = f'{_input:n}.stdout', entrypoint=entrypoint   \n",
    "    rm -rf \"${_input:ad}/${name}_cache/\"\n",
    "    rm -rf ${_input:an}.temp\n",
    "\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "    library(data.table)\n",
    "\n",
    "    mtx <- read_delim(${_input:r})\n",
    "    files <- mtx %>% filter(!is.na(conditions_top_loci)) %>% pull(combined_data) %>% paste0(${_input[0]:dr},'/',.)\n",
    "    all_top_loci <- data.frame()\n",
    "\n",
    "    for (i in seq_along(files)) {\n",
    "      file <- files[i]\n",
    "      res <- readRDS(file)\n",
    "\n",
    "      file_top_loci <- lapply(names(res), function(block) {\n",
    "        lapply(names(res[[block]]), function(study) {\n",
    "          lapply(names(res[[block]][[study]]), function(method) {\n",
    "            if (!is.null(res[[block]][[study]][[method]]$top_loci)) {\n",
    "              temp_df <- res[[block]][[study]][[method]]$top_loci\n",
    "              mutate(temp_df, study = study, method = method, block = block)\n",
    "            } else {\n",
    "              NULL  \n",
    "            }\n",
    "          })\n",
    "        }) %>% bind_rows()  \n",
    "      }) %>% bind_rows() \n",
    "\n",
    "      all_top_loci <- bind_rows(all_top_loci, file_top_loci)\n",
    "      if (i %% 100 == 0) {\n",
    "        message(sprintf(\"Have processed %d files.\", i))\n",
    "      }\n",
    "    }\n",
    "    fwrite(all_top_loci, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0c81d-f297-43d5-aabe-ccfa4edc3d65",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# simply combine seperate meta files, not works for having an exsiting exported file for now. \n",
    "[combine_export_meta]\n",
    "parameter: cache_path=path\n",
    "parameter: output_file = str\n",
    "parameter: remove_cache = False\n",
    "output: f\"{cache_path:d}/{output_file}\" \n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_input:n}.stderr', stdout = f'{_input:n}.stdout', entrypoint=entrypoint   \n",
    "    head -n 1 -q ${cache_path}/*.tsv | sed 's/^chr/#chr/' | head -n 1 > ${_output}\n",
    "    tail -n +2 -q ${cache_path}/*.tsv >> ${_output}\n",
    "    if ${\"true\" if remove_cache else \"false\"}; then\n",
    "        rm -rf ${cache_path}\n",
    "    fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508da2b5-8d84-498d-8020-ccb405c5cdfe",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[export_top_loci]\n",
    "parameter: export_path=path\n",
    "parameter: region = str\n",
    "parameter: prefix = str\n",
    "parameter: suffix = str\n",
    "parameter: fsusie_prefix = ''\n",
    "input: f\"{export_path}/{prefix}.{fsusie_prefix}{region}.{suffix}\" \n",
    "output: f\"{cwd:a}/{prefix}.{region}.toploci.csv.gz\" \n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = job_size, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "    library(data.table)\n",
    "    res <- readRDS(\"${_input}\")\n",
    "    file_top_loci <- lapply(names(res), function(gene) {\n",
    "        lapply(names(res[[gene]]), function(context) {\n",
    "          lapply(c('top_loci','preset_top_loci'), function(method) {\n",
    "            if (!is.null(res[[gene]][[context]][[method]])) {\n",
    "              temp_df <- res[[gene]][[context]][[method]]\n",
    "              mutate(temp_df, study = context, method = method, region = gene)\n",
    "            } else {\n",
    "              NULL  \n",
    "            }\n",
    "          })\n",
    "        }) %>% bind_rows()  \n",
    "    }) %>% bind_rows() \n",
    "    fwrite(file_top_loci,\"${_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a563cfe-7cff-4536-b440-c34126e7a192",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overlap QTL and GWAS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860a8e7-60ba-4c06-b1fa-80075b463392",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[overlap_qtl_gwas_1]\n",
    "parameter: per_chunk = 100\n",
    "parameter: gwas_meta_path = path()\n",
    "parameter: qtl_meta_path = path()\n",
    "parameter: gwas_file_path = ''\n",
    "parameter: qtl_file_path = ''\n",
    "# Optional: if a region list is provide the analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "# Otherwise, all regions with both genotype and phenotype files will be analyzed\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "if qtl_file_path == '':\n",
    "    qtl_file_path = qtl_meta_path.parent\n",
    "if gwas_file_path == '':\n",
    "    gwas_file_path = gwas_meta_path.parent\n",
    "    \n",
    "# Load the data, suppressing messages is not typically done in pandas as it does not inherently output messages when loading files\n",
    "gwas_meta = pd.read_csv(gwas_meta_path, sep='\\t', low_memory=False)\n",
    "gwas_meta = gwas_meta[gwas_meta['conditions_top_loci'].notna()]\n",
    "\n",
    "qtl_meta = pd.read_csv(qtl_meta_path, sep='\\t', low_memory=False)\n",
    "qtl_meta = qtl_meta[qtl_meta['conditions_top_loci'].notna()]\n",
    "\n",
    "# Filter and mutate operations, translated to pandas\n",
    "gwas_meta['combined_data_toploci'] = gwas_meta.apply(lambda row: row['combined_data'] if pd.notnull(row['conditions_top_loci']) else pd.NA, axis=1)\n",
    "\n",
    "region_ids=[]\n",
    "# If region_list is provided, read the file and extract IDs\n",
    "if not region_list.is_dir():\n",
    "    if region_list.is_file():\n",
    "        region_list_df = pd.read_csv(region_list, sep='\\t', header=None, comment = \"#\")\n",
    "        region_ids = region_list_df.iloc[:, -1].unique()  # Extracting the last column for IDs\n",
    "    else:\n",
    "        raise ValueError(\"The region_list path provided is not a file.\")\n",
    "        \n",
    "if len(region_name) > 0:\n",
    "    region_ids = list(set(region_ids).union(set(region_name)))\n",
    "    \n",
    "if len(region_ids) > 0:\n",
    "    qtl_meta = qtl_meta[qtl_meta['region_id'].isin(region_ids)]\n",
    "    \n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "grouped_gwas_meta = {k: v for k, v in gwas_meta.groupby('#chr')}\n",
    "def check_overlap(gene_row, grouped_gwas_meta):\n",
    "    chr_group = gene_row['#chr']\n",
    "    if chr_group in grouped_gwas_meta:\n",
    "        block_region = grouped_gwas_meta[chr_group]\n",
    "        overlaps = block_region[\n",
    "            (block_region['start'] <= gene_row['end']) &\n",
    "            (block_region['end'] >= gene_row['start'])\n",
    "        ]\n",
    "        if not overlaps.empty:\n",
    "            return ','.join(overlaps['combined_data_toploci'].astype(str))\n",
    "    return pd.NA\n",
    "\n",
    "stop_if(len(qtl_meta) == 0, f'No file left for analysis ')\n",
    "\n",
    "qtl_meta_cand = qtl_meta.apply(lambda row: pd.Series({\n",
    "    'gwas_file': check_overlap(row, grouped_gwas_meta)\n",
    "}), axis=1)\n",
    "\n",
    "# Concatenate the new columns to the original qtl_meta DataFrame\n",
    "qtl_meta_cand = pd.concat([qtl_meta, qtl_meta_cand], axis=1)\n",
    "qtl_meta_filtered = qtl_meta_cand[qtl_meta_cand['gwas_file'].notna()]\n",
    "qtl_meta_filtered = qtl_meta_filtered.dropna(subset=['gwas_file'])\n",
    "\n",
    "\n",
    "regional_data = {\n",
    "    'meta': [(row['#chr'], row['start'], row['end'], row['region_id'], row['gwas_file'].split(',')) for _, row in qtl_meta_filtered.iterrows()],\n",
    "    'qtl_data': [f\"{qtl_file_path}/{row['combined_data']}\" for _, row in qtl_meta_filtered.iterrows()]\n",
    "}\n",
    "\n",
    "meta_info = regional_data['meta']\n",
    "stop_if(len(regional_data['qtl_data']) == 0, f'No file left for analysis ')\n",
    "\n",
    "input: regional_data[\"qtl_data\"], group_by = lambda x: group_by_region(x, regional_data[\"qtl_data\"]), group_with = \"meta_info\"\n",
    "output: f\"{cwd}/gwas_qtl/cache/{name}_gwas_batch_meta_{_meta_info[3]}.tsv\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = job_size, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "    library(plyr)\n",
    "    library(data.table)\n",
    "\n",
    "    # Function to add 'chr' in variants\n",
    "    add_chr_prefix <- function(var) {\n",
    "      if (any(grepl(\"chr\", var))) {\n",
    "        var <- var\n",
    "      } else {\n",
    "        var <- paste0(\"chr\", var)\n",
    "      }\n",
    "      return(var)\n",
    "    }\n",
    "\n",
    "    # Function to check if a dataframe has rows\n",
    "    has_rows <- function(df) {\n",
    "      !is.null(df) && nrow(df) > 0\n",
    "    }\n",
    "\n",
    "\n",
    "    extract_top_loci <- function(res, include_method = FALSE) {\n",
    "      all_top_loci <- lapply(names(res), function(region) {\n",
    "        lapply(names(res[[region]]), function(study) {\n",
    "          if (include_method) {\n",
    "            method_results <- lapply(names(res[[region]][[study]]), function(method) {\n",
    "              top_loci <- NULL\n",
    "              if (!is.null(res[[region]][[study]][[method]]$top_loci) && nrow(res[[region]][[study]][[method]]$top_loci) > 0) {\n",
    "                top_loci <- mutate(res[[region]][[study]][[method]]$top_loci, study = study, method = method, region = region)\n",
    "              }\n",
    "              return(top_loci)\n",
    "            })\n",
    "            return(bind_rows(method_results))\n",
    "          } else {\n",
    "            top_loci <- list()\n",
    "            if (!is.null(res[[region]][[study]]$top_loci) && nrow(res[[region]][[study]]$top_loci) > 0) {\n",
    "              top_loci[[length(top_loci) + 1]] <- mutate(res[[region]][[study]]$top_loci, study = study, region = region, method = 'top_loci')\n",
    "            }\n",
    "            if (!is.null(res[[region]][[study]]$preset_top_loci) && nrow(res[[region]][[study]]$preset_top_loci) > 0) {\n",
    "              top_loci[[length(top_loci) + 1]] <- mutate(res[[region]][[study]]$preset_top_loci, study = study, region = region, method = 'preset_top_loci')\n",
    "            }\n",
    "            return(bind_rows(top_loci))\n",
    "          }\n",
    "        })\n",
    "      }) %>% bind_rows() %>% na.omit()\n",
    "\n",
    "      return(all_top_loci)\n",
    "    }\n",
    "\n",
    "  \n",
    "    # load data \n",
    "    qtl_file = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input])})\n",
    "     # Extract info from each RDS file\n",
    "    gwas_files = c(${\",\".join('\"%s\"' % x for x in _meta_info[4])}) %>% paste0('${gwas_file_path}','/',.)\n",
    "    # Process GWAS files\n",
    "    gwas_all_top_loci <- do.call(rbind.fill, lapply(gwas_files, function(file) {\n",
    "      res <- readRDS(file)\n",
    "      gwas_all_top_loci <- extract_top_loci(res, include_method = TRUE) \n",
    "    }))\n",
    "    if(!is.null(gwas_all_top_loci) && nrow(gwas_all_top_loci) > 0){# fixme: could remove this judge if we get a solid enough meta\n",
    "      gwas_all_top_loci <- gwas_all_top_loci%>% select(-c('z'))\n",
    "\n",
    "      # Process QTL file\n",
    "      qtl <- readRDS(qtl_file)\n",
    "      qtl_all_top_loci <- extract_top_loci(qtl) \n",
    "      if(!is.null(qtl_all_top_loci) && nrow(qtl_all_top_loci) > 0){# fixme: could remove this judge if we get a solid enough meta\n",
    "        # qtl_all_top_loci <- qtl_all_top_loci%>% select(-c('betahat','sebetahat','maf'))\n",
    "\n",
    "        cs_cal <- c('cs_coverage_0.95','cs_coverage_0.7','cs_coverage_0.5')\n",
    "\n",
    "        qtl_all_var <- qtl_all_top_loci %>%\n",
    "            #filter(rowSums(.[,cs_cal]) > 0 ) %>% #fixme\n",
    "            pull(variant_id)\n",
    "\n",
    "        gwas_all_var <- gwas_all_top_loci %>%\n",
    "            #filter(rowSums(.[,cs_cal]) > 0 ) %>% #fixme\n",
    "            pull(variant_id)\n",
    "\n",
    "        gwas_all_var <- if(any(grepl(\"chr\", qtl_all_var))) add_chr_prefix(gwas_all_var) else gsub(\"chr\", \"\", gwas_all_var)\n",
    "        gwas_all_top_loci$variant_id <- gwas_all_var\n",
    "        # since both qtl and gwas haven mapped to geno ref in exporting, here we intersect them directly\n",
    "        int_var <- intersect(qtl_all_var, gwas_all_var)\n",
    "        if(length(int_var) > 0){\n",
    "            gwas_all_top_loci <- gwas_all_top_loci %>% filter(variant_id %in% int_var) # fixme: keep all gwas variants or intersected ones\n",
    "\n",
    "            all_top_loci <- rbind.fill(gwas_all_top_loci, qtl_all_top_loci)\n",
    "            fwrite(all_top_loci,  gsub('_gwas_batch_meta','_gwas_batch_export',${_output:r}))\n",
    "\n",
    "            new_gwas <- split(gwas_all_top_loci, gwas_all_top_loci$study)\n",
    "            new_gwas <- lapply(new_gwas, function(df) {\n",
    "              split(df, df$method)\n",
    "            })\n",
    "\n",
    "            qtl[[1]] <- c(qtl[[1]], new_gwas)\n",
    "            new_qtl_path <-  paste0(${_output:ddr},\"/\",gsub(\".rds\",\".overlapped.gwas.rds\",basename(qtl_file)))\n",
    "            saveRDS(qtl, new_qtl_path)\n",
    "  \n",
    "            block_top_loci = gwas_all_top_loci$region %>% unique %>% paste(., collapse = ',')\n",
    "            final_combined_data = new_qtl_path %>% basename\n",
    "        } else {block_top_loci = final_combined_data = NA} # fixme: could remove this judge if we get a solid enough meta\n",
    "      } else {block_top_loci = final_combined_data = NA} # fixme: could remove this judge if we get a solid enough meta\n",
    "    } else {\n",
    "        block_top_loci = final_combined_data = NA\n",
    "    }\n",
    " \n",
    "    qtl_meta <- suppressMessages(read_delim('${qtl_meta_path}'))\n",
    "    qtl_meta <- qtl_meta %>% filter(region_id == '${_meta_info[3]}')  %>% mutate(block_top_loci = block_top_loci,\n",
    "                                                     final_combined_data = final_combined_data)\n",
    "    fwrite(qtl_meta, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d6a0f-b09f-4b09-92ee-8fe8b9f1905e",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[overlap_qtl_gwas_2]\n",
    "# set it as True if you only want run first step (running parallel) and combine them manually after all finished \n",
    "parameter: step1_only = False\n",
    "skip_if(step1_only)\n",
    "input: group_by = 'all'\n",
    "output: f\"{cwd}/{name}.overlapped.gwas.tsv\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = job_size, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    library(data.table)\n",
    "    exp_path <- ${_input[0]:adr}\n",
    "    meta_files <- c(${\",\".join(['\"%s\"' % x.absolute() for x in _input])})\n",
    "    exp_files <- list.files(exp_path, \"_gwas_batch_export\", full.names = T)\n",
    "    meta_list <- exp_list <- list()\n",
    "    meta_combined <- rbindlist(lapply(meta_files, fread), fill = TRUE)\n",
    "    exp_combined <- rbindlist(lapply(exp_files, fread), fill = TRUE)\n",
    "    fwrite(exp_combined, gsub(\"tsv\",\"export.csv.gz\",\"${_output}\"))\n",
    "    fwrite(meta_combined, \"${_output}\", sep = '\\t')\n",
    "  \n",
    "#bash: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    # rm -rf ${_input[0]:adr}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
