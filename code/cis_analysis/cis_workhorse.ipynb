{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Advanced cis-QTL analysis with individual level data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook performs various advanced statistical analysis on multiple xQTL in a given region. Current procedures implemented include:\n",
    "\n",
    "1. Univariate analysis\n",
    "    - SuSiE\n",
    "    - Univeriate TWAS weights: LASSO, Elastic net, mr.mash and SuSiE (optional)\n",
    "    - Cross validation of TWAS methods (optional but highly recommended if TWAS weights are computed)\n",
    "2. Functional data (epigenomic xQTL) analysis\n",
    "    - fSuSiE\n",
    "3. Multivariate analysis\n",
    "    - mvSuSiE\n",
    "    - mr.mash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "1. A list of regions to be analyzed (optional); the last column of this file should be region name.\n",
    "2. Either a list of per chromosome genotype files, or one file for genotype data of the entire genome. Genotype data has to be in PLINK `bed` format. \n",
    "3. Vector of lists of phenotype files per region to be analyzed, in UCSC `bed.gz` with index in `bed.gz.tbi` formats.\n",
    "4. Vector of covariate files corresponding to the lists above.\n",
    "5. Customized cis windows file. If it is not provided, a fixed sized cis-window will be used.\n",
    "6. Optionally a vector of names of the phenotypic conditions in the form of `cond1 cond2 cond3` separated with whitespace.\n",
    "\n",
    "Input 2 and 3 should be outputs from `genotype_per_region` and `annotate_coord` modules in previous preprocessing steps. 4 should be output of `covariate_preprocessing` pipeline that contains genotype PC, phenotypic hidden confounders and fixed covariates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Example genotype data\n",
    "\n",
    "```\n",
    "#chr        path\n",
    "chr21 /mnt/mfs/statgen/xqtl_workflow_testing/protocol_example.genotype.chr21.bed\n",
    "chr22 /mnt/mfs/statgen/xqtl_workflow_testing/protocol_example.genotype.chr22.bed\n",
    "```\n",
    "\n",
    "Alternatively, simply use `protocol_example.genotype.chr21_22.bed` if all chromosomes are in the same file.\n",
    "\n",
    "### Example phenotype list\n",
    "\n",
    "```\n",
    "#chr    start   end ID  path\n",
    "chr12   752578  752579  ENSG00000060237  /home/gw/GIT/github/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/MWE/output/phenotype/protocol_example.protein.bed.gz\n",
    "chr12   990508  990509  ENSG00000082805  /home/gw/GIT/github/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/MWE/output/phenotype/protocol_example.protein.bed.gz\n",
    "chr12   2794969 2794970 ENSG00000004478  /home/gw/GIT/github/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/MWE/output/phenotype/protocol_example.protein.bed.gz\n",
    "chr12   4649113 4649114 ENSG00000139180  /home/gw/GIT/github/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/MWE/output/phenotype/protocol_example.protein.bed.gz\n",
    "chr12   6124769 6124770 ENSG00000110799  /home/gw/GIT/github/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/MWE/output/phenotype/protocol_example.protein.bed.gz\n",
    "chr12   6534516 6534517 ENSG00000111640  /home/gw/GIT/github/fungen-xqtl-analysis/analysis/Wang_Columbia/ROSMAP/MWE/output/phenotype/protocol_example.protein.bed.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Example cis-window file\n",
    "\n",
    "It should have strictly 4 columns, with the header a commented out line:\n",
    "\n",
    "```\n",
    "#chr    start    end    gene_id\n",
    "chr10   0    6480000    ENSG00000008128\n",
    "chr1    0    6480000    ENSG00000008130\n",
    "chr1    0    6480000    ENSG00000067606\n",
    "chr1    0    7101193    ENSG00000069424\n",
    "chr1    0    7960000    ENSG00000069812\n",
    "chr1    0    6480000    ENSG00000078369\n",
    "chr1    0    6480000    ENSG00000078808\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The key is that the 4th column ID should match with the 4th column ID in the phenotype list. Otherwise the cis-window to analyze will not be found.\n",
    "\n",
    "### About indels\n",
    "\n",
    "Option `--no-indel` will remove indel from analysis. FIXME: Gao need to provide more guidelines how to deal with indels in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "For each analysis region, the output is SuSiE model fitted and saved in RDS format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### i. SuSiE with TWAS weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing [FIXME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Below we duplicate the examples for phenotype and covariates to demonstrate that when there are multiple phenotypes for the same genotype it is possible to use this pipeline to analyze all of them (more than two is accepted as well).\n",
    "\n",
    "Here using `--region-name` we focus the analysis on 3 genes. In practice if this parameter is dropped, the union of all regions in all phenotype region lists will be analyzed. It is possible for some of the regions there are no genotype data, in which case the pipeline will output RDS files with a warning message to indicate the lack of genotype data to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**Note:** Suggested output naming convention is cohort_modality, eg ROSMAP_snRNA_pseudobulk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run pipeline/cis_workhorse.ipynb susie_twas  \\\n",
    "    --name protocol_example_protein  \\\n",
    "    --genoFile input/xqtl_association/protocol_example.genotype.chr21_22.bed   \\\n",
    "    --phenoFile output/phenotype/protocol_example.protein.region_list.txt \\\n",
    "                output/phenotype/protocol_example.protein.region_list.txt \\\n",
    "    --covFile output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
    "              output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz  \\\n",
    "    --customized-cis-windows input/xqtl_association/protocol_example.protein.enhanced_cis_chr21_chr22.bed \\\n",
    "    --region-name ENSG00000241973_P42356 ENSG00000160209_O00764 ENSG00000100412_Q99798 \\\n",
    "    --phenotype-names trait_A trait_B \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "It is also possible to analyze a selected list of regions using option `--region-list`. The last column of this file will be used for the list to analyze. Here for example use the same list of regions as we used for customized cis-window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run xqtl-pipeline/pipeline/cis_workhorse.ipynb susie_twas  \\\n",
    "    --name protocol_example_protein  \\\n",
    "    --genoFile xqtl_association/protocol_example.genotype.chr21_22.bed   \\\n",
    "    --phenoFile output/phenotype/protocol_example.protein.region_list.txt \\\n",
    "                output/phenotype/protocol_example.protein.region_list.txt \\\n",
    "    --covFile output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
    "              output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz  \\\n",
    "    --customized-cis-windows xqtl_association/protocol_example.protein.enhanced_cis_chr21_chr22.bed \\\n",
    "    --region-list xqtl_association/protocol_example.protein.enhanced_cis_chr21_chr22.bed \\\n",
    "    --phenotype-names trait_A trait_B \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "**Note:** When both `--region-name` and `--region-list` are used, the union of regions from these parameters will be analyzed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "FIXME: We should probably just explain these parameters, will work better for conversion script\n",
    "\n",
    "\n",
    "To perform fine-mapping only without TWAS weights,\n",
    "\n",
    "```\n",
    "sos run pipeline/cis_workhorse.ipynb susie_twas --no-twas-weights ... # rest of parameters the same. \n",
    "```\n",
    "\n",
    "To perform fine-mapping and TWAS weights without cross validation,\n",
    "\n",
    "```\n",
    "sos run pipeline/cis_workhorse.ipynb susie_twas --twas-cv-folds 0 ... # rest of parameters the same. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "It is also possible to specify a subset of samples to analyze, using `--keep-samples` parameter. For example we create a file to keep the ID of 50 samples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "zcat output/covariate/protocol_example.protein.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz | head -1 | awk '{for (i=2; i<=51; i++) printf $i \" \"; print \"\"}'> output/keep_samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "then use them in our analysis,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "```\n",
    "sos run xqtl-pipeline/pipeline/cis_workhorse.ipynb susie_twas --keep-samples output/keep_samples.txt ... # rest of parameters the same\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### ii. fSuSiE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing [FIXME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**Note:** Suggested output naming convention is cohort_modality, eg ROSMAP_snRNA_pseudobulk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sos run pipeline/cis_workhorse.ipynb fsusie \\\n",
    "    --name protocol_example_methylation \\\n",
    "    --genoFile xqtl_association/protocol_example.genotype.chr21_22.plink_per_chrom.txt \\\n",
    "    --phenoFile output/phenotype_by_region/protocol_example.methylation.bed.phenotype_by_region_files.txt \\\n",
    "                output/phenotype_by_region/protocol_example.methylation.bed.phenotype_by_region_files.txt  \\\n",
    "    --covFile output/covariate/protocol_example.methylation.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
    "              output/covariate/protocol_example.methylation.protocol_example.samples.protocol_example.genotype.chr21_22.pQTL.plink_qc.prune.pca.Marchenko_PC.gz \\\n",
    "    --container oras://ghcr.io/cumc/pecotmr_apptainer:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run cis_workhorse.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Workflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path(\"output\")\n",
    "# A list of file paths for genotype data, or the genotype data itself. \n",
    "parameter: genoFile = path\n",
    "# One or multiple lists of file paths for phenotype data.\n",
    "parameter: phenoFile = paths\n",
    "# One or multiple lists of file paths for phenotype ID mapping file. The first column should be the original ID, the 2nd column should be the ID to be mapped to.\n",
    "parameter: phenoIDFile = paths()\n",
    "# Covariate file path\n",
    "parameter: covFile = paths\n",
    "# Optional: if a region list is provide the analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "# Otherwise, all regions with both genotype and phenotype files will be analyzed\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "# Only focus on a subset of samples\n",
    "parameter: keep_samples = path()\n",
    "# An optional list documenting the custom cis window for each region to analyze, with four column, chr, start, end, region ID (eg gene ID).\n",
    "# If this list is not provided, the default `window` parameter (see below) will be used.\n",
    "parameter: customized_cis_windows = path()\n",
    "# Specify the cis window for the up and downstream radius to analyze around the region of interest in units of bp\n",
    "# When this is zero, we will rely on customized_cis_windows\n",
    "parameter: window = 0\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = str\n",
    "# save data object or not\n",
    "parameter: save_data = False\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 200\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"1h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"20G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "# Name of phenotypes\n",
    "parameter: phenotype_names = [f'{x:bn}' for x in phenoFile]\n",
    "parameter: seed = 999\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "# input is genoFile, phenoFile, covFile and optionally region_list. If region_list presents then we only analyze what's contained in the list.\n",
    "# regional_data should be a dictionary like:\n",
    "#{'data': [(\"genotype_1.bed\", \"phenotype_1.bed.gz\", \"covariate_1.gz\"), (\"genotype_2.bed\", \"phenotype_1.bed.gz\", \"phenotype_2.bed.gz\", \"covariate_1.gz\", \"covariate_2.gz\") ... ],\n",
    "# 'meta_info': [(\"chr12:752578-752579\",\"chr12:752577-752580\", \"gene_1\", \"trait_1\"), (\"chr13:852580-852581\",\"chr13:852579-852580\", \"gene_2\", \"trait_1\", \"trait_2\") ... ]}\n",
    "\n",
    "def process_pheno_files(pheno_files, cov_files, phenotype_names, pheno_id_files):\n",
    "    '''\n",
    "    Example output:\n",
    "    #chr    start      end    start_cis       end_cis           ID  Original_ID   path     cov_path             cond             coordinate     geno_path\n",
    "    0  chr12   752578   752579  652578   852579  ENSG00000060237  Q9H4A3,P62873  protocol_example.protein_1.bed.gz,protocol_example.protein_2.bed.gz  covar_1.gz,covar_2.gz  trait_A,trait_B    chr12:752578-752579  protocol_example.genotype.chr21_22.bed\n",
    "    '''\n",
    "    # Initialize an empty DataFrame for accumulation\n",
    "    accumulated_pheno_df = pd.DataFrame()\n",
    "\n",
    "    merge_keys = ['#chr', 'start', 'end', 'ID', 'Original_ID']\n",
    "    if len(pheno_id_files) == 0:\n",
    "        pheno_id_files = [None] * len(pheno_files)\n",
    "    for pheno_path, cov_path, phenotype_name, id_map in zip(pheno_files, cov_files, phenotype_names, pheno_id_files):\n",
    "        if not os.path.isfile(cov_path):\n",
    "            raise FileNotFoundError(f\"No valid path found for file: {cov_path}\")\n",
    "\n",
    "        # Read and process each phenotype file\n",
    "        pheno_df = pd.read_csv(pheno_path, sep=\"\\s+\", header=0)\n",
    "        \n",
    "        # Adapt pheno file paths and add additional information\n",
    "        pheno_df.iloc[:, 4] = adapt_file_path_all(pheno_df, pheno_df.columns[4], f\"{pheno_path:a}\")\n",
    "        pheno_df = pheno_df.assign(\n",
    "            Original_ID=pheno_df['ID'],\n",
    "            cov_path=str(cov_path), \n",
    "            cond=phenotype_name\n",
    "        )\n",
    "        # change ID mapping\n",
    "        if id_map is not None:\n",
    "            # Load id_map file, skipping lines that start with '#'\n",
    "            id_map_df = pd.read_csv(id_map, sep='\\s+', header=None, comment='#', names=['old_ID', 'new_ID'])\n",
    "\n",
    "            # Get unique old_IDs from id_map_df\n",
    "            unique_old_IDs = id_map_df['old_ID'].unique()\n",
    "\n",
    "            for old_ID in unique_old_IDs:\n",
    "                # Find matching rows in pheno_df\n",
    "                match_indices = pheno_df[pheno_df['ID'] == old_ID].index\n",
    "                # Get all new_IDs associated with the old_ID\n",
    "                new_IDs = id_map_df[id_map_df['old_ID'] == old_ID]['new_ID'].tolist()\n",
    "                for new_ID in new_IDs:\n",
    "                    if new_ID == new_IDs[0]:\n",
    "                        # Update the ID directly for the first new_ID\n",
    "                        pheno_df.loc[match_indices, 'ID'] = new_ID\n",
    "                    else:\n",
    "                        # For subsequent new_IDs, duplicate the rows and update the ID\n",
    "                        for idx in match_indices:\n",
    "                            new_row = pheno_df.loc[idx].copy()\n",
    "                            new_row['ID'] = new_ID\n",
    "                            pheno_df = pd.concat([pheno_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        # Merge with the accumulated DataFrame\n",
    "        if accumulated_pheno_df.empty:\n",
    "            accumulated_pheno_df = pheno_df\n",
    "        else:\n",
    "            # Merge on specified keys with default suffixes\n",
    "            merged_df = pd.merge(accumulated_pheno_df, pheno_df, on=merge_keys, how='outer', suffixes=('_x', '_y'))\n",
    "\n",
    "            # Determine non-key columns\n",
    "            non_key_columns = [col for col in pheno_df.columns if col not in merge_keys]\n",
    "\n",
    "            # Concatenate non-key columns for matching keys\n",
    "            for col in non_key_columns:\n",
    "                col_x = f'{col}_x'\n",
    "                col_y = f'{col}_y'\n",
    "\n",
    "                # Handling concatenation for matching keys\n",
    "                merged_df[col] = merged_df.apply(\n",
    "                    lambda row: row[col_x] if pd.isna(row[col_y]) else \n",
    "                                (row[col_y] if pd.isna(row[col_x]) else f'{row[col_x]},{row[col_y]}'), axis=1)\n",
    "\n",
    "                # Drop the temporary columns\n",
    "                merged_df.drop([col_x, col_y], axis=1, inplace=True)\n",
    "\n",
    "            accumulated_pheno_df = merged_df\n",
    "    # Check for duplicates in accumulated_pheno_df\n",
    "    if accumulated_pheno_df.duplicated(keep=False).any():\n",
    "        duplicated_rows = accumulated_pheno_df[accumulated_pheno_df.duplicated(keep=False)]\n",
    "        raise ValueError(f\"Duplicated rows found:\\n{duplicated_rows}\")\n",
    "    # Combine rows with identical values except for Original_ID\n",
    "    combined_df = accumulated_pheno_df.groupby(accumulated_pheno_df.columns.difference(['Original_ID']).tolist(), as_index=False).agg({'Original_ID': ','.join})\n",
    "    if combined_df['Original_ID'].duplicated(keep=False).any():\n",
    "        duplicated_rows = combined_df[combined_df['Original_ID'].duplicated(keep=False)]\n",
    "        error_message = \"Original phenotypic ID should be unique, but duplicates are found. \" \\\n",
    "                \"Please check your phenotype data file and/or phenotype ID mapping files \" \\\n",
    "                \"to ensure there are no duplicates in the original ID. \" \\\n",
    "                \"Duplicated rows:\\n{}\".format(duplicated_rows)\n",
    "        raise ValueError(error_message)\n",
    "    return combined_df\n",
    "\n",
    "# Load phenotype meta data\n",
    "if len(phenoFile) != len(covFile):\n",
    "    raise ValueError(\"Number of input phenotypes files must match that of covariates files\")\n",
    "if len(phenoFile) != len(phenotype_names):\n",
    "    raise ValueError(\"Number of input phenotypes files must match the number of phenotype names\")\n",
    "if len(phenoIDFile) > 0 and len(phenoFile) != len(phenoIDFile):\n",
    "    raise ValueError(\"Number of input phenotypes files must match the number of phenotype ID mapping files\")\n",
    "meta_data = process_pheno_files(phenoFile, covFile, phenotype_names, phenoIDFile)\n",
    "\n",
    "# Load genotype meta data\n",
    "if f\"{genoFile:x}\" == \".bed\":\n",
    "    geno_meta_data = pd.DataFrame([(\"chr\"+str(x), f\"{genoFile:a}\") for x in range(1,23)] + [(\"chrX\", f\"{genoFile:a}\")], columns=['#chr', 'geno_path'])\n",
    "else:\n",
    "    geno_meta_data = pd.read_csv(f\"{genoFile:a}\", sep = \"\\s+\", header=0)\n",
    "    geno_meta_data.iloc[:, 1] = adapt_file_path_all(geno_meta_data, geno_meta_data.columns[1], f\"{genoFile:a}\")\n",
    "    geno_meta_data.columns = ['#chr', 'geno_path']\n",
    "    geno_meta_data['#chr'] = geno_meta_data['#chr'].apply(lambda x: str(x) if str(x).startswith('chr') else f'chr{x}')\n",
    "\n",
    "# Checking the DataFrame\n",
    "valid_chr_values = [f'chr{x}' for x in range(1, 23)] + ['chrX']\n",
    "if not all(value in valid_chr_values for value in geno_meta_data['#chr']):\n",
    "    raise ValueError(\"Invalid chromosome values found. Allowed values are chr1 to chr22 and chrX.\")\n",
    "\n",
    "meta_data = meta_data.merge(geno_meta_data, on='#chr', how='inner')\n",
    "\n",
    "if len(meta_data.index) == 0:\n",
    "    raise ValueError(\"No region overlap between genotype and any of the phenotypes\")\n",
    "\n",
    "region_ids = []\n",
    "# If region_list is provided, read the file and extract IDs\n",
    "if region_list.is_file():\n",
    "    region_list_df = pd.read_csv(region_list, delim_whitespace=True, header=None, comment = \"#\")\n",
    "    region_ids = region_list_df.iloc[:, -1].unique()  # Extracting the last column for IDs\n",
    "\n",
    "# If region_name is provided, include those IDs as well\n",
    "# --region-name A B C will result in a list of [\"A\", \"B\", \"C\"] here\n",
    "if len(region_name) > 0:\n",
    "    region_ids = list(set(region_ids).union(set(region_name)))\n",
    "\n",
    "# If either region_list or region_name is provided, filter the meta_data\n",
    "if len(region_ids) > 0:\n",
    "    meta_data = meta_data[meta_data['ID'].isin(region_ids)]\n",
    "\n",
    "# Adjust cis-window\n",
    "if os.path.isfile(customized_cis_windows):\n",
    "    print(f\"Loading customized cis-window data from {customized_cis_windows}\")\n",
    "    cis_list = pd.read_csv(customized_cis_windows, comment=\"#\", header=None, names=[\"#chr\",\"start\",\"end\",\"ID\"], sep=\"\\t\")\n",
    "    meta_data = pd.merge(meta_data, cis_list, on=['#chr', 'ID'], how='left', suffixes=('', '_cis')) \n",
    "    mismatches = meta_data[meta_data['start_cis'].isna()]\n",
    "    if not mismatches.empty:\n",
    "        print(\"First 5 mismatches:\")\n",
    "        print(mismatches[['ID']].head())\n",
    "        raise ValueError(f\"{len(mismatches)} regions to analyze cannot be found in ``{customized_cis_windows}``. Please check your ``{customized_cis_windows}`` database to make sure it contains all cis-window definitions. \")\n",
    "else:\n",
    "    if window <=0 :\n",
    "        raise ValueError(\"Please either input valid path to cis-window file via ``--customized-cis-windows``, or set ``--window`` to a positive integer\")\n",
    "    meta_data['start_cis'] = meta_data['start'].apply(lambda x: max(x - window, 0))\n",
    "    meta_data['end_cis'] = meta_data['end'] + window\n",
    "# Create the final dictionary\n",
    "regional_data = {\n",
    "    'data': [(row['geno_path'], *row['path'].split(','), *row['cov_path'].split(',')) for _, row in meta_data.iterrows()],\n",
    "    'meta_info': [(f\"{row['#chr']}:{row['start']}-{row['end']}\", # this is the phenotype region\n",
    "                   f\"{row['#chr']}:{row['start_cis']}-{row['end_cis']}\", # this is the cis-window region\n",
    "                   row['ID'], row['Original_ID'], *row['cond'].split(',')) for _, row in meta_data.iterrows()]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Univariate analysis: SuSiE and TWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[susie_twas_1]\n",
    "# initial number of single effects for SuSiE\n",
    "parameter: init_L = 8\n",
    "# maximum number of single effects to use for SuSiE\n",
    "parameter: max_L = 30\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 1.0\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.0025\n",
    "# MAC cutoff, on top of MAF cutoff\n",
    "parameter: mac = 5\n",
    "# Remove indels if indel = False\n",
    "parameter: indel = True\n",
    "parameter: pip_cutoff = 0.025\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "# Perform Fine-mapping\n",
    "parameter: fine_mapping = True\n",
    "# Compute TWAS weights as well\n",
    "parameter: twas_weights = True\n",
    "# Perform K folds valiation CV for TWAS\n",
    "# Set it to zero if this is to be skipped\n",
    "parameter: twas_cv_folds = 5\n",
    "parameter: twas_cv_threads = twas_cv_folds\n",
    "# maximum number of variants to consider for CV\n",
    "# We will randomly pick a subset of it for CV purpose\n",
    "parameter: max_cv_variants = 5000\n",
    "# Further limit CV to only using common variants\n",
    "parameter: min_cv_maf = 0.05\n",
    "parameter: ld_reference_meta_file = path()\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# Check if both 'data' and 'meta_info' are empty lists\n",
    "stop_if(len(regional_data['data']) == 0, f'Either genotype or phenotype data are not available for region {\", \".join(region_name)}.')\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[3].replace(\",\",\"_\")}.univariate{\"_susie\" if fine_mapping else \"\"}{\"_twas_weights\" if twas_weights else \"\"}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    options(warn=1)\n",
    "    library(pecotmr)\n",
    "    # extract subset of samples\n",
    "    keep_samples = NULL\n",
    "    if (${\"TRUE\" if keep_samples.is_file() else \"FALSE\"}) {\n",
    "      keep_samples = unlist(strsplit(readLines(${keep_samples:ar}), \"\\\\s+\"))\n",
    "      message(paste(length(keep_samples), \"samples are selected to be loaded for analysis\"))\n",
    "    }\n",
    "    # Load regional association data\n",
    "    tryCatch({\n",
    "    fdat = load_regional_univariate_data(genotype = ${_input[0]:anr},\n",
    "                                          phenotype = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[1:len(_input)//2+1]])}),\n",
    "                                          covariate = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[len(_input)//2+1:]])}),\n",
    "                                          region = \"${_meta_info[0]}\",\n",
    "                                          cis_window = \"${_meta_info[1]}\",\n",
    "                                          conditions = c(${\",\".join(['\"%s\"' % x for x in _meta_info[4:]])}),\n",
    "                                          maf_cutoff = ${maf},\n",
    "                                          mac_cutoff = ${mac},\n",
    "                                          imiss_cutoff = ${imiss},\n",
    "                                          keep_indel = ${\"TRUE\" if indel else \"FALSE\"},\n",
    "                                          keep_samples = keep_samples,\n",
    "                                          extract_region_name = c(${\",\".join(['\"%s\"' % x for x in _meta_info[3].split(',')])}),\n",
    "                                          phenotype_header = 4,\n",
    "                                          region_name_col = 4,\n",
    "                                          scale_residuals = FALSE)\n",
    "    }, NoSNPsError = function(e) {\n",
    "        message(\"Error: \", paste(e$message, \"${_meta_info[2] + '@' + _meta_info[1]}\"))\n",
    "        #saveRDS(NULL, ${_output:ar})\n",
    "        saveRDS(list(${_meta_info[2]} = e$message), ${_output:ar}, compress='xz')\n",
    "        quit(save=\"no\")\n",
    "    })\n",
    "  \n",
    "    if (${\"TRUE\" if not (fine_mapping or twas_weights) else \"FALSE\"}) {\n",
    "      # only export data\n",
    "      saveRDS(list(${_meta_info[2]} = fdat), ${_output:ar}, compress='xz')\n",
    "      quit(save=\"no\")\n",
    "    } else {\n",
    "      if (${\"TRUE\" if save_data else \"FALSE\"}) {\n",
    "          # save data object for debug purpose\n",
    "          saveRDS(list(${_meta_info[2]} = fdat), \"${_output:ann}.univariate.rds\", compress='xz')\n",
    "      }\n",
    "    }\n",
    "    # Univeriate analysis suite\n",
    "    run_univariate_pipeline <- function(X, Y, X_scalar, Y_scalar, maf, dropped_samples, pip_cutoff_to_skip = 0) {\n",
    "      if (pip_cutoff_to_skip>0) {\n",
    "          # return a NULL set if the top loci model does not show any potentially significant variants\n",
    "          top_model_pip = susie(X,Y,L=1)$pip\n",
    "          if (!any(top_model_pip>pip_cutoff_to_skip)) {\n",
    "              return(NULL)\n",
    "          }\n",
    "      }\n",
    "      st = proc.time()\n",
    "      if (${\"TRUE\" if fine_mapping else \"FALSE\"}) {\n",
    "          res = susie_wrapper(X, Y, init_L=${init_L}, max_L=${max_L}, refine=TRUE, coverage = ${coverage[0]})\n",
    "          res = susie_post_processor(res, X, Y, X_scalar, Y_scalar, maf,\n",
    "                                 secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff},\n",
    "                                 other_quantities = list(dropped_samples = dropped_samples))\n",
    "      }\n",
    "      else {\n",
    "          res = list()\n",
    "      }\n",
    "      if ( ${\"TRUE\" if twas_weights else \"FALSE\"} ) {\n",
    "        twas_weights_output <- twas_weights_pipeline(X, Y, maf, susie_fit=res$susie_result_trimmed, \n",
    "                                     ld_reference_meta_file = ${('\"%s\"' % ld_reference_meta_file) if not ld_reference_meta_file.is_dir() else \"NULL\"},\n",
    "                                     X_scalar = X_scalar, y_scalar = Y_scalar,\n",
    "                                     cv_folds = ${twas_cv_folds}, coverage=${coverage[0]}, secondary_coverage=c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff},\n",
    "                                     min_cv_maf=${min_cv_maf}, max_cv_variants=${max_cv_variants}, cv_seed=${seed}, cv_threads=${twas_cv_threads})\n",
    "        # clean up the output database\n",
    "        res = c(res, twas_weights_output)\n",
    "        res$twas_weights = lapply(res$twas_weights, function(x) { rownames(x) <- NULL; return(x) })\n",
    "      }\n",
    "      res$total_time_elapsed = proc.time() - st\n",
    "      if (\"${_meta_info[2]}\" != \"${_meta_info[3]}\") {\n",
    "          region_name = c(\"${_meta_info[2]}\", ${\",\".join(['\"%s\"' % x for x in _meta_info[3].split(',')])})\n",
    "      } else {\n",
    "          region_name = \"${_meta_info[2]}\"\n",
    "      }\n",
    "      res$region_info = list(region_coord=parse_region(\"${_meta_info[0]}\"), grange=parse_region(\"${_meta_info[1]}\"), region_name=region_name)\n",
    "      return (res)\n",
    "    }\n",
    "  \n",
    "    fitted = list()\n",
    "    condition_names = vector()\n",
    "    r = 1\n",
    "    while (r<=length(fdat$residual_Y)) {\n",
    "      dropped_samples = list(X=fdat$dropped_sample$dropped_samples_X[[r]], \n",
    "                             y=fdat$dropped_sample$dropped_samples_Y[[r]], \n",
    "                             covar=fdat$dropped_sample$dropped_samples_covar[[r]])\n",
    "      if (ncol(fdat$residual_Y[[r]]) == 1) {\n",
    "          condition_names = c(condition_names, names(fdat$residual_Y)[r])\n",
    "      } else {\n",
    "          new_names = colnames(fdat$residual_Y[[r]])\n",
    "          if (is.null(new_names)) {\n",
    "              # column names does not exist, create generic names instead\n",
    "              new_names = 1:ncol(fdat$residual_Y[[r]])\n",
    "          }\n",
    "          new_names = paste(names(fdat$residual_Y)[r], new_names, sep=\"_\") # DLPFC_iso1 DLPFC_iso2 \n",
    "          condition_names = c(condition_names, new_names) # ACC DLPFC_iso1 DLPFC_iso2 \n",
    "      }\n",
    "      results <- lapply(1:ncol(fdat$residual_Y[[r]]), function(i) run_univariate_pipeline(fdat$residual_X[[r]], \n",
    "                                                                                    fdat$residual_Y[[r]][,i,drop=FALSE], \n",
    "                                                                                    fdat$residual_X_scalar[[r]], \n",
    "                                                                                    if (fdat$residual_Y_scalar[[r]] == 1) 1 else fdat$residual_Y_scalar[[r]][,i,drop=FALSE], \n",
    "                                                                                    fdat$maf[[r]], dropped_samples))\n",
    "      fitted = c(fitted, results)\n",
    "      # original data no longer relevant, set to NA to release memory\n",
    "      fdat$residual_X[[r]] <- NA\n",
    "      fdat$residual_Y[[r]] <- NA\n",
    "      r = r + 1\n",
    "    }\n",
    "    names(fitted) <- condition_names\n",
    "    saveRDS(list(\"${_meta_info[2]}\" = fitted), ${_output:ar}, compress='xz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Multivariate analysis: mvSuSiE and mr.mash\n",
    "\n",
    "### mvSuSiE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mvsusie_1]\n",
    "# Prior model file generated from mashr. \n",
    "# Default will be used if it does not exist.\n",
    "parameter: mixture_prior = path()\n",
    "parameter: max_L = 20\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 0.1\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.0\n",
    "# MAC cutoff, on top of MAF cutoff\n",
    "# Here I set default to mac = 10 rather than using an MAF cutoff\n",
    "# I don't set it to 5 because I'm not so sure of performance of SuSiE on somewhat infrequent variants\n",
    "# MAC = 10 would not be too infrequenty for xQTL data where sample size is about ~1,000 at most (as of 2022)\n",
    "parameter: mac = 10\n",
    "# Remove indels if indel = False\n",
    "parameter: indel = True\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# Check if both 'data' and 'meta_info' are empty lists\n",
    "stop_if(len(regional_data['data']) == 0, f'Either genotype or phenotype data are not available for region {\", \".join(region_name)}.')\n",
    "\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[0]}.susie_fitted.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "   \n",
    "    get_prior_indices <- function(Y, U) {\n",
    "      # make sure the prior col/rows match the colnames of the Y matrix\n",
    "      y_names = colnames(Y)\n",
    "      u_names = colnames(U)\n",
    "      if (is.null(y_names) || is.null(u_names)) {\n",
    "          return(NULL)\n",
    "      } else if (identical(y_names, u_names)) {\n",
    "          return(NULL)\n",
    "      } else {\n",
    "          return(match(y_names, u_names))\n",
    "      }\n",
    "    }\n",
    "\n",
    "    # Load regional association data\n",
    "    fdat = load_regional_multivariate_data(genotype = ${_input[0]:anr},\n",
    "                                          phenotype = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[1::2]])}),\n",
    "                                          covariate = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[2::2]])}),\n",
    "                                          region = ${'\"%s:%s-%s\"' % (_meta_info[1], _meta_info[2], _meta_info[3])},\n",
    "                                          maf_cutoff = ${maf},\n",
    "                                          mac_cutoff = ${mac},\n",
    "                                          imiss_cutoff = ${imiss},\n",
    "                                          keep_indel = ${\"TRUE\" if indel else \"FALSE\"})\n",
    "\n",
    "    # univariate summary statistics\n",
    "    non_missing = lapply(1:ncol(fdat$residual_Y), function(r)) which(!is.na(fdat$residual_Y[,r]))\n",
    "    univariate_res = lapply(1:ncol(fdat$residual_Y), function(r) susieR:::univariate_regression(X[non_missing[[r]], ], fdat$residual_Y[non_missing[[r]], r]))\n",
    "    sumstat = list(bhat=do.call(cbind, lapply(1:ncol(fdat$residual_Y), function(r) univariate_res[[r]]$betahat)),\n",
    "                   sbhat=do.call(cbind, lapply(1:ncol(fdat$residual_Y), function(r) univariate_res[[r]]$sebetahat)))\n",
    "  \n",
    "    # Multivariate fine-mapping\n",
    "    # FIXME: handle it when prior does not exist\n",
    "    prior = readRDS(${mixture_prior:r})\n",
    "    print(paste(\"Number of components in the mixture prior:\", length(prior$U)))\n",
    "    prior = mvsusieR::create_mash_prior(mixture_prior=list(weights=prior$w, matrices=prior$U), include_indices = get_prior_indices(fdat$residual_Y, prior$U[[1]]), max_mixture_len=-1)   \n",
    "    resid_Y = compute_cov_flash(fdat$residual_Y)\n",
    "    st = proc.time()\n",
    "    fitted = mvsusieR::mvsusie(fdat$X, \n",
    "                               fdat$residual_Y, \n",
    "                               L=${max_L}, \n",
    "                               prior_variance=prior, \n",
    "                               residual_variance=resid_Y, \n",
    "                               precompute_covariances=F, \n",
    "                               compute_objective=T, \n",
    "                               estimate_residual_variance=F, \n",
    "                               estimate_prior_variance=T, \n",
    "                               estimate_prior_method='EM',\n",
    "                               max_iter = 200, \n",
    "                               n_thread=${numThreads}, \n",
    "                               approximate=F)\n",
    "    fitted$analysis_time = proc.time() - st\n",
    "    fitted$cs_corr = susieR::get_cs_correlation(fitted, X=fdat$X)\n",
    "    fitted$cs_snps = names(fitted$X_column_scale_factors[unlist(fitted$sets$cs)])\n",
    "    fitted$variable_name = names(fitted$pip)\n",
    "    fitted$analysis_script = load_script()\n",
    "    fitted$dropped_samples = fdat$dropped_sample\n",
    "    fitted$sample_names = colnames(fdat$residual_Y)\n",
    "    fitted$residual_y = resid_Y\n",
    "    saveRDS(fitted, ${_output:ar})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### mr.mash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mrmash_1]\n",
    "# Prior model file generated from mashr. \n",
    "# Default will be used if it does not exist.\n",
    "parameter: mixture_prior = path()\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 0.1\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.05\n",
    "# MAC cutoff, on top of MAF cutoff\n",
    "parameter: mac = 0\n",
    "# Remove indels if indel = False\n",
    "parameter: indel = True\n",
    "# Path to prior grid data file: an RDS file with scaling factors\n",
    "parameter: prior_grid = path('.')\n",
    "# Path to prior weights data file: an RDS file with prior weights\n",
    "parameter: prior_weights = path('.')\n",
    "# Path to summary statistics directory\n",
    "parameter: sumstats_file = path('.')\n",
    "# Path to sample partition for cross validation\n",
    "parameter: sample_partition = path('.')\n",
    "parameter: fold = 1\n",
    "parameter: var_cutoff = 0.05\n",
    "parameter: n_nonmiss_Y = 100\n",
    "parameter: canonical_mats = False\n",
    "parameter: standardize = True\n",
    "parameter: update_w0 = True\n",
    "parameter: w0_threshold = 0.0\n",
    "parameter: update_V = True\n",
    "parameter: update_V_method = \"full\"\n",
    "parameter: B_init_method = \"enet\"\n",
    "parameter: max_iter = 5000\n",
    "parameter: tol = 1e-2\n",
    "parameter: verbose = False\n",
    "parameter: save_model = False\n",
    "parameter: glmnet_pred = False\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# Check if both 'data' and 'meta_info' are empty lists\n",
    "stop_if(len(regional_data['data']) == 0, f'Either genotype or phenotype data are not available for region {\", \".join(region_name)}.')\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}_fold_{fold}/{name}.{_meta_info[0]}.mrmash.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(pecotmr)\n",
    "    # Load regional association data\n",
    "    fdat = load_regional_multivariate_data(genotype = ${_input[0]:anr},\n",
    "                                          phenotype = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[1::2]])}),\n",
    "                                          covariate = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[2::2]])}),\n",
    "                                          region = \"${_meta_info[0]}\",\n",
    "                                          conditions = c('${_meta_info[2]}'),\n",
    "                                          maf_cutoff = ${maf},\n",
    "                                          mac_cutoff = ${mac},\n",
    "                                          xvar_cutoff = ${var_cutoff},\n",
    "                                          imiss_cutoff = ${imiss},\n",
    "                                          matrix_y_min_complete = ${n_nonmiss_Y},\n",
    "                                          keep_indel = ${\"TRUE\" if indel else \"FALSE\"})\n",
    "    res = mrmash_wrapper(fdat$X, fdat$residual_Y, # here, X and Y are both matrices. X does not have missing data, Y may have it. Y is residualized (covariates removed) X is not.\n",
    "                            ${sumstats_file:ar}, \n",
    "                            ${prior_matrices:ar}),\n",
    "                            ${prior_weights:ar},\n",
    "                            ${prior_grid:ar},\n",
    "                            ${sample_partition:r}, # if available, will perform CV; otherwise will not perform CV\n",
    "                            fold = ${fold},\n",
    "                            nthreads = ${nthreads},\n",
    "                            canonical_mats = ${\"T\" if canonical_mats else \"F\"},\n",
    "                            standardize = ${\"T\" if standardize else \"F\"},\n",
    "                            update_w0 = ${\"T\" if update_w0 else \"F\"},\n",
    "                            w0_threshold = ${w0_threshold},\n",
    "                            update_V = ${\"T\" if update_V else \"F\"},\n",
    "                            update_V_method = \"${update_V_method}\",\n",
    "                            B_init_method = \"${B_init_method}\",\n",
    "                            max_iter = ${max_iter},\n",
    "                            tol = ${tol},\n",
    "                            verbose = ${\"T\" if verbose else \"F\"},\n",
    "                            save_model = ${\"T\" if save_model else \"F\"},\n",
    "                            glmnet_pred = ${\"T\" if glmnet_pred else \"F\"}\n",
    "                          )\n",
    "    st = proc.time()\n",
    "    res$analysis_time = proc.time() - st\n",
    "    res$analysis_script = load_script()\n",
    "    res$dropped_samples = fdat$dropped_sample\n",
    "    res$sample_names = colnames(fdat$residual_Y)\n",
    "    saveRDS(res, ${_output:ar}, compress='xz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Functional regression fSuSiE for epigenomic QTL fine-mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fsusie_1]\n",
    "# initial number of single effects for SuSiE\n",
    "parameter: init_L = 8\n",
    "# maximum number of single effects to use for SuSiE\n",
    "parameter: max_L = 20\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "# here we don't remove any because we have done QC before\n",
    "parameter: imiss = 1.0\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.005\n",
    "# MAC cutoff, on top of MAF cutoff\n",
    "parameter: mac = 5\n",
    "# Remove indels if indel = False\n",
    "parameter: indel = True\n",
    "parameter: pip_cutoff = 0.025\n",
    "parameter: coverage = [0.95, 0.7, 0.5]\n",
    "# prior can be either of [\"mixture_normal\", \"mixture_normal_per_scale\"]\n",
    "parameter: prior = \"mixture_normal\"\n",
    "parameter: max_SNP_EM = 100\n",
    "# Max scale is such that 2^max_scale being the number of phenotypes in the transformed space. Default to 2^10  = 1024. Don't change it unless you know what you are doing. Max_scale should be at least larger than 5.\n",
    "parameter:  max_scale = 10\n",
    "# Purity and coverage used to call cs\n",
    "parameter:  min_purity = 0.5\n",
    "# Epigenetics mark filter\n",
    "parameter: epigenetics_mark_treshold = 16\n",
    "# Run susie for top pc of the fsusie input\n",
    "parameter: run_susie_top_pc = False\n",
    "# Compute TWAS weights as well\n",
    "parameter: twas_weights = True\n",
    "# Perform K folds valiation CV for TWAS\n",
    "# Set it to zero if this is to be skipped\n",
    "parameter: twas_cv_folds = 5\n",
    "parameter: twas_cv_threads = twas_cv_folds\n",
    "# maximum number of variants to consider for CV\n",
    "# We will randomly pick a subset of it for CV purpose\n",
    "parameter: max_cv_variants = 5000\n",
    "# Further limit CV to only using common variants\n",
    "parameter: min_cv_maf = 0.05\n",
    "parameter: ld_reference_meta_file = path()\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# Check if both 'data' and 'meta_info' are empty lists\n",
    "stop_if(len(regional_data['data']) == 0, f'Either genotype or phenotype data are not available for region {\", \".join(region_name)}.')\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[0]}.fsusie_{prior}{\"_weights_db\" if twas_weights else \"\"}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    options(warn=1)\n",
    "    # extract subset of samples\n",
    "    keep_samples = NULL\n",
    "    if (${\"TRUE\" if keep_samples.is_file() else \"FALSE\"}) {\n",
    "      keep_samples = unlist(strsplit(readLines(${keep_samples:ar}), \"\\\\s+\"))\n",
    "      message(paste(length(keep_samples), \"samples are selected to be loaded for analysis\"))\n",
    "    }\n",
    "\n",
    "    # Load regional functional data\n",
    "    library(pecotmr)\n",
    "    tryCatch({\n",
    "    fdat = load_regional_functional_data(genotype = ${_input[0]:anr},\n",
    "                                          phenotype = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[1:len(_input)//2+1]])}),\n",
    "                                          covariate = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[len(_input)//2+1:]])}),\n",
    "                                          region = \"${_meta_info[0]}\",\n",
    "                                          cis_window = \"${_meta_info[1]}\",\n",
    "                                          conditions = c(${\",\".join(['\"%s\"' % x for x in _meta_info[4:]])}),\n",
    "                                          maf_cutoff = ${maf},\n",
    "                                          mac_cutoff = ${mac},\n",
    "                                          imiss_cutoff = ${imiss},\n",
    "                                          keep_indel = ${\"TRUE\" if indel else \"FALSE\"},\n",
    "                                          keep_samples = keep_samples,\n",
    "                                          tabix_header = TRUE,\n",
    "                                          phenotype_header = 4,\n",
    "                                          region_name_col = 4,\n",
    "                                          scale_residuals = FALSE)\n",
    "    }, NoSNPsError = function(e) {\n",
    "        message(\"Error: \", paste(e$message, \"${_meta_info[2] + '@' + _meta_info[1]}\"))\n",
    "        #saveRDS(NULL, ${_output:ar})\n",
    "        saveRDS(list(\"${_meta_info[0]}\" = e$message), ${_output:ar}, compress='xz')\n",
    "        quit(save=\"no\")\n",
    "    })\n",
    "    # Filter out list fdat that with less than a treshold of epigenomic marker.\n",
    "    library(tidyverse)\n",
    "    filter_fdat_except_specific_names <- function(fdat, n) {\n",
    "        # Identify which elements in list1 meet the row count criteria\n",
    "        indices_to_keep <- sapply(fdat$Y_coordinates, function(x) nrow(x) >= n)\n",
    "        fdat_filtered <- map(fdat[!names(fdat) %in% c(\"dropped_sample\", \"X\", \"chrom\")],~.x[indices_to_keep]) \n",
    "        return(c(fdat_filtered,fdat[names(fdat) %in% c(\"dropped_sample\", \"X\", \"chrom\")]))\n",
    "    }\n",
    "\n",
    "    fdat = filter_fdat_except_specific_names(fdat, n = ${epigenetics_mark_treshold})\n",
    "    # Check if Y_coordinates is empty after filtering\n",
    "    if (length(fdat$Y_coordinates) == 0) {\n",
    "        e_msg = paste0(\"None of the study have more than or equal to \",${epigenetics_mark_treshold}, \" epigenetics marks, region skipped\")\n",
    "        message(e_msg)\n",
    "        saveRDS(list(\"${_meta_info[0]}\" = e_msg ),  ${_output:ar}, compress='xz')\n",
    "        quit(save=\"no\")\n",
    "    }\n",
    "\n",
    "    if (${\"TRUE\" if save_data else \"FALSE\"}) {\n",
    "      # save data object for debug purpose\n",
    "      saveRDS(list(\"${_meta_info[0]}\" = fdat), \"${_output:ann}.${epigenetics_mark_treshold}_marks.dataset.rds\", compress='xz')\n",
    "    }\n",
    "  \n",
    "    fitted = setNames(replicate(length(fdat$residual_Y), list(), simplify = FALSE), names(fdat$residual_Y))\n",
    "    for (r in 1:length(fitted)) {\n",
    "        st = proc.time()\n",
    "        fitted[[r]] = list()\n",
    "        message(paste(\"Dimension of Y matrix is \", nrow(fdat$residual_Y[[r]]), \"rows by\", ncol(fdat$residual_Y[[r]]), \"columns.\"))\n",
    "        \n",
    "        # Get top PC data\n",
    "        top_pc_data <- prcomp(fdat$residual_Y[[r]], center = TRUE, scale. = TRUE)$x[,1]\n",
    "        \n",
    "        # Run SuSiE on top PC\n",
    "        if(${\"TRUE\" if (run_susie_top_pc or twas_weights) else \"FALSE\"}) {\n",
    "            fitted[[r]]$susie_on_top_pc <- susie_wrapper(fdat$residual_X[[r]], top_pc_data, init_L=${init_L}, max_L=${max_L}, refine=TRUE, coverage = ${coverage[0]})\n",
    "            fitted[[r]]$susie_on_top_pc <- susie_post_processor(fitted[[r]]$susie_on_top_pc, fdat$residual_X[[r]], top_pc_data, fdat$residual_X_scalar[[r]], 1, fdat$maf[[r]],\n",
    "                                           secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff},\n",
    "                                           other_quantities = list(dropped_samples = list(X=fdat$dropped_sample$dropped_samples_X[[r]], \n",
    "                                                                   y=fdat$dropped_sample$dropped_samples_Y[[r]], \n",
    "                                                                   covar=fdat$dropped_sample$dropped_samples_covar[[r]])))\n",
    "        }\n",
    "        \n",
    "        # Run TWAS weights on top PC\n",
    "        # Exactly the same codes copied from susie_twas\n",
    "        if ( ${\"TRUE\" if twas_weights else \"FALSE\"} ) {\n",
    "            twas_weights_output <- twas_weights_pipeline(fdat$residual_X[[r]], top_pc_data, fdat$maf[[r]], susie_fit=fitted[[r]]$susie_on_top_pc$susie_result_trimmed, \n",
    "                                     ld_reference_meta_file = ${('\"%s\"' % ld_reference_meta_file) if not ld_reference_meta_file.is_dir() else \"NULL\"},\n",
    "                                     X_scalar = fdat$residual_X_scalar[[r]], y_scalar = fdat$residual_Y_scalar[[r]],\n",
    "                                     cv_folds = ${twas_cv_folds}, coverage=${coverage[0]}, secondary_coverage=c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff},\n",
    "                                     min_cv_maf=${min_cv_maf}, max_cv_variants=${max_cv_variants}, cv_seed=${seed}, cv_threads=${twas_cv_threads})\n",
    "            # clean up the output database\n",
    "            fitted[[r]] = c(fitted[[r]], twas_weights_output)\n",
    "            fitted[[r]]$twas_weights = lapply(fitted[[r]]$twas_weights, function(x) { rownames(x) <- NULL; return(x) })\n",
    "        }\n",
    "          \n",
    "        # Run fSuSiE -- this can take a while\n",
    "        fitted[[r]]$fsusie_result <- fsusie_wrapper(X = fdat$residual_X[[r]],\n",
    "                                      Y = fdat$residual_Y[[r]],\n",
    "                                      pos=fdat$Y_coordinates[[r]]$start,\n",
    "                                      L=${max_L},\n",
    "                                      prior=\"${prior}\",\n",
    "                                      max_SNP_EM=${max_SNP_EM}, \n",
    "                                      max_scale = ${max_scale},\n",
    "                                      min.purity = ${min_purity},\n",
    "                                      cov_lev = ${coverage[0]})\n",
    "        fitted[[r]]$fsusie_summary <- susie_post_processor(fitted[[r]]$fsusie_result, fdat$residual_X[[r]], top_pc_data, fdat$residual_X_scalar[[r]], 1, fdat$maf[[r]], \n",
    "                                                          secondary_coverage = c(${\",\".join([str(x) for x in coverage[1:]])}), signal_cutoff = ${pip_cutoff},\n",
    "                                                          other_quantities = list(dropped_samples = list(X=fdat$dropped_sample$dropped_samples_X[[r]], y=fdat$dropped_sample$dropped_samples_Y[[r]], \n",
    "                                                                                  covar=fdat$dropped_sample$dropped_samples_covar[[r]])))\n",
    "        fitted[[r]]$fsusie_summary$susie_result_trimmed = NULL\n",
    "        fitted[[r]]$total_time_elapsed = proc.time() - st\n",
    "        fitted[[r]]$region_info = list(region_coord=parse_region(\"${_meta_info[0]}\"), grange=parse_region(\"${_meta_info[1]}\"), region_name=\"${_meta_info[2]}\")\n",
    "        # original data no longer relevant, set to NA to release memory\n",
    "        fdat$residual_X[[r]] <- NA\n",
    "        fdat$residual_Y[[r]] <- NA\n",
    "    }\n",
    "    saveRDS(list(\"${_meta_info[0]}\" = fitted), ${_output:ar}, compress='xz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Functional regression fSuSiE with other modality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mvfsusie_1]\n",
    "parameter: max_L = 30\n",
    "# remove a variant if it has more than imiss missing individual level data\n",
    "parameter: imiss = 0.1\n",
    "# MAF cutoff\n",
    "parameter: maf = 0.0\n",
    "# MAC cutoff, on top of MAF cutoff\n",
    "# Here I set default to mac = 10 rather than using an MAF cutoff\n",
    "# I don't set it to 5 because I'm not so sure of performance of SuSiE on somewhat infrequent variants\n",
    "# MAC = 10 would not be too infrequenty for xQTL data where sample size is about ~1,000 at most (as of 2022)\n",
    "parameter: mac = 10\n",
    "# prior can be either of [\"mixture_normal\", \"mixture_normal_per_scale\"]\n",
    "parameter: prior  = \"mixture_normal_per_scale\"\n",
    "parameter: max_SNP_EM = 1000\n",
    "\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# Check if both 'data' and 'meta_info' are empty lists\n",
    "stop_if(len(regional_data['data']) == 0, f'Either genotype or phenotype data are not available for region {\", \".join(region_name)}.')\n",
    "\n",
    "\n",
    "meta_info = regional_data['meta_info']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[0]}.mvfsusie_{prior}.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    # Load regional association data\n",
    "    fdat = load_regional_association_data(genotype = ${_input[0]:anr},\n",
    "                                          phenotype = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[1::2]])}),\n",
    "                                          covariate = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input[2::2]])}),\n",
    "                                          region = ${'\"%s:%s-%s\"' % (_meta_info[1], _meta_info[2], _meta_info[3])},\n",
    "                                          maf_cutoff = ${maf},\n",
    "                                          mac_cutoff = ${mac},\n",
    "                                          imiss_cutoff = ${imiss})\n",
    "    # Fine-mapping with mvfSuSiE\n",
    "    library(\"mvf.susie.alpha\")\n",
    "    Y = map(fdat$residual_Y, ~left_join(fdat$X[,1]%>%as.data.frame%>%rownames_to_column(\"rowname\"), .x%>%t%>%as.data.frame%>%rownames_to_column(\"rowname\") , by = \"rowname\")%>%select(-2)%>%column_to_rownames(\"rowname\")%>%as.matrix )\n",
    "    fitted <- multfsusie(Y_f = list(Y[[1]],Y[[3]]), \n",
    "                         Y_u = Reduce(cbind, Y[[2]]),\n",
    "                         pos = list(pos1 =fdat$phenotype_coordiates[[1]], pos2 = fdat$phenotype_coordiates[[3]]),\n",
    "                         X=X,\n",
    "                         L=${max_L},\n",
    "                         data.format=\"list_df\")\n",
    "    saveRDS(fitted, ${_output:ar})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "calysto_bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
