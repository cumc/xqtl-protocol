{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "forbidden-ocean",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# GWAS integration: enrichment and colocalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-salon",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This workflow processes fine-mapping results for xQTL, generated by `susie_twas` in the `cis_analysis.ipynb` notebook for cis xQTL, and GWAS fine-mapping results produced by `susie_rss` in the `rss_analysis.ipynb` notebook. It is designed to perform enrichment and colocalization analysis, particularly when fine-mapping results originate from different regions in the case of cis-xQTL and GWAS. The pipeline is capable to integrate and analyze data across these distinct regions. Originally tailored for cis-xQTL and GWAS integration, this pipeline can be applied to other pairwise integrations. An example of such application is in trans analysis, where the fine-mapped regions might be identical between trans-xQTL and GWAS, representing a special case of this broader implementation.\n",
    "\n",
    "## Input\n",
    "\n",
    "Lists of SuSiE fine-mapping output objects, in RDS format, of `class(susie)` in R. \n",
    "\n",
    "- For GWAS the list is meta-data of format: `chr`, `start`, `end`, `study_id`, `file_path` where `file_path` is an RDS file.\n",
    "- For xQTL the list is meta-data of format: `chr`, `start`, `end`, `region_id`, `condition_id`, `file_path` where `file_path` is an RDS file. `condition_id` should be optional -- if that is the case, all conditions inside of the xQTL dataset will be analyzed.\n",
    "\n",
    "## Output\n",
    "\n",
    "1. Enrichment analysis results --- this is a global enrichment estimate that combines all input data\n",
    "2. Colocalization results for regions of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b8d7a2-cfde-4221-9431-0ced7eab6957",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example\n",
    "enrichment\n",
    "```\n",
    "sos run ~/codes/xqtl-pipeline/pipeline/SuSiE_enloc.ipynb xqtl_gwas_enrichment \\\n",
    "--gwas_finemapped_meta_data  gwas_meta.tsv \\\n",
    "--xqtl_meta_data  xqtl_meta.tsv \\\n",
    "--xqtl_finemapping_obj Mic susie_result_trimmed  \\\n",
    "--xqtl_varname_obj Mic variant_names\n",
    "```\n",
    "\n",
    "coloc\n",
    "```\n",
    "sos run ~/codes/xqtl-pipeline/pipeline/SuSiE_enloc.ipynb susie_coloc \\\n",
    "--gwas_finemapped_meta_data  gwas_meta.tsv \\\n",
    "--xqtl_meta_data  xqtl_meta.tsv \\\n",
    "--xqtl_finemapping_obj  susie_result_trimmed  \\\n",
    "--xqtl_varname_obj  variant_names \\\n",
    "--xqtl_region_obj  region_info \\\n",
    "--enrichment_data /mnt/vast/hpc/csg/rf2872/Work/pecotmr/encoloc_test/output/xqtl_meta.gwas_meta.enrichment.txt  \\\n",
    "--ld_meta_file_path /mnt/vast/hpc/csg/data_public/20240120_ADSP_LD_matrix/ld_meta_file.tsv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a6d8d-0a48-48a7-8583-dd8ee1b1c108",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "\n",
    "eg: `gwas_meta.tsv`\n",
    "\n",
    "\n",
    "```\n",
    "chrom    start    end    region_id    file_path\n",
    "8        2000     6000   block1     /mnt/vast/hpc/homes/dmc2245/project/UKBB_GWAS_dev/code/python/output/SuSiE_RSS/study1.8_26225312-27515963.susie_rss.rds\n",
    "8        3000     7000   block2     /mnt/vast/hpc/homes/dmc2245/project/UKBB_GWAS_dev/code/python/output/SuSiE_RSS/study1.8_25007602-26225312.susie_rss.rds\n",
    "```\n",
    "\n",
    "\n",
    "eg: `xqtl_meta.tsv`\n",
    "\n",
    "```\n",
    "chrom    start    end    region_id    condition    file_path\n",
    "8        2000     6000   ENSG00000140090      cohor1:tissue1:eQTL     /mnt/vast/hpc/csg/rf2872/Work/test/susie_test/MWE_2024/Mic_example.ENSG00000092964.susie_weights_db.mod.rds\n",
    "1        3000     7000   ENSG00000030582      cohor1:tissue1:eQTL      /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/rds_files/ROSMAP_eQTL.ENSG00000030582.susie_weights_db.rds, /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/rds_files/ROSMAP_sQTL.ENSG00000030582.susie_weights_db.rds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-implement",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Workdir\n",
    "parameter: cwd = path(\"output\")\n",
    "# A list of file paths for fine-mapped GWAS results. \n",
    "parameter: gwas_finemapped_meta_data = path\n",
    "# A list of file paths for fine-mapped xQTL results. \n",
    "parameter: xqtl_meta_data = path\n",
    "# Optional: if a region list is provide the enrichment analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "# It is required to input the name of the analysis\n",
    "parameter: name = f\"{xqtl_meta_data:bn}.{gwas_finemapped_meta_data:bn}\"\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 200\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5m\"\n",
    "# Memory expected: quite large for enrichment analysis but small for xQTL colocalization\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "parameter: xqtl_finemapping_obj = []\n",
    "parameter: xqtl_varname_obj = []\n",
    "parameter: gwas_finemapping_obj = []\n",
    "parameter: gwas_varname_obj = []\n",
    "parameter: xqtl_region_obj = []\n",
    "parameter: gwas_region_obj = []\n",
    "parameter: gwas_path = ''\n",
    "parameter: qtl_path = ''\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adapt_file_path(file_path, reference_file):\n",
    "    \"\"\"\n",
    "    Adapt a single file path based on its existence and a reference file's path.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): The file path to adapt.\n",
    "    - reference_file (str): File path to use as a reference for adaptation.\n",
    "\n",
    "    Returns:\n",
    "    - str: Adapted file path.\n",
    "\n",
    "    Raises:\n",
    "    - FileNotFoundError: If no valid file path is found.\n",
    "    \"\"\"\n",
    "    reference_path = os.path.dirname(reference_file)\n",
    "\n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(file_path):\n",
    "        return file_path\n",
    "\n",
    "    # Check file name without path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    if os.path.isfile(file_name):\n",
    "        return file_name\n",
    "\n",
    "    # Check file name in reference file's directory\n",
    "    file_in_ref_dir = os.path.join(reference_path, file_name)\n",
    "    if os.path.isfile(file_in_ref_dir):\n",
    "        return file_in_ref_dir\n",
    "\n",
    "    # Check original file path prefixed with reference file's directory\n",
    "    file_prefixed = os.path.join(reference_path, file_path)\n",
    "    if os.path.isfile(file_prefixed):\n",
    "        return file_prefixed\n",
    "\n",
    "    # If all checks fail, raise an error\n",
    "    raise FileNotFoundError(f\"No valid path found for file: {file_path}\")\n",
    "\n",
    "def adapt_file_path_all(df, column_name, reference_file):\n",
    "    return df[column_name].apply(lambda x: adapt_file_path(x, reference_file))\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885fc43-2777-4e2c-9345-e3fa1f1911d3",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[get_analysis_regions: shared = \"regional_data\"]\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "from collections import OrderedDict\n",
    "\n",
    "def check_required_columns(df, required_columns):\n",
    "    \"\"\"Check if the required columns are present in the dataframe.\"\"\"\n",
    "    missing_columns = [col for col in required_columns if col not in list(df.columns)]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "def extract_regional_data(gwas_meta_data, xqtl_meta_data):\n",
    "    \"\"\"\n",
    "    Extracts fine-mapped results data from GWAS and xQTL metadata files and additional GWAS data provided. \n",
    "\n",
    "    Args:\n",
    "    - gwas_meta_data (str): File path to the GWAS metadata file.\n",
    "    - xqtl_meta_data (str): File path to the xQTL weight metadata file.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of two dictionaries:\n",
    "        - GWAS Dictionary: Nested dictionary with region IDs as keys\n",
    "        - xQTL Dictionary: Nested dictionary with region IDs as keys.\n",
    "    \"\"\"\n",
    "    required_columns = [ '#chr', 'start', 'end', 'region_id','TSS','original_data','combined_data','combined_data_sumstats','conditions','conditions_top_loci']\n",
    "\n",
    "    # Process xQTL metadata\n",
    "    xqtl_df = pd.read_csv(xqtl_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(xqtl_df, required_columns)\n",
    "    xqtl_df = xqtl_df[xqtl_df['block_top_loci'].notna()]\n",
    "\n",
    "    # Process GWAS metadata\n",
    "    gwas_df = pd.read_csv(gwas_meta_data, sep=\"\\t\")\n",
    "    check_required_columns(gwas_df, required_columns)\n",
    "    gwas_df = gwas_df[gwas_df['region_id'].isin(xqtl_df['block_top_loci'])]\n",
    "    \n",
    "    gwas_dict = OrderedDict()\n",
    "    for _, row in gwas_df.iterrows():\n",
    "        file_paths = [fp.strip() for fp in row['combined_data'].split(',')]\n",
    "        gwas_dict[row['region_id']] = {\"meta_info\": [row['#chr'], row['start'], row['end'], row['region_id']],\n",
    "                                       \"files\": file_paths}\n",
    "\n",
    "    xqtl_dict = OrderedDict()\n",
    "    for _, row in xqtl_df.iterrows():\n",
    "        file_paths = [fp.strip() for fp in row['combined_data'].split(',')]\n",
    "        xqtl_dict[row['region_id']] = {\"meta_info\": [row['#chr'], row['start'], row['end'], row['region_id'], row['conditions']],\n",
    "                                       \"files\": file_paths}\n",
    "    return gwas_dict, xqtl_dict\n",
    "\n",
    "xqtl_df = pd.read_csv(xqtl_meta_data, sep=\"\\t\")\n",
    "xqtl_df = xqtl_df[xqtl_df['block_top_loci'].notna()]\n",
    "\n",
    "gwas_df = pd.read_csv(gwas_finemapped_meta_data, sep=\"\\t\")\n",
    "gwas_df = gwas_df[gwas_df['region_id'].isin(xqtl_df['block_top_loci'])]\n",
    "\n",
    "region_to_combined_data = dict(zip(gwas_df['region_id'], gwas_df['combined_data']))\n",
    "# region_to_start= dict(zip(gwas_df['region_id'], gwas_df['start']))\n",
    "# region_to_end = dict(zip(gwas_df['region_id'], gwas_df['end']))\n",
    "\n",
    "xqtl_df['block_combined_data'] = xqtl_df['block_top_loci'].map(region_to_combined_data)\n",
    "# xqtl_df['block_start'] = xqtl_df['block_top_loci'].map(region_to_start)\n",
    "# xqtl_df['block_end'] = xqtl_df['block_top_loci'].map(region_to_end)\n",
    "if qtl_path == '':\n",
    "    qtl_path = xqtl_meta_data.parent\n",
    "if gwas_path == '':\n",
    "    gwas_path = gwas_finemapped_meta_data.parent\n",
    "xqtl_df['combined_data'] = xqtl_df['combined_data'].apply(lambda x: f\"{qtl_path}/{x}\")\n",
    "xqtl_df['block_combined_data'] = xqtl_df['block_combined_data'].apply(lambda x: f\"{gwas_path}/{x}\")\n",
    "\n",
    "\n",
    "# Assuming df is your dataframe loaded with your data\n",
    "# You would replace the following with: df = pd.read_csv('your_file.csv') or another appropriate pandas read function.\n",
    "\n",
    "# Create a list of unique conditions\n",
    "conditions = pd.unique(xqtl_df['conditions_top_loci'].str.split(',', expand=True).stack().str.strip())\n",
    "\n",
    "# Initialize an empty dataframe to store the results\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "for condition in conditions:\n",
    "    # Select rows where the condition is present in the 'conditions_top_loci' column\n",
    "    mask = xqtl_df['conditions_top_loci'].str.contains(condition)\n",
    "    \n",
    "    # Create a new row for the new dataframe\n",
    "    tmp = pd.DataFrame({\n",
    "        'condition': [condition],\n",
    "        'QTL_exported_data': [','.join(xqtl_df.loc[mask, 'combined_data'])],\n",
    "        'GWAS_exported_data': [','.join(xqtl_df.loc[mask, 'block_combined_data'])]\n",
    "    })\n",
    "\n",
    "    # Append the new row to the new dataframe\n",
    "    new_df = pd.concat([new_df, tmp], ignore_index=True)\n",
    "\n",
    "regional_data = {\n",
    "    'data': [(row['QTL_exported_data'].split(',')) for _, row in new_df.iterrows()],\n",
    "    'conditions': [(f\"{row['condition']}\",*row['GWAS_exported_data'].split(',')) for _, row in new_df.iterrows()]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excess-carnival",
   "metadata": {
    "kernel": "SoS",
    "tags": [],
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "[xqtl_gwas_enrichment]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "meta = regional_data['conditions']\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta\"\n",
    "output: f'{cwd:a}/{name}.{_meta[0]}.enrichment.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "  # RDS files for GWAS data\n",
    "  gwas_finemapped_data = c(${paths([x for x in _meta[1:len(_meta)]]):r,})\n",
    "  # RDS files for xQTL data\n",
    "  xqtl_finemapped_data = c(${paths([x for x in _input]):r,})\n",
    "  result = pecotmr::xqtl_enrichment_wrapper(gwas_files = gwas_finemapped_data, xqtl_files = xqtl_finemapped_data, \n",
    "                                              xqtl_finemapping_obj =  c(\"${_meta[0]}\",${\",\".join(['\"%s\"' % x  for x in xqtl_finemapping_obj]) if len(xqtl_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                              xqtl_varname_obj =   c(\"${_meta[0]}\",${\",\".join(['\"%s\"' % x  for x in xqtl_varname_obj]) if len(xqtl_varname_obj) != 0 else \"NULL\"}), \n",
    "                                              gwas_finemapping_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_finemapping_obj]) if len(gwas_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                              gwas_varname_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_varname_obj]) if len(gwas_varname_obj) != 0 else \"NULL\"}))\n",
    "  writeLines(paste(names(result), unlist(result), sep = \":\"), ${_output:ar})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aggressive-benchmark",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_coloc]\n",
    "depends: sos_variable(\"regional_data\")\n",
    "# depends: f'{cwd:a}/{name}.enrichment.txt'\n",
    "parameter: enrichment_data = path\n",
    "parameter: ld_meta_file_path=path()\n",
    "meta_info = [x[\"meta_info\"] for x in regional_data['xQTL'].values()]\n",
    "xqtl_files = [x[\"files\"] for x in regional_data['xQTL'].values()]\n",
    "input: xqtl_files, group_by = lambda x: group_by_region(x, xqtl_files), group_with = \"meta_info\"\n",
    "output: f'{cwd:a}/{step_name[:-2]}/{name}.{_meta_info[3]}.coloc.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(tidyverse)\n",
    "    library(pecotmr)\n",
    "    pkgs <- list.files(\"/mnt/vast/hpc/homes/rf2872/codes/pecotmr/R\", full.names = TRUE)\n",
    "    for(i in pkgs){\n",
    "        source(i)\n",
    "    }\n",
    "    concat_var <- function(var) {\n",
    "      if (!is.null(var)) {\n",
    "        return(c(con, var))\n",
    "      } else {\n",
    "        return(NULL)\n",
    "      }\n",
    "    }\n",
    "    # RDS files for xQTL data\n",
    "    xqtl_finemapped_datas = c(${paths([x for x in _input]):r,})\n",
    "    chrom = ${_meta_info[0]}\n",
    "    start = ${_meta_info[1]} \n",
    "    end = ${_meta_info[2]}\n",
    "    region = \"${_meta_info[3]}\"\n",
    "    xqtl_condition = \"${_meta_info[4]}\"\n",
    "    gwas_regions = c(${', '.join([f'\"{\":\".join(map(str, meta_info[:3]))}\"' for meta_info in [info['meta_info'] for info in regional_data[\"GWAS\"].values()]])})\n",
    "    gwas_blocks = c(${', '.join([f'\"{\":\".join(map(str, meta_info[3:4]))}\"' for meta_info in [info['meta_info'] for info in regional_data[\"GWAS\"].values()]])})\n",
    "    gwas_paths =c(${', '.join([f'\"{file}\"' for info in regional_data[\"GWAS\"].values() for file in info['files']])})\n",
    "    \n",
    "    # Step 1: find relevant GWAS regions that overlap with the xQTL region of interest\n",
    " \n",
    "    gwas_regions <- gwas_regions %>% strsplit(.,\",\") %>% .[[1]]%>% unlist\n",
    "    overlap_index <- NULL\n",
    "    for (i in 1:length(gwas_regions)) {\n",
    "        print(i)\n",
    "      region <- gwas_regions[i]\n",
    "      split_region <- unlist(strsplit(region, \":\"))\n",
    "      block_chrom <- as.numeric(split_region[1])\n",
    "      block_start <- as.numeric(split_region[2])\n",
    "      block_end <- as.numeric(split_region[3])\n",
    "      if (chrom == block_chrom && (start <= block_end | end >= block_start)) {\n",
    "        overlap_index <- c(overlap_index, i)\n",
    "      }\n",
    "    }\n",
    "\n",
    "    if (!is.null(overlap_index)) {\n",
    "        message(\"The region overlaps with \", c(gwas_blocks[overlap_index]))\n",
    "        gwas_finemapped_data <- gwas_paths[overlap_index]\n",
    "\n",
    "        # Step 2: load enrichment analysis results\n",
    "        # coloc_priors = get_coloc_prior(${enrichment_data:r})\n",
    "        # Function to extract the numeric value for a given parameter name\n",
    "        get_coloc_prior <- function(param_name, lines) {\n",
    "          line <- grep(paste0(param_name, \":\"), lines, value = TRUE)\n",
    "          numeric_part <- as.numeric(gsub(paste0(\".*\", param_name, \":\"), \"\", line))\n",
    "          return(numeric_part)\n",
    "        }\n",
    "\n",
    "        # Extract values for p1, p2, and p12\n",
    "        p1 <- get_coloc_prior(\"p1\", readLines(${enrichment_data:r}))\n",
    "        p2 <- get_coloc_prior(\"p2\", readLines(${enrichment_data:r}))\n",
    "        p12 <- get_coloc_prior(\"p12\", readLines(${enrichment_data:r}))\n",
    "\n",
    "        message(\"Priors are P1:\", p1, \"; p2: \", p2, \"; p12: \", p12)\n",
    "        \n",
    "       # Step 3: Apply colocalization analysis between each condition and GWAS\n",
    "\n",
    "       coloc_res <- list()\n",
    "       for(xqtl_finemapped_data in xqtl_finemapped_datas){\n",
    "         cons <- readRDS(xqtl_finemapped_data)[[1]] %>% names \n",
    "  \n",
    "         \n",
    "         xqtl_finemapping_obj =  c(${f'\"{xqtl_finemapping_obj}\"' if xqtl_finemapping_obj else \"NULL\"}) %>% concat_var\n",
    "         gwas_finemapping_obj =  c(${f'\"{gwas_finemapping_obj}\"' if gwas_finemapping_obj else \"NULL\"}) %>% concat_var\n",
    "         xqtl_varname_obj =  c(${f'\"{xqtl_varname_obj}\"' if xqtl_varname_obj else \"NULL\"})  %>% concat_var\n",
    "         gwas_varname_obj =  c(${f'\"{gwas_varname_obj}\"' if gwas_varname_obj else \"NULL\"})  %>% concat_var\n",
    "         xqtl_region_obj =  c(${f'\"{xqtl_region_obj}\"' if xqtl_region_obj else \"NULL\"})  %>% concat_var\n",
    "         gwas_region_obj =  c(${f'\"{gwas_region_obj}\"' if gwas_region_obj else \"NULL\"})   %>% concat_var\n",
    "          \n",
    "         coloc_res[[con]] <- coloc_wrapper(xqtl_file = xqtl_finemapped_data, gwas_files = gwas_finemapped_data, \n",
    "                                              xqtl_finemapping_obj =  c(${\",\".join(['\"%s\"' % x  for x in xqtl_finemapping_obj]) if len(xqtl_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                              gwas_finemapping_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_finemapping_obj]) if len(gwas_finemapping_obj) != 0 else \"NULL\"}), \n",
    "                                              xqtl_varname_obj =   c(${\",\".join(['\"%s\"' % x  for x in xqtl_varname_obj]) if len(xqtl_varname_obj) != 0 else \"NULL\"}), \n",
    "                                              gwas_varname_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_varname_obj]) if len(gwas_varname_obj) != 0 else \"NULL\"}),\n",
    "                                              xqtl_region_obj =  c(${\",\".join(['\"%s\"' % x for x in xqtl_region_obj]) if len(xqtl_region_obj) != 0 else \"NULL\"}), \n",
    "                                              gwas_region_obj =  c(${\",\".join(['\"%s\"' % x for x in gwas_region_obj]) if len(gwas_region_obj) != 0 else \"NULL\"}),\n",
    "                                              p1 = p1, p2 = p2, p12 = p12)\n",
    "  \n",
    "          if (${\"TRUE\" if ld_meta_file_path.is_file() else \"FALSE\"}) {\n",
    "          coloc_res[[con]] <- coloc_post_processor(coloc_res[[con]], LD_meta_file_path = ${ld_meta_file_path:r}, analysis_region= coloc_res[[con]]$analysis_region)\n",
    "          }\n",
    "\n",
    "        \n",
    "     \n",
    "      }\n",
    "    } else {\n",
    "      print(\"No overlap found\")\n",
    "      coloc_res <-  \"No overlap found\"\n",
    "    }\n",
    "    saveRDS(coloc_res, ${_output:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "SoS",
     "sos",
     "",
     ""
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
