{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "wrong-encounter",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Genotype VCF file quality control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-charger",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This implements some recommendations from UK Biobank on [exome sequence data quality control](https://www.medrxiv.org/content/10.1101/2020.11.02.20222232v1.full-text)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-christianity",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this module is to perform QC on VCF files, including \n",
    "\n",
    "1. Handling the formatting of multi-allelic sites, \n",
    "2. Genotype and variant level filtering based on genotype calling qualities. \n",
    "3. Known/novel variants annotation\n",
    "4. Summary statistics before and after QC, in particular the ts/tv ratio, to assess the effectiveness of QC.\n",
    "\n",
    "3 and 4 above are for explorative analysis on the overall quality assessment of genotype data in the VCF files. We annotate known and novel variants because ts/tv are expected to be different between known and novel variants, and is important QC metric to assess the effectiveness of our QC.\n",
    "\n",
    "### Multi-allelic sites\n",
    "\n",
    "Mult-allelic sites can be problematic in many ways for downstreams analysis, even of they are handled in terms of formatting after QC. We provide an optional workflow module to keep only bi-allelic sites from data, although by default we will include these sites in the VCF file we generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-species",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Default VCF QC filters\n",
    "\n",
    "\n",
    "1. Genotype depth filters: For WES data, UK Biobank recommends **SNPs DP>10 and Indels DP>10 for indels.** However we think for WGS we can be less stringent, or simply rely on GQ. Users can set it to 1 eg, `--DP 1 --DP-indel 1 `\n",
    "2. Genotype quality GQ>20.\n",
    "3. At least one sample per site passed the allele balance threshold >= 0.15 for SNPs and >=0.20 for indels (heterozygous variants). \n",
    "    - Allele balance is calculated for heterozygotes as the number of bases supporting the least-represented allele over the total number of base observations.\n",
    "\n",
    "Filtering are done with `bcftools`. Here is a [useful cheatsheet from github user @elowy01](https://gist.github.com/elowy01/93922762e131d7abd3c7e8e166a74a0b).\n",
    "\n",
    "## A note on TS/TV summary from VCF genotype data\n",
    "\n",
    "`bcftools stats` command provides useful summary statistics including TS/TV ratio, which is routinely used as a quality measure of variant calls. With dbSNP based annotation of novel and known variants, `bcftools` can compute TS/TV for novel and known variants at variant level, and at sample level. It should be noted that variant level TS/TV does not take sample genotype into consideration -- it simply counts the TS and TV event for observed SNPs in the data. Other tools, such as `snpsift`, implements variant level TS/TV by counting TS and TV events in sample genotypes and compute the ratio after summing up TS and TV across all samples. See [here](https://github.com/samtools/bcftools/issues/1526) some discussions on this issue. We provide these TS/TV calculations before and after QC but users should be aware of the difference when interpreting the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-provincial",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "a list \n",
    "1.1. The target `VCF` file\n",
    "    - If its chromosome name does not have the `chr` prefix and you need it to match with reference `fasta` file, please run `rename_chrs` workflow to add `chr`.\n",
    "2. dbSNP database in `VCF` format\n",
    "3. A reference sequence `fasta` file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-database",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "1. QC-ed genotype data in VCF and in PLINK format\n",
    "2. A set of sumstats to help evaluate quality of genotype before and after QC\n",
    "    - Particularly useful is the TS/TV ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-gibraltar",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "The MWE is generated via \n",
    "```\n",
    "bcftools query -l get-dosage.ALL.vcf.gz | head -40 > MWE_sample_list\n",
    "bcftools view -S MWE_sample_list  get-dosage.ALL.vcf.gz > sample_filtered.vcf &\n",
    "bgzip -c sample_filtered.vcf >  sample_filtered.vcf.gz\n",
    "tabix -p vcf sample_filtered.vcf.gz\n",
    "bcftools view --regions chr1 sample_filtered.vcf.gz > chr1_sample_filtered.vcf &\n",
    "cat chr1_sample_filtered.vcf | head -20000 > MWE_genotype.vcf\n",
    "```\n",
    "and was stored here: https://drive.google.com/file/d/1sxxPdPIyKma0mAl8TKwhgyRHlOh0Oyrc/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-silly",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The MWE was used as follows:\n",
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb rename_chrs \\\n",
    "    --genoFile reference_data/00-All.vcf.gz \\\n",
    "    --cwd reference_data --container ./bioinfo.sif\n",
    "```\n",
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb dbsnp_annotate \\\n",
    "    --genoFile reference_data/00-All.add_chr.vcf.gz \\\n",
    "    --cwd reference_data --container ./bioinfo.sif\n",
    "```\n",
    "\n",
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb qc    \\\n",
    "--genoFile data/MWE/MWE_genotype.vcf     \\\n",
    "--dbsnp-variants data/reference_data/00-All.add_chr.variants.gz  \\\n",
    "--reference-genome data/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta   \\\n",
    "--cwd MWE/output/genotype_1 --container ./bioinfo.sif -J 1 -c csg.yml -q csg  &\n",
    "```\n",
    "\n",
    "To produce the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-gravity",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "- Total TS/TV for 19639 known variants before QC: 2.599\n",
    "- Total TS/TV for 19573 known variants after QC: 2.600\n",
    "- There is no novel variants included in the MWE.\n",
    "\n",
    "The Total TS/TV is extracted from the last step of QC. For known variant before QC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "apart-serve",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.599\n"
     ]
    }
   ],
   "source": [
    "grep Ts/Tv MWE_genotype.leftnorm.known_variant.snipsift_tstv | rev | cut -d',' -f1 | rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-senegal",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For known variant after QC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "directed-crystal",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.600\n"
     ]
    }
   ],
   "source": [
    "grep Ts/Tv MWE_genotype.leftnorm.filtered.*_variant.snipsift_tstv | rev | cut -d',' -f1 | rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-spoke",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For novel variant before/after QC, TS/TV is not avaible since no novel_variants presented in the MWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-component",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "grep Ts/Tv MWE_genotype.leftnorm.novel_variant.snipsift_tstv | rev | cut -d',' -f1 | rev\n",
    "grep Ts/Tv MWE_genotype.leftnorm.filtered.novel_variant.snipsift_tstv | rev | cut -d',' -f1 | rev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outside-german",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Running in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-printer",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "\n",
    "```\n",
    "sos run VCF_QC.ipynb qc    \\\n",
    "--genoFile data/mwe/mwe_genotype_list    \\\n",
    "--dbsnp-variants data/reference_data/00-All.add_chr.variants.gz  \\\n",
    "--reference-genome data/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta   \\\n",
    "--cwd MWE/output/genotype_4 --container ./bioinfo.sif -J 3 -c csg.yml -q csg --add_chr  &\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-spoke",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "established-elements",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run VCF_QC.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  rename_chrs\n",
      "  dbsnp_annotate\n",
      "  qc\n",
      "\n",
      "Global Workflow Options:\n",
      "  --genoFile VAL (as path, required)\n",
      "                        input\n",
      "  --cwd VAL (as path, required)\n",
      "                        Workdir\n",
      "  --numThreads 1 (as int)\n",
      "                        Number of threads\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Walltime\n",
      "  --mem 60G\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  rename_chrs:\n",
      "  dbsnp_annotate:\n",
      "  qc_1:                 Handel multi-allelic sites, left normalization of indels\n",
      "                        and add variant ID\n",
      "    Workflow Options:\n",
      "      --dbsnp-variants VAL (as path, required)\n",
      "                        Path to dbSNP variants generated previously\n",
      "      --reference-genome VAL (as path, required)\n",
      "                        Path to fasta file for HG reference genome, eg\n",
      "                        GRCh38_full_analysis_set_plus_decoy_hla.fa\n",
      "      --[no-]bi-allelic (default to False)\n",
      "      --[no-]snp-only (default to False)\n",
      "  qc_2:                 genotype QC\n",
      "    Workflow Options:\n",
      "      --geno-filter 0.1 (as float)\n",
      "                        Maximum missingess per-variant\n",
      "      --DP-snp 10 (as int)\n",
      "                        Sample level QC - read depth (DP) to filter out SNPs\n",
      "                        below this value\n",
      "      --GQ 20 (as int)\n",
      "                        Sample level QC - genotype quality (GQ) of specific\n",
      "                        sample. This measure tells you how confident we are that\n",
      "                        the genotype we assigned to a particular sample is\n",
      "                        correct\n",
      "      --DP-indel 10 (as int)\n",
      "                        Sample level QC - read depth (DP) to filter out indels\n",
      "                        below this value\n",
      "      --AB-snp 0.15 (as float)\n",
      "                        Allele balance for snps\n",
      "      --AB-indel 0.2 (as float)\n",
      "                        Allele balance for indels\n",
      "      --hwe-filter 1e-06 (as float)\n",
      "                        HWE filter\n",
      "  qc_3:\n",
      "  qc_4:\n"
     ]
    }
   ],
   "source": [
    "sos run VCF_QC.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-agency",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optical-display",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# input can either be 1 vcf genoFile, or a list of vcf genoFile.\n",
    "parameter: genoFile = paths\n",
    "# Workdir\n",
    "parameter: cwd = path\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Walltime \n",
    "parameter: walltime = '5h'\n",
    "parameter: mem = '60G'\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f\"{cwd:a}\")\n",
    "parameter: add_chr = False\n",
    "\n",
    "import os\n",
    "def get_genotype_file(geno_file_paths):\n",
    "    #\n",
    "    def valid_geno_file(x):\n",
    "        suffixes = path(x).suffixes\n",
    "        if suffixes[-1] == '.bed':\n",
    "            return True\n",
    "        if len(suffixes)>1 and ''.join(suffixes[-2:]) == \".vcf.gz\":\n",
    "            return True\n",
    "        return False\n",
    "    #\n",
    "    def complete_geno_path(x, geno_file):\n",
    "        if not valid_geno_file(x):\n",
    "            raise ValueError(f\"Genotype file {x} should be VCF (end with .vcf.gz) or PLINK bed file (end with .bed)\")\n",
    "        if not os.path.isfile(x):\n",
    "            # relative path\n",
    "            if not os.path.isfile(f'{geno_file:ad}/' + x):\n",
    "                raise ValueError(f\"Cannot find genotype file {x}\")\n",
    "            else:\n",
    "                x = f'{geno_file:ad}/' + x\n",
    "        return x\n",
    "    # \n",
    "    def format_chrom(chrom):\n",
    "        if chrom.startswith('chr'):\n",
    "            chrom = chrom[3:]\n",
    "        return chrom\n",
    "    # Inputs are either VCF or bed, or a vector of them \n",
    "    if len(geno_file_paths) > 1:\n",
    "        if all([valid_geno_file(x) for x in geno_file_paths]):\n",
    "            return paths(geno_file_paths)\n",
    "        else: \n",
    "            raise ValueError(f\"Invalid input {geno_file_paths}\")\n",
    "    # Input is one genotype file or text list of genotype files\n",
    "    geno_file = geno_file_paths[0]\n",
    "    if valid_geno_file(geno_file):\n",
    "        return paths(geno_file)\n",
    "    else: \n",
    "        units = [x.strip().split() for x in open(geno_file).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "        if all([len(x) == 1 for x in units]):\n",
    "            return paths([complete_geno_path(x[0], geno_file) for x in units])\n",
    "        elif all([len(x) == 2 for x in units]):\n",
    "            genos = dict([(format_chrom(x[0]), path(complete_geno_path(x[1], geno_file))) for x in units])\n",
    "        else:\n",
    "            raise ValueError(f\"{geno_file} should contain one column of file names, or two columns of chrom number and corresponding file name\")\n",
    "        return genos\n",
    "    \n",
    "genoFile = get_genotype_file(genoFile)\n",
    "if add_chr:\n",
    "    genoFile = [f'{x:nn}.add_chr.vcf.gz' for x in genoFile]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-natural",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Annotation of known and novel variants\n",
    "\n",
    "The known variant reference can be downloaded from https://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606_b150_GRCh38p7/VCF/00-All.vcf.gz.\n",
    "\n",
    "The procedure/rationale is [explained in this post](https://hbctraining.github.io/In-depth-NGS-Data-Analysis-Course/sessionVI/lessons/03_annotation-snpeff.html).\n",
    "\n",
    "It takes ~1hr for `rename_chrs` to complete. For every hour it can produce ~13Gb of data, please set the --walltime parameter according to the size of your input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-apache",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[rename_chrs: provides = '{filename}.add_chr.vcf.gz']\n",
    "parameter: walltime = '24h'\n",
    "# This file can be downloaded from https://ftp.ncbi.nlm.nih.gov/snp/organisms//human_9606_b150_GRCh38p7/VCF/00-All.vcf.gz.\n",
    "input: f'{filename}.vcf.gz'\n",
    "output: f'{_input:nn}.add_chr.vcf.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:nn}.stderr', stdout = f'{_output:nn}.stdout'\n",
    "    for i in {1..22} X Y MT; do echo \"$i chr$i\"; done > ${_output:nn}.chr_name_conv.txt\n",
    "    bcftools annotate --rename-chrs ${_output:nn}.chr_name_conv.txt ${_input} -Oz -o ${_output}\n",
    "    tabix -p vcf ${_output}\n",
    "    rm -f ${_output:nn}.chr_name_conv.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-oakland",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[dbsnp_annotate]\n",
    "input: genoFile\n",
    "output: f\"{_input:nn}.variants.gz\"\n",
    "task: trunk_workers = 1, trunk_size=job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    bcftools query  -f'%CHROM\\t%POS\\t%ID\\t%REF\\t%ALT\\n' ${_input}  | \\\n",
    "        awk 'BEGIN{OFS=\"\\t\";} {if (length ($4) > length ($5)) {print $1,$2,$2+ (length ($4) - 1),$3} else {print $1,$2, $2 + (length ($5) -1 ),$3}}' | \\\n",
    "        bgzip -c > ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-canvas",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Genotype QC\n",
    "\n",
    "This step handles multi-allelic sites and annotate variants to known and novel. We add an RS ID to variants in dbSNP. Variants without rsID are considered novel variants. For every hour it can produce ~14Gb of data, please set the --walltime parameter according to the size of your input files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rocky-charleston",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Handel multi-allelic sites, left normalization of indels and add variant ID\n",
    "[qc_1 (variant preprocessing)]\n",
    "parameter: walltime = '24h'\n",
    "# Path to dbSNP variants generated previously\n",
    "parameter: dbsnp_variants = path\n",
    "# Path to fasta file for HG reference genome, eg GRCh38_full_analysis_set_plus_decoy_hla.fa\n",
    "parameter: reference_genome = path\n",
    "parameter: bi_allelic = False\n",
    "parameter: snp_only = False\n",
    "input: genoFile, group_by = 1\n",
    "output: f'{cwd}/{_input:bnn}.{\"leftnorm\" if not bi_allelic else \"biallelic\"}{\".snp\" if snp_only else \"\"}.vcf.gz'\n",
    "task: trunk_workers = 1, trunk_size=job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    # split multiallelic sites into biallelic records \n",
    "    ${'bcftools norm -m-any' if not bi_allelic else 'bcftools view -m2 -M2'} ${'-v snps' if snp_only else \"\"} ${_input} |\\\n",
    "    # when incorrect or missing REF allele is encountered: warn (w), no left normalization is done.\n",
    "    bcftools norm -d exact -N --check-ref w -f ${reference_genome}  -Oz --threads ${numThreads} |\\\n",
    "    bcftools +fill-tags -- -t all,F_MISSING,'VD=sum(DP)' | \\\n",
    "    bcftools annotate -x ID -I +'%CHROM:%POS:%REF:%ALT' | \\\n",
    "    bcftools annotate -a ${dbsnp_variants}  -h <(echo '##INFO=<ID=RSID,Number=1,Type=String,Description=\"dbSNP rsID\">') -c CHROM,FROM,TO,INFO/RSID -Oz --threads ${numThreads} -o ${_output}\n",
    "    echo \"${_output} successfully generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-hughes",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step filter variants based on FILTER PASS, DP and QC, fraction of missing genotypes (all samples), and on HWE, for snps and indels. It will also remove monomorphic sites -- using `bcftools view -c1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accompanied-robert",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# genotype QC\n",
    "[qc_2 (variant level QC)]\n",
    "parameter: walltime = '24h'\n",
    "# Maximum missingess per-variant\n",
    "parameter: geno_filter = 0.1\n",
    "# Sample level QC - read depth (DP) to filter out SNPs below this value\n",
    "# Default to 10, with WES data in mind \n",
    "# But for WGS, setting it to 2 may be fine considering the WGS may have low DP but the GQ filter should be good enough\n",
    "parameter: DP_snp = 10\n",
    "# Sample level QC - genotype quality (GQ) of specific sample. This measure tells you how confident we are that the genotype we assigned to a particular sample is correct\n",
    "parameter: GQ = 20\n",
    "# Sample level QC - read depth (DP) to filter out indels below this value\n",
    "parameter: DP_indel = 10\n",
    "# Allele balance for snps\n",
    "parameter: AB_snp = 0.15\n",
    "# Allele balance for indels\n",
    "parameter: AB_indel = 0.2\n",
    "# HWE filter \n",
    "parameter: hwe_filter = 1e-06\n",
    "output: f\"{_input:nn}.filtered.vcf.gz\"\n",
    "task: trunk_workers = 1, trunk_size=job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    bcftools filter -S . -e '(TYPE=\"SNP\" & (FMT/DP)<${DP_snp} & (FMT/GQ)<${GQ})|(TYPE=\"INDEL\" & (FMT/DP)<${DP_indel} & (FMT/GQ)<${GQ})' ${_input} | \\\n",
    "    bcftools view -c1  | bcftools view -f PASS | \\\n",
    "    bcftools filter -i 'GT=\"hom\" | TYPE=\"snp\" & GT=\"het\" & (FORMAT/AD[*:1])/(FORMAT/AD[*:0] + FORMAT/AD[*:1]) >= ${AB_snp} | TYPE=\"indel\" & GT=\"het\" & (FORMAT/AD[*:1])/(FORMAT/AD[*:0] + FORMAT/AD[*:1]) >= ${AB_indel}' | \\\n",
    "    bcftools filter -i 'F_MISSING<${geno_filter} & HWE>${hwe_filter}' -Oz --threads ${numThreads} -o ${_output}  \n",
    "    echo \"${_output} successfully generated\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-mapping",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Finally we export it to PLINK 1.0 format, **without keeping allele orders**. Notice that PLINK 1.0 format does not allow for dosages. PLINK 2.0 format support it, but it is generally not supported by downstreams data analysis.  \n",
    "\n",
    "In the following code block the option `--vcf-half-call m`  treat half-call as missing.\n",
    "\n",
    "Also, intentionally, `--keep-allele-order` is not applied. The resulting PLINK will lose ref/alt allele information but will go by major/minor allele, as conventionally used in standard PLINK format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-location",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[qc_3 (export to PLINK)]\n",
    "parameter: walltime = '24h'\n",
    "parameter: remove_duplicates = False\n",
    "output: f'{_input:nn}.bed'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    plink2 --vcf ${_input} \\\n",
    "        --vcf-half-call m \\\n",
    "        --vcf-require-gt \\\n",
    "        --allow-extra-chr \\\n",
    "        --freqx gz \\\n",
    "        --make-bed --out ${_output:n} --make-bed --out ${_output:n} ${\"--rm-dup exclude-all\" if remove_duplicates else \"\" }\n",
    "    echo \"${_output} successfully generated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-password",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[qc_4 (genotype data summary statistics)]\n",
    "parameter: walltime = '24h'\n",
    "input: output_from('qc_1'), output_from('qc_2'), group_by = 1\n",
    "output: f\"{cwd}/{_input:bnn}.novel_variant_sumstats\", \n",
    "        f\"{cwd}/{_input:bnn}.known_variant_sumstats\", \n",
    "        f\"{cwd}/{_input:bnn}.novel_variant.snipsift_tstv\",\n",
    "        f\"{cwd}/{_input:bnn}.known_variant.snipsift_tstv\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: container = container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    # Compute summary statistics, including TS/TV\n",
    "    bcftools stats -i 'RSID=\".\"' -v  ${_input} > ${_output[0]}\n",
    "    bcftools stats -i 'RSID!=\".\"' -v  ${_input} > ${_output[1]}\n",
    "    bcftools filter -i 'RSID=\".\"'  ${_input}   | java -jar /opt/snpEff/SnpSift.jar tstv - > ${_output[2]}\n",
    "    bcftools filter -i 'RSID!=\".\"' ${_input}  | java -jar /opt/snpEff/SnpSift.jar tstv - > ${_output[3]}\n",
    "    echo \"${_output} successfully generated\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
