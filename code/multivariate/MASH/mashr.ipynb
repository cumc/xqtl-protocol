{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# MASH analysis pipeline with data-driven prior matrices\n",
    "\n",
    "This notebook is a pipeline written in SoS to run `flashr + mashr` for multivariate analysis described in Urbut et al (2019). This pipeline was last applied to analyze GTEx V8 eQTL data, although it can be used as is to perform similar multivariate analysis for other association studies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "*Version: 2021.02.28 by Gao Wang and Yuxin Zou*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <table class=\"revision_table\">\n",
       "        <tr>\n",
       "        <th>Revision</th>\n",
       "        <th>Author</th>\n",
       "        <th>Date</th>\n",
       "        <th>Message</th>\n",
       "        <tr>\n",
       "        <tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/2570aa68b8c8c1a543c8a98d1cf486575736cbe1/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">2570aa6<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2019-10-03</td>\n",
       "<td>Update and improve data preprocessing pipelines</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/3a02d2d7635f52aa1e81687479a8b782cfdf19a4/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">3a02d2d<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2019-07-11</td>\n",
       "<td>Update mashr version</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/3de66784e79d0b11a9f995ff11fdc7a4fbe12223/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">3de6678<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2019-02-05</td>\n",
       "<td>Add corshrink pipeline implementation</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/80e89a86f9ce380b7f30dc4dd64ef4ec632ef2d0/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">80e89a8<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2019-01-28</td>\n",
       "<td>Add a note on HPC job submission</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/982a1b4cc4d8a14a77587fd55a825bf9ef00391e/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">982a1b4<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2019-01-28</td>\n",
       "<td>Implement new $\\hat{V}$ estimation method (with Yuxin Zou)</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/515e86951d09a0f2c1d4d3efde43882bfb75427e/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">515e869<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-22</td>\n",
       "<td>Add a prompt for empty input posterior</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/ba9b20e5d705b20cda80826989293022452c3101/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">ba9b20e<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-22</td>\n",
       "<td>Add posterior calculation for input 'strong' set</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/49494706a413999201362e72c9e20cbb95351be2/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">4949470<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-22</td>\n",
       "<td>Fix mashr null correlation estimate interface</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/6145b3366b7850eac2e10bdd69634f6482e3359f/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">6145b33<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-21</td>\n",
       "<td>Add --optmethod to configure convex optimization method to use</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/485381c8483bc4b770f8648b325a6386cd1a68d9/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">485381c<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-20</td>\n",
       "<td>Fix mixSQP package name #2</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/c587791ce58a5d393782466ba762a32544edcf0e/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">c587791<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-11-20</td>\n",
       "<td>Use the first cran release of mixSQP in default flashr workflow</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/092e20680cce52f594dd7fb95ee2c6b8ea85f036/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">092e206<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-24</td>\n",
       "<td>Minor edits to documentation</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/4a5f2b78f94619f5759448e1fd0a9ba8fb600cb0/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">4a5f2b7<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-24</td>\n",
       "<td>Add notes to results</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/e653d39708c6ba5b96da27472aed2896232814a8/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">e653d39<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-24</td>\n",
       "<td>Configure posterior computation resources</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/ca56229760bd274d193689dbe9bf3efea8103b65/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">ca56229<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-23</td>\n",
       "<td>Add posterior computation for input data-set</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/7db5500fd4549d39a21360fbcbf689bf5576094f/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">7db5500<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-09-23</td>\n",
       "<td>Notes on GTEx V8 data conversion steps</td></tr><tr><td><a target=\"_blank\" href=\"git@github.com:stephenslab/gtexresults/blob/68d2571e7d6284d02ac42d53e901968546298251/mashr_flashr_workflow.ipynb\"><span class=\"revision_id\">68d2571<span></a></td>\n",
       "<td>Gao Wang</td>\n",
       "<td>2018-05-30</td>\n",
       "<td>Add mashr_flashr workflow</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%revisions -s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "`fastqtl` summary statistics data were obtained from dbGaP (data on CRI at UChicago Genetic Medicine). It has 49 tissues. [more description to come]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Preparing MASH input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Using an established workflow (which takes 33hrs to run on a cluster system as configured by `midway2.yml`; see inside `fastqtl_to_mash.ipynb` for a note on computing environment),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "```\n",
    "INPUT_DIR=/project/compbio/GTEx_dbGaP/GTEx_Analysis_2017-06-05_v8/eqtl/GTEx_Analysis_v8_eQTL_all_associations\n",
    "JOB_OPT=\"-c midway2.yml -q midway2\"\n",
    "sos run workflows/fastqtl_to_mash.ipynb --data-list $INPUT_DIR/FastQTLSumStats.list --common-suffix \".allpairs.txt\" $JOB_OPT\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "As a result of command above I obtained the \"mashable\" data-set in the same format [as described here](https://stephenslab.github.io/gtexresults/gtexdata.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Some data integrity check\n",
    "\n",
    "1. Check if I get the same number of groups (genes) at the end of HDF5 data conversion:\n",
    "\n",
    "```\n",
    "$ zcat Whole_Blood.allpairs.txt.gz | cut -f1 | sort -u | wc -l\n",
    "20316\n",
    "$ h5ls Whole_Blood.allpairs.txt.h5 | wc -l\n",
    "20315\n",
    "```\n",
    "\n",
    "The results agreed on Whole Blood sample (the original data has a header thus one line more than the H5 version). We should be good (since the pipeline reported success for all other files)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Data & job summary\n",
    "\n",
    "The command above took 33 hours on UChicago RCC `midway2`. \n",
    "\n",
    "```\n",
    "[MW] cat FastQTLSumStats.log\n",
    "39832 out of 39832 groups merged!\n",
    "```\n",
    "\n",
    "So we have a total of 39832 genes (union of 49 tissues).\n",
    "\n",
    "```\n",
    "[MW] cat FastQTLSumStats.portable.log\n",
    "15636 out of 39832 groups extracted!\n",
    "```\n",
    "\n",
    "We have 15636 groups without missing data in any tissue. This will be used to train the MASH model.\n",
    "\n",
    "The \"mashable\" data file is `FastQTLSumStats.mash.rds`, 124Mb serialized R file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Multivariate adaptive shrinkage (MASH) analysis of eQTL data\n",
    "\n",
    "Below is a \"blackbox\" implementation of the `mashr` eQTL workflow -- blackbox in the sense that you can run this pipeline as an executable, without thinking too much about it, if you see your problem fits our GTEx analysis scheme. However when reading it as a notebook it is a good source of information to help developing your own `mashr` analysis procedures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Since the submission to biorxiv of Urbut 2017 we have improved implementation of MASH algorithm and made a new R package, [`mashr`](https://github.com/stephenslab/mashr). Major improvements compared to Urbut 2019 are:\n",
    "\n",
    "1. Faster computation of likelihood and posterior quantities via matrix algebra tricks and a C++ implementation.\n",
    "2. Faster computation of MASH mixture via convex optimization.\n",
    "3. Replace `SFA` with `FLASH`, a new sparse factor analysis method to generate prior covariance candidates.\n",
    "4. Improve estimate of residual variance $\\hat{V}$.\n",
    "\n",
    "At this point, the input data have already been converted from the original eQTL summary statistics to a format convenient for analysis in MASH, as a result of running the data conversion pipeline in `fastqtl_to_mash.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Example command:\n",
    "\n",
    "\n",
    "```bash\n",
    "JOB_OPT=\"-j 8\"\n",
    "#JOB_OPT=\"-c midway2.yml -q midway2\"\n",
    "sos run workflows/mashr_flashr_workflow.ipynb mash $JOB_OPT # --data ... --cwd ... --vhat ...\n",
    "```\n",
    "\n",
    "**FIXME: add comments on submitting jobs to HPC. Here we use the UChicago RCC cluster but other users can similarly configure their computating system to run the pipeline on HPC.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Global parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: cwd = path('./mashr_flashr_workflow_output')\n",
    "# Input summary statistics data\n",
    "parameter: data = path(\"fastqtl_to_mash_output/FastQTLSumStats.mash.rds\")\n",
    "# Prefix of output files. If not specified, it will derive it from data.\n",
    "# If it is specified, for example, `--output-prefix AnalysisResults`\n",
    "# It will save output files as `{cwd}/AnalysisResults*`.\n",
    "parameter: output_prefix = ''\n",
    "# Exchangable effect (EE) or exchangable z-scores (EZ)\n",
    "parameter: effect_model = 'EZ'\n",
    "# Identifier of $\\hat{V}$ estimate file\n",
    "# Options are \"identity\", \"simple\", \"mle\", \"vhat_corshrink_xcondition\", \"vhat_simple_specific\"\n",
    "parameter: vhat = 'simple'\n",
    "parameter: mixture_components = ['flash', 'flash_nonneg', 'pca',\"canonical\"]\n",
    "parameter: container = str\n",
    "data = data.absolute()\n",
    "cwd = cwd.absolute()\n",
    "if len(output_prefix) == 0:\n",
    "    output_prefix = f\"{data:bn}\"\n",
    "prior_data = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.prior.rds\")\n",
    "vhat_data = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.V_{vhat}.rds\")\n",
    "mash_model = file_target(f\"{cwd:a}/{output_prefix}.{effect_model}.V_{vhat}.mash_model.rds\")\n",
    "\n",
    "def sort_uniq(seq):\n",
    "    seen = set()\n",
    "    return [x for x in seq if not (x in seen or seen.add(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run mashr_flashr_workflow.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  flash\n",
      "  flash_nonneg\n",
      "  pca\n",
      "  vhat_identity\n",
      "  vhat_simple\n",
      "  vhat_mle\n",
      "  vhat_corshrink_xcondition\n",
      "  vhat_simple_specific\n",
      "  prior\n",
      "  mash\n",
      "  posterior\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd mashr_flashr_workflow_output (as path)\n",
      "  --data fastqtl_to_mash_output/FastQTLSumStats.mash.rds (as path)\n",
      "                        Input summary statistics data\n",
      "  --output-prefix ''\n",
      "                        Prefix of output files. If not specified, it will derive\n",
      "                        it from data. If it is specified, for example,\n",
      "                        `--output-prefix AnalysisResults` It will save output\n",
      "                        files as `{cwd}/AnalysisResults*`.\n",
      "  --effect-model EZ\n",
      "                        Exchangable effect (EE) or exchangable z-scores (EZ)\n",
      "  --vhat mle\n",
      "                        Identifier of $\\hat{V}$ estimate file Options are\n",
      "                        \"identity\", \"simple\", \"mle\",\n",
      "                        \"vhat_corshrink_xcondition\", \"vhat_simple_specific\"\n",
      "  --mixture-components flash flash_nonneg pca (as list)\n",
      "\n",
      "Sections\n",
      "  flash:                Perform FLASH analysis with non-negative factor\n",
      "                        constraint (time estimate: 20min)\n",
      "  flash_nonneg:         Perform FLASH analysis with non-negative factor\n",
      "                        constraint (time estimate: 20min)\n",
      "  pca:\n",
      "    Workflow Options:\n",
      "      --npc 3 (as int)\n",
      "                        Number of components in PCA analysis for prior set to 3\n",
      "                        as in mash paper\n",
      "  vhat_identity:        V estimate: \"identity\" method\n",
      "  vhat_simple:          V estimate: \"simple\" method (using null z-scores)\n",
      "  vhat_mle:             V estimate: \"mle\" method\n",
      "    Workflow Options:\n",
      "      --n-subset 6000 (as int)\n",
      "                        number of samples to use\n",
      "      --max-iter 6 (as int)\n",
      "                        maximum number of iterations\n",
      "  vhat_corshrink_xcondition_1: Estimate each V separately via corshrink\n",
      "    Workflow Options:\n",
      "      --util-script /project/mstephens/gtex/scripts/SumstatQuery.R (as path)\n",
      "                        Utility script\n",
      "      --gene-list . (as path)\n",
      "                        List of genes to analyze\n",
      "  vhat_simple_specific_1: Estimate each V separately via \"simple\" method\n",
      "    Workflow Options:\n",
      "      --util-script /project/mstephens/gtex/scripts/SumstatQuery.R (as path)\n",
      "                        Utility script\n",
      "      --gene-list . (as path)\n",
      "                        List of genes to analyze\n",
      "  vhat_corshrink_xcondition_2, vhat_simple_specific_2: Consolidate Vhat into one\n",
      "                        file\n",
      "    Workflow Options:\n",
      "      --gene-list . (as path)\n",
      "                        List of genes to analyze\n",
      "  prior:                Compute data-driven / canonical prior matrices (time\n",
      "                        estimate: 2h ~ 12h for ~30 49 by 49 matrix mixture)\n",
      "  mash_1:               Fit MASH mixture model (time estimate: <15min for 70K by\n",
      "                        49 matrix)\n",
      "  mash_2:               Compute posterior for the \"strong\" set of data as in\n",
      "                        Urbut et al 2017. This is optional because most of the\n",
      "                        time we want to apply the MASH model learned on much\n",
      "                        larger data-set.\n",
      "    Workflow Options:\n",
      "      --[no-]compute-posterior (default to True)\n",
      "                        default to True; use --no-compute-posterior to disable\n",
      "                        this\n",
      "  posterior:            Apply posterior calculations\n",
      "    Workflow Options:\n",
      "      --mash-model  path(f\"{vhat_data:n}.mash_model.rds\")\n",
      "\n",
      "      --posterior-input  paths()\n",
      "\n",
      "      --posterior-vhat-files  paths()\n",
      "\n",
      "      --data-table-name ''\n",
      "                        eg, if data is saved in R list as data$strong, then when\n",
      "                        you specify `--data-table-name strong` it will read the\n",
      "                        data as readRDS('{_input:r}')$strong\n",
      "      --bhat-table-name Bhat\n",
      "      --shat-table-name Shat\n"
     ]
    }
   ],
   "source": [
    "sos run mashr_flashr_workflow.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Factor analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Perform FLASH analysis with non-negative factor constraint (time estimate: 20min)\n",
    "[flash]\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.flash.rds\"\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_flash(dat, factors=\"default\", remove_singleton=${\"TRUE\" if \"canonical\" in mixture_components else \"FALSE\"}, output_model=\"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Perform FLASH analysis with non-negative factor constraint (time estimate: 20min)\n",
    "[flash_nonneg]\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.flash_nonneg.rds\"\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_flash(dat, factors=\"nonneg\", remove_singleton=${\"TRUE\" if \"canonical\" in mixture_components else \"FALSE\"}, output_model=\"${_output:n}.model.rds\")\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[pca]\n",
    "# Number of components in PCA analysis for prior\n",
    "# set to 3 as in mash paper\n",
    "parameter: npc = 2\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.pca.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_pca(dat, ${npc})\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[canonical]\n",
    "input: data\n",
    "output: f\"{cwd}/{output_prefix}.canonical.rds\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    library(\"mashr\")\n",
    "    dat = readRDS(${_input:r})\n",
    "    dat = mashr::mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    res = mashr::cov_canonical(dat)\n",
    "    saveRDS(res, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Estimate residual variance\n",
    "\n",
    "FIXME: add some narratives here explaining what we do in each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"identity\" method\n",
    "[vhat_identity]\n",
    "input: data\n",
    "output: f'{vhat_data:nn}.V_identity.rds'\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    dat = readRDS(${_input:r})\n",
    "    saveRDS(diag(ncol(dat$random.b)), ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"simple\" method (using null z-scores)\n",
    "[vhat_simple]\n",
    "depends: R_library(\"mashr\")\n",
    "input: data\n",
    "output: f'{vhat_data:nn}.V_simple.rds'\n",
    "task: trunk_workers = 1, walltime = '2h', trunk_size = 1, mem = '8G', cores = 2, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input:r})\n",
    "    vhat = estimate_null_correlation_simple(mash_set_data(dat$random.b, Shat=dat$random.s, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3))\n",
    "    saveRDS(vhat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# V estimate: \"mle\" method\n",
    "[vhat_mle]\n",
    "# number of samples to use\n",
    "parameter: n_subset = 6000\n",
    "# maximum number of iterations\n",
    "parameter: max_iter = 6\n",
    "depends: R_library(\"mashr\")\n",
    "input: data, prior_data\n",
    "output: f'{vhat_data:nn}.V_mle.rds'\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    # choose random subset\n",
    "    set.seed(1)\n",
    "    random.subset = sample(1:nrow(dat$random.b), min(${n_subset}, nrow(dat$random.b)))\n",
    "    random.subset = mash_set_data(dat$random.b[random.subset,], dat$random.s[random.subset,], alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    # estimate V mle\n",
    "    vhatprior = mash_estimate_corr_em(random.subset, readRDS(${_input[1]:r}), max_iter = ${max_iter})\n",
    "    vhat = vhat$V\n",
    "    saveRDS(vhat, ${_output:r})\n",
    "    saveRDS(vhat, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Estimate each V separately via corshrink\n",
    "[vhat_corshrink_xcondition_1]\n",
    "# Utility script\n",
    "parameter: util_script = path('/project/mstephens/gtex/scripts/SumstatQuery.R')\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "fail_if(not util_script.is_file() and len(str(util_script)), msg = 'Please specify valid path for --util-script')\n",
    "genes = sort_uniq([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "\n",
    "depends: R_library(\"CorShrink\")\n",
    "input: data, for_each = 'genes'\n",
    "output: f'{vhat_data:nn}/{vhat_data:bnn}_V_corshrink_{_genes}.rds'\n",
    "task: trunk_workers = 1, walltime = '3m', trunk_size = 500, mem = '3G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    source(${util_script:r})\n",
    "    CorShrink_sum = function(gene, database, z_thresh = 2){\n",
    "      print(gene)\n",
    "      dat <- GetSS(gene, database)\n",
    "      z = dat$\"z-score\"\n",
    "      max_absz = apply(abs(z), 1, max)\n",
    "      nullish = which(max_absz < z_thresh)\n",
    "      # if (length(nullish) < ncol(z)) {\n",
    "        # stop(\"not enough null data to estimate null correlation\")\n",
    "      # }\n",
    "      if (length(nullish) <= 1){\n",
    "        mat = diag(ncol(z))\n",
    "      } else {\n",
    "        nullish_z = z[nullish, ]  \n",
    "        mat = as.matrix(CorShrink::CorShrinkData(nullish_z, ash.control = list(mixcompdist = \"halfuniform\"))$cor)\n",
    "      }\n",
    "      return(mat)\n",
    "    }\n",
    "    V = Corshrink_sum(\"${_genes}\", ${data:r})\n",
    "    saveRDS(V, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Estimate each V separately via \"simple\" method\n",
    "[vhat_simple_specific_1]\n",
    "# Utility script\n",
    "parameter: util_script = path('/project/mstephens/gtex/scripts/SumstatQuery.R')\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "fail_if(not util_script.is_file() and len(str(util_script)), msg = 'Please specify valid path for --util-script')\n",
    "genes = sort_uniq([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "depends: R_library(\"Matrix\")\n",
    "input: data, for_each = 'genes'\n",
    "output: f'{vhat_data:nn}/{vhat_data:bnn}_V_simple_{_genes}.rds'\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1m', trunk_size = 500, mem = '3G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    source(${util_script:r})\n",
    "    simple_V = function(gene, database, z_thresh = 2){\n",
    "      print(gene)\n",
    "      dat <- GetSS(gene, database)\n",
    "      z = dat$\"z-score\"\n",
    "      max_absz = apply(abs(z), 1, max)\n",
    "      nullish = which(max_absz < z_thresh)\n",
    "      # if (length(nullish) < ncol(z)) {\n",
    "        # stop(\"not enough null data to estimate null correlation\")\n",
    "      # }\n",
    "      if (length(nullish) <= 1){\n",
    "        mat = diag(ncol(z))\n",
    "      } else {\n",
    "        nullish_z = z[nullish, ]\n",
    "        mat = as.matrix(Matrix::nearPD(as.matrix(cov(nullish_z)), conv.tol=1e-06, doSym = TRUE, corr=TRUE)$mat)\n",
    "      }\n",
    "      return(mat)\n",
    "    }\n",
    "    V = simple_V(\"${_genes}\", ${data:r})\n",
    "    saveRDS(V, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Consolidate Vhat into one file\n",
    "[vhat_corshrink_xcondition_2, vhat_simple_specific_2]\n",
    "depends: R_library(\"parallel\")\n",
    "# List of genes to analyze\n",
    "parameter: gene_list = path()\n",
    "\n",
    "fail_if(not gene_list.is_file(), msg = 'Please specify valid path for --gene-list')\n",
    "genes = paths([x.strip().strip('\"') for x in open(f'{gene_list:a}').readlines() if not x.strip().startswith('#')])\n",
    "\n",
    "\n",
    "input: group_by = 'all'\n",
    "output: f\"{vhat_data:nn}.V_{step_name.rsplit('_',1)[0]}.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    library(parallel)\n",
    "    files = sapply(c(${genes:r,}), function(g) paste0(c(${_input[0]:adr}), '/', g, '.rds'), USE.NAMES=FALSE)\n",
    "    V = mclapply(files, function(i){ readRDS(i) }, mc.cores = 1)\n",
    "    R = dim(V[[1]])[1]\n",
    "    L = length(V)\n",
    "    V.array = array(as.numeric(unlist(V)), dim=c(R, R, L))\n",
    "    saveRDS(V.array, ${_output:ar})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Compute MASH priors \n",
    "\n",
    "Main reference are our `mashr` vignettes [this for mashr eQTL outline](https://stephenslab.github.io/mashr/articles/eQTL_outline.html) and [this for using FLASH prior](https://github.com/stephenslab/mashr/blob/master/vignettes/flash_mash.Rmd). \n",
    "\n",
    "The outcome of this workflow should be found under `./mashr_flashr_workflow_output` folder (can be configured). File names have pattern `*.mash_model_*.rds`. They can be used to computer posterior for input list of gene-SNP pairs (see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Compute data-driven / canonical prior matrices (time estimate: 2h ~ 12h for ~30 49 by 49 matrix mixture)\n",
    "[prior]\n",
    "depends: R_library(\"mashr\")\n",
    "# if vhat method is `mle` it should use V_simple to analyze the data to provide a rough estimate, then later be refined via `mle`.\n",
    "input: [data, vhat_data if vhat != \"mle\" else f'{vhat_data:nn}.V_simple.rds'] + [f\"{cwd}/{output_prefix}.{m}.rds\" for m in mixture_components]\n",
    "output: prior_data\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 4, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    library(mashr)\n",
    "    rds_files = c(${_input:r,})\n",
    "    dat = readRDS(rds_files[1])\n",
    "    vhat = readRDS(rds_files[2])\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, V=vhat, alpha=${1 if effect_model == 'EZ' else 0}, zero_Bhat_Shat_reset = 1E3)\n",
    "    # setup prior\n",
    "    U = list(XtX = t(mash_data$Bhat) %*% mash_data$Bhat / nrow(mash_data$Bhat))\n",
    "    for (f in rds_files[3:length(rds_files)]) U = c(U, readRDS(f))\n",
    "    U.ed = cov_ed(mash_data, U, logfile=${_output:nr})\n",
    "    # Canonical matrices\n",
    "    U.can = cov_canonical(mash_data)\n",
    "    saveRDS(c(U.ed, U.can), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `mashr` mixture model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Fit MASH mixture model (time estimate: <15min for 70K by 49 matrix)\n",
    "[mash_1]\n",
    "depends: R_library(\"mashr\")\n",
    "input: data, vhat_data, prior_data\n",
    "output: mash_model\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    vhat = readRDS(${_input[1]:r})\n",
    "    U = readRDS(${_input[2]:r})\n",
    "    mash_data = mash_set_data(dat$random.b, Shat=dat$random.s, alpha=${1 if effect_model == 'EZ' else 0}, V=vhat, zero_Bhat_Shat_reset = 1E3)\n",
    "    saveRDS(mash(mash_data, Ulist = U, outputlevel = 1), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Optional posterior computations\n",
    "\n",
    "Additionally provide posterior for the \"strong\" set in MASH input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Compute posterior for the \"strong\" set of data as in Urbut et al 2017.\n",
    "# This is optional because most of the time we want to apply the \n",
    "# MASH model learned on much larger data-set.\n",
    "[mash_2]\n",
    "# default to True; use --no-compute-posterior to disable this\n",
    "parameter: compute_posterior = True\n",
    "# input Vhat file for the batch of posterior data\n",
    "skip_if(not compute_posterior)\n",
    "depends: R_library(\"mashr\")\n",
    "input: data, vhat_data, mash_model\n",
    "output: f\"{cwd:a}/{output_prefix}.{effect_model}.posterior.rds\"\n",
    "\n",
    "task: trunk_workers = 1, walltime = '36h', trunk_size = 1, mem = '4G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\", container = container\n",
    "    library(mashr)\n",
    "    dat = readRDS(${_input[0]:r})\n",
    "    vhat = readRDS(${_input[1]:r})\n",
    "    mash_data = mash_set_data(dat$strong.b, Shat=dat$strong.s, alpha=${1 if effect_model == 'EZ' else 0}, V=vhat, zero_Bhat_Shat_reset = 1E3)\n",
    "    mash_model = readRDS(${_input[2]:ar})\n",
    "    saveRDS(mash_compute_posterior_matrices(mash_model, mash_data), ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Compute MASH posteriors\n",
    "\n",
    "In the GTEx V6 paper we assumed one eQTL per gene and applied the model learned above to those SNPs. Under that assumption, the input data for posterior calculation will be the `dat$strong.*` matrices.\n",
    "It is a fairly straightforward procedure as shown in [this vignette](https://stephenslab.github.io/mashr/articles/eQTL_outline.html).\n",
    "\n",
    "But it is often more interesting to apply MASH to given list of eQTLs, eg, from those from fine-mapping results. In GTEx V8 analysis we obtain such gene-SNP pairs from DAP-G fine-mapping analysis. See [this notebook](https://stephenslab.github.io/gtex-eqtls/analysis/Independent_eQTL_Results.html) for how the input data is prepared. The workflow below takes a number of input chunks (each chunk is a list of matrices `dat$Bhat` and `dat$Shat`) \n",
    "and computes posterior for each chunk. It is therefore suited for running in parallel posterior computation for all gene-SNP pairs, if input data chunks are provided.\n",
    "\n",
    "\n",
    "```\n",
    "JOB_OPT=\"-c midway2.yml -q midway2\"\n",
    "DATA_DIR=/project/compbio/GTEx_eQTL/independent_eQTL\n",
    "sos run workflows/mashr_flashr_workflow.ipynb posterior \\\n",
    "    $JOB_OPT \\\n",
    "    --posterior-input $DATA_DIR/DAPG_pip_gt_0.01-AllTissues/DAPG_pip_gt_0.01-AllTissues.*.rds \\\n",
    "                      $DATA_DIR/ConditionalAnalysis_AllTissues/ConditionalAnalysis_AllTissues.*.rds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Apply posterior calculations\n",
    "[posterior_1]\n",
    "parameter: analysis_units = path\n",
    "regions = [x.replace(\"\\\"\",\"\").strip().split() for x in open(analysis_units).readlines() if x.strip() and not x.strip().startswith('#')]\n",
    "parameter: mash_model = path(f\"{cwd:a}/{output_prefix}.{effect_model}.V_{vhat}.mash_model.rds\")\n",
    "parameter: posterior_input = [path(x[0]) for x in regions]\n",
    "parameter: posterior_vhat_files = paths()\n",
    "# eg, if data is saved in R list as data$strong, then\n",
    "# when you specify `--data-table-name strong` it will read the data as\n",
    "# readRDS('{_input:r}')$strong\n",
    "parameter: data_table_name = ''\n",
    "parameter: bhat_table_name = 'bhat'\n",
    "parameter: shat_table_name = 'sbhat'\n",
    "mash_model = f\"{mash_model:a}\"\n",
    "##  conditions can be excluded if needs arise. If nothing to exclude keep the default 0\n",
    "parameter: exclude_condition = [\"1\",\"3\"]\n",
    "\n",
    "skip_if(len(posterior_input) == 0, msg = \"No posterior input data to compute on. Please specify it using --posterior-input.\")\n",
    "fail_if(len(posterior_vhat_files) > 1 and len(posterior_vhat_files) != len(posterior_input), msg = \"length of --posterior-input and --posterior-vhat-files do not agree.\")\n",
    "for p in posterior_input:\n",
    "    fail_if(not p.is_file(), msg = f'Cannot find posterior input file ``{p}``')\n",
    "\n",
    "depends: R_library(\"mashr\"), mash_model\n",
    "input: posterior_input, group_by = 1\n",
    "output: f\"{cwd}/{_input:bn}.posterior.rds\"\n",
    "task: trunk_workers = 1, walltime = '20h', trunk_size = 1, mem = '20G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    library(mashr)\n",
    "    data = readRDS(\"${_input}\")${('$' + data_table_name) if data_table_name else ''}\n",
    "    if(c(${\",\".join(exclude_condition)})[1] > 0 ){\n",
    "      message(paste(\"Excluding condition ${exclude_condition} from the analysis\"))\n",
    "      data$bhat = data$bhat[,-c(${\",\".join(exclude_condition)})]\n",
    "      data$sbhat = data$sbhat[,-c(${\",\".join(exclude_condition)})]\n",
    "      data$Z = data$Z[,-c(${\",\".join(exclude_condition)})]\n",
    "    }\n",
    "  \n",
    "    vhat = readRDS(\"${vhat_data if len(posterior_vhat_files) == 0 else posterior_vhat_files[_index]}\")\n",
    "    mash_data = mash_set_data(data$${bhat_table_name}, Shat=data$${shat_table_name}, alpha=${1 if effect_model == 'EZ' else 0}, V=vhat, zero_Bhat_Shat_reset = 1E3)\n",
    "    mash_output = mash_compute_posterior_matrices(readRDS(\"${mash_model}\"), mash_data)\n",
    "    mash_output$snps = data$snps\n",
    "    saveRDS(mash_output, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[posterior_2]\n",
    "input: group_by = \"all\"\n",
    "output:f\"{cwd}/mash_output_list\"\n",
    "python: expand = \"$[ ]\", workdir = cwd, stderr = f\"{_output:n}.stderr\", stdout = f\"{_output:n}.stdout\"\n",
    "    import pandas as pd\n",
    "    pd.DataFrame({\"#mash_result\" :  [$[_input:ar,]] }).to_csv(\"$[_output]\",index = False ,header = False, sep = \"t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Posterior results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1. The outcome of the `[posterior]` step should produce a number of serialized R objects `*.batch_*.posterior.rds` (can be loaded to R via `readRDS()`) -- I chopped data to batches to take advantage of computing in multiple cluster nodes. It should be self-explanary but please let me know otherwise.\n",
    "2. Other posterior related files are:\n",
    "    1. `*.batch_*.yaml`: gene-SNP pairs of interest, identified elsewhere (eg. fine-mapping analysis). \n",
    "    2. The corresponding univariate analysis summary statistics for gene-SNPs from `*.batch_*.yaml` are extracted and saved to `*.batch_*.rds`, creating input to the `[posterior]` step.\n",
    "    3. Note the `*.batch_*.stdout` file documents some SNPs found in fine-mapping results but not found in the original `fastqtl` output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0,
    "style": "side"
   },
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
