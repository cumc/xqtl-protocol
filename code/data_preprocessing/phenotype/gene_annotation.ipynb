{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-singles",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Gene coordinate annotation\n",
    "\n",
    "\n",
    "This workflow adds genomic coordinate annotation to gene-level molecular phenotype files generated, eg in GCT format, converting them to `bed` format. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-click",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This pipeline is based on [`pyqtl`, as demonstrated here](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py).\n",
    "\n",
    "### Alternative implementation\n",
    "\n",
    "Previously we use `biomaRt` package in R instead of code from `pyqtl`. The core function calls are:\n",
    "\n",
    "```r\n",
    "    ensembl = useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "    ensembl_df <- getBM(attributes=c(\"ensembl_gene_id\",\"chromosome_name\", \"start_position\", \"end_position\"),mart=ensembl)\n",
    "```\n",
    "\n",
    "We require ENSEMBL version to be specified explicitly in this pipeline. As of 2021 for the Brain xQTL project, we use ENSEMBL version 103."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-anchor",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "1. Molecular phenotype data with the first column being ENSEMBL ID and other columns being sample names. \n",
    "2. GTF for collapsed gene model\n",
    "    - the gene names must be consistent with the GCT matrices (eg ENSG00000000003 vs. ENSG00000000003.1 will not work) \n",
    "3. (Optional) Meta-data to match between sample names in expression data and genotype files\n",
    "    - Required input\n",
    "    - Tab delimited with header\n",
    "    - Only 2 columns: first column is sample name in expression data, 2nd column is sample name in genotype data\n",
    "    - **must contains all the sample name in expression matrices even if they don't existing in genotype data**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-commodity",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "Molecular phenotype data in `bed` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-laugh",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "The MWE is uploaded to the [Google Drive](https://drive.google.com/drive/u/0/folders/1Rv2bWHBbX_tastTh49ToYVDMV6rFP5Wk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-limitation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run gene_annotation.ipynb annotate_coord \\\n",
    "    --cwd output \\\n",
    "    --phenoFile data/MWE.pheno_log2cpm.tsv.gz \\\n",
    "    --annotation-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf \\\n",
    "    --sample-participant-lookup data/sampleSheetAfterQC.txt \\\n",
    "    --container container/rna_quantification.sif --phenotype-id-type gene_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-myrtle",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "after-coaching",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run gene_annotation.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  annotate_coord\n",
      "  annotate_coord_biomart\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        Work directory & output directory\n",
      "  --phenoFile VAL (as path, required)\n",
      "                        Molecular phenotype matrix\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 1 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "\n",
      "Sections\n",
      "  annotate_coord:\n",
      "    Workflow Options:\n",
      "      --annotation-gtf VAL (as path, required)\n",
      "                        gene gtf annotation table\n",
      "      --sample-participant-lookup . (as path)\n",
      "                        A file to map sample ID from expression to genotype,\n",
      "                        must contain two columns, sample_id and participant_id,\n",
      "                        mapping IDs in the expression files to IDs in the\n",
      "                        genotype (these can be the same).\n",
      "      --phenotype-id-type 'gene_id'\n",
      "                        Whether the input data is named by gene_id or gene_name.\n",
      "                        By default it is gene_id, if not, please change it to\n",
      "                        gene_name\n",
      "  annotate_coord_biomart:\n",
      "    Workflow Options:\n",
      "      --ensembl-version VAL (as int, required)\n"
     ]
    }
   ],
   "source": [
    "sos run gene_annotation.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arabic-enemy",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "cd /mnt/mfs/statgen/xqtl_workflow_testing/normalization_r2\n",
    "sos run gene_annotation.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "satisfactory-bedroom",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path\n",
    "#  gene gtf annotation table\n",
    "parameter: annotation_gtf = path\n",
    "# Molecular phenotype matrix\n",
    "parameter: phenoFile = path\n",
    "# Whether the input data is named by gene_id or gene_name. By default it is gene_id, if not, please change it to gene_name\n",
    "parameter: phenotype_id_type = 'gene_id'\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "parameter: container = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75689b-7ddf-4f3a-9e4a-e47ebdd7536a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Region List generation\n",
    "To partitioning the data by genes require a region list file which:\n",
    "    1. have 5 columns: chr,start,end,gene_id,gene_name\n",
    "    2. have the same gene as or less gene than that of the bed file\n",
    "Input:\n",
    "    1. A gtf file used to generated the bed\n",
    "    2. A phenotype bed file, must have a gene_id column indicating the name of genes.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06cd895-342b-4206-8692-795cc39f4c20",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[region_list_generation]\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bnn}.region_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    # get the five column data\n",
    "    bed_template_df_id = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_id\" )\n",
    "    bed_template_df_name = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_name\" )\n",
    "    bed_template_df = bed_template_df_id.merge(bed_template_df_name, on = [\"chr\",\"start\",\"end\"])\n",
    "    # retaining only somatic chromosome\n",
    "    bed_template_df = bed_template_df[bed_template_df.chr.isin([\"chr\" + str(x) for x in (range(1,23))])]\n",
    "    bed_template_df.columns = [\"#chr\",\"start\",\"end\",\"gene_id\",\"gene_name\"]\n",
    "    pheno = pd.read_csv(${_input[0]:r}, sep = \"\\t\")\n",
    "    # Retaining only the genes in the data\n",
    "    region_list = bed_template_df[bed_template_df.${phenotype_id_type}.isin(pheno.gene_id)]\n",
    "    region_list.to_csv(\"${_output}\", sep = \"\\t\",index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-credits",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Implementation using `pyqtl`\n",
    "\n",
    "Implementation based on [GTEx pipeline](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fda214-3ef6-41c8-9ae2-fcb10a8bae63",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-cycling",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord]\n",
    "\n",
    "# A file to map sample ID from expression to genotype, must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path()\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bn}.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "    def prepare_bed(df, bed_template_df, chr_subset=None):\n",
    "        bed_df = pd.merge(bed_template_df, df, left_index=True, right_index=True)\n",
    "        # sort by start position\n",
    "        bed_df = bed_df.groupby('chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "        if chr_subset is not None:\n",
    "            # subset chrs from VCF\n",
    "            bed_df = bed_df[bed_df.chr.isin(chr_subset)]\n",
    "        return bed_df\n",
    "    # Load data\n",
    "    df = pd.read_csv(${_input[0]:ar}, sep='\\t', skiprows=0, index_col=0)\n",
    "    sample_participant_lookup = Path(\"${sample_participant_lookup:a}\")\n",
    "    \n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup.is_file():\n",
    "        sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=0, dtype={0:str,1:str}, squeeze=True)\n",
    "        df.rename(columns=sample_participant_lookup_s.to_dict(), inplace=True)\n",
    "\n",
    "    if sum(qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_id\" ).index.duplicated()) >0:\n",
    "        raise ValueError(f\"GTF file ${_input[1]:ar} needs to be collapsed into gene model by reference data processing module\")\n",
    "         \n",
    "    bed_template_df = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"${phenotype_id_type}\" )\n",
    "\n",
    "    ### Detect duplicated gene_id\n",
    "    dup_count = bed_template_df.groupby(bed_template_df.index).cumcount().astype(str).values\n",
    "    dup_count = pd.Series([f'.{x}'.replace(\".0\",\"\") for x in dup_count])\n",
    "    ### Add surfix to the duplicated gene_id\n",
    "    bed_template_df.index = bed_template_df.index +  dup_count\n",
    "    bed_df = prepare_bed(df, bed_template_df)\n",
    "    qtl.io.write_bed(bed_df, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-george",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Implementation using biomaRt\n",
    "This workflow adds the annotations of chr pos(TSS where start = end -1) and gene_ID to the `bed` file. **This workflow is obsolete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-protocol",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_biomart]\n",
    "parameter: ensembl_version=int\n",
    "\n",
    "input: phenoFile\n",
    "output: f'{cwd:a}/{_input:bn}.bed.gz',\n",
    "        f'{cwd:a}/{_input:bn}.region_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "R:  expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout' ,container = container\n",
    "    library(\"biomaRt\")\n",
    "    library(dplyr)\n",
    "    library(readr)\n",
    "    biomartCacheClear()\n",
    "    gene_exp = readr::read_delim(\"$[_input[0]]\",delim = \"\\t\")\n",
    "    if(\"#chr\" %in% colnames(gene_exp) ){\n",
    "      # need to re-annotate\n",
    "      gene_exp = gene_exp[,4:ncol(gene_exp)]\n",
    "    }\n",
    "    ensembl = useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "    ensembl_df <- getBM(attributes=c(\"ensembl_gene_id\",\"chromosome_name\", \"start_position\", \"end_position\"),mart=ensembl)\n",
    "    my_genes = gene_exp$gene_ID\n",
    "    keep_genes =  my_genes\n",
    "    my_genes_ann = ensembl_df[match(my_genes, ensembl_df$ensembl_gene_id),]%>%filter(chromosome_name%in%1:23)%>%dplyr::rename( \"#chr\" = chromosome_name, \"start\" = start_position, \"end\" = end_position,\"gene_ID\" = ensembl_gene_id)%>%filter(gene_ID!=\"NA\", gene_ID%in%keep_genes)\n",
    "    my_genes_ann%>%select(`#chr`,start,end,gene_ID)%>%write_delim(path = \"$[_output[1]]\",\"\\t\")\n",
    "    my_gene_bed = inner_join(my_genes_ann %>%mutate(end = start + 1) %>%select(`#chr`,start,end,gene_ID),gene_exp,by = \"gene_ID\" )%>%arrange(`#chr`,start) \n",
    "    my_gene_bed%>%readr::write_tsv( path = \"$[_output[0]:n]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")\n",
    "\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "        bgzip -f $[_output[0]:n]\n",
    "        tabix -p bed $[_output[0]] -f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
