{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "healthy-circle",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Bulk RNA-seq counts normalization\n",
    "\n",
    "Quantile normalization of TPM counts, and TMM normalization of read counts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recovered-milan",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "Currently, we have implemented two pipelines for RNA-seq data normalization along the lines of the GTEx V8 workflow:\n",
    "\n",
    "\n",
    "- A. Read counts -> TPM (within sample normalization) -> TPM level QC -> Quantile normalization (between sample normalization) -> inverse normal transformation\n",
    "- B. Read counts -> TMM (via edgeR, between sample normalization) -> inverse normal transformation\n",
    "\n",
    "The GTEx protocol, described [here](https://gtexportal.org/home/documentationPage#staticTextAnalysisMethods), suggests that:\n",
    "\n",
    "1. Genes were selected based on expression thresholds of >0.1 TPM in at least 20% of samples and â‰¥6 reads in at least 20% of samples.\n",
    "2. Expression values were normalized between samples using TMM as implemented in edgeR (Robinson & Oshlack, Genome Biology, 2010 ).\n",
    "3. For each gene, expression values were additionally normalized across samples using an inverse normal transform.\n",
    "\n",
    "In other words, GTEx implemented normalization on the count data using TMM (Pipeline B outlined above) although the TPM QC results were used to select samples and genes. \n",
    "\n",
    "## Caveats\n",
    "\n",
    "A couple of possible improvement over the existing pipeline:\n",
    "\n",
    "1. Should we try [GeTMM](https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2246-7) instead? According to their paper, GeTMM improves intra-sample analysis and is very easy to implement (add one line to TMM code, as shown in [this post](https://www.reneshbedre.com/blog/expression_units.html)). However inter-sample analysis such as DEG performs the same as TMM so perhaps not necessary for eQTL studies.\n",
    "2. Should we control for batch effect if we know the batches explicitly, so we don't rely on hidden factor analysis? What we can do are:\n",
    "    1. Read counts -> Combat-Seq -> inverse normal transformation\n",
    "    2. Do what we already have -> Add a batch adjustment using Combat on normalized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-shopping",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "1. TPM matrix and read count matrix in RNA-SeQC format\n",
    "    - the first two rows should be commented text with `#` prefix.\n",
    "    - the matrix should be tab delimited.\n",
    "    - the matrix files should end with `gct` suffix\n",
    "    - These requirements are satisfied if the inputs are outputs from [`bulk_expression_QC` pipeline](bulk_expression_QC.html).\n",
    "2. GTF for collapsed gene model\n",
    "    - the gene names must be consistent with the GCT matrices (eg ENSG00000000003 vs. ENSG00000000003.1 will not work) \n",
    "    - chromosome names must have `chr` prefix (although we can make it an option in the pipeline, currently we assume the `chr` prefix convention)\n",
    "3. Meta-data to match between sample names in expression data and genotype files\n",
    "    - Required input\n",
    "    - Tab delimited with header\n",
    "    - Only 2 columns: first column is sample name in expression data, 2nd column is sample name in genotype data\n",
    "    - **must contains all the sample name in expression matrices even if they don't existing in genotype data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-repair",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "Normalized expression file in `bed` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "monthly-advisory",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Minimal Working Example\n",
    "\n",
    "Expression matrices can be generated by the MWE of `bulk_expression_QC.ipynb`. A full set of MWE can be found [on Google Drive](https://drive.google.com/drive/u/0/folders/1Rv2bWHBbX_tastTh49ToYVDMV6rFP5Wk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-cleaners",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run bulk_expression_normalization.ipynb normalize \\\n",
    "    --cwd output \\\n",
    "    --tpm-gct data/mwe.low_expression_filtered.outlier_removed.tpm.gct.gz \\\n",
    "    --counts-gct data/mwe.low_expression_filtered.outlier_removed.geneCount.gct.gz \\\n",
    "    --annotation-gtf data/gene.gtf  \\\n",
    "    --sample-participant-lookup data/sampleSheetAfterQC.txt \\\n",
    "    --container containers/rna_quantification.sif \\\n",
    "    --count-threshold 1   # to make the MWE work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-diagnosis",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fewer-niagara",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run bulk_expression_normalization.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  normalize\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        Work directory & output directory\n",
      "  --counts-gct VAL (as path, required)\n",
      "                        gene count table\n",
      "  --tpm-gct VAL (as path, required)\n",
      "                        gene TPM table\n",
      "  --annotation-gtf VAL (as path, required)\n",
      "                        gene gtf annotation table\n",
      "  --sample-participant-lookup VAL (as path, required)\n",
      "                        A file to map sample ID from expression to genotype,must\n",
      "                        contain two columns, sample_id and participant_id,\n",
      "                        mapping IDs in the expression files to IDs in the\n",
      "                        genotype (these can be the same).\n",
      "  --tpm-threshold 0.1 (as float)\n",
      "  --count-threshold 6 (as int)\n",
      "  --sample-frac-threshold 0.2 (as float)\n",
      "  --normalization-method tmm\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 20 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "\n",
      "Sections\n",
      "  normalize:\n"
     ]
    }
   ],
   "source": [
    "sos run bulk_expression_normalization.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aggressive-edgar",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path\n",
    "#  gene count table\n",
    "parameter: counts_gct = path\n",
    "#  gene TPM table\n",
    "parameter: tpm_gct = path\n",
    "#  gene gtf annotation table\n",
    "parameter: annotation_gtf = path\n",
    "# A file to map sample ID from expression to genotype,must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path\n",
    "parameter: tpm_threshold = 0.1\n",
    "parameter: count_threshold = 6\n",
    "parameter: sample_frac_threshold = 0.2\n",
    "# Normalization method: TMM (tmm) or quantile normalization (qn)\n",
    "parameter: normalization_method = 'tmm'\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "parameter: container = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-jaguar",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[normalize]\n",
    "# Path to the input molecular phenotype data, should be a processd and indexed bed.gz file, with tabix index.\n",
    "input: tpm_gct, counts_gct, annotation_gtf, sample_participant_lookup\n",
    "output: f'{cwd:a}/{_input[0]:bnnn}.{normalization_method}.expression.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]:nn}.stderr', stdout = f'{_output[0]:nn}.stdout',container = container\n",
    "    for i in {1..22} X Y MT; do echo chr$i; done > ${_output[0]:bnnn}.vcf_chr_list\n",
    "    eqtl_prepare_expression.py ${_input[0]} ${_input[1]} ${_input[2]} \\\n",
    "        ${_input[3]} ${_output[0]:bnnn}.vcf_chr_list ${_output[0]:nnn} \\\n",
    "        --tpm_threshold ${tpm_threshold} \\\n",
    "        --count_threshold ${count_threshold} \\\n",
    "        --sample_frac_threshold ${sample_frac_threshold} \\\n",
    "        --normalization_method ${normalization_method} && \\\n",
    "    rm -f ${_output[0]:bnnn}.vcf_chr_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
