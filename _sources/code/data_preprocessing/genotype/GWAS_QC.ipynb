{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "gross-treaty",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# Genotype PLINK file quality control\n",
    "\n",
    "This workflow implements some prelimary data QC steps for PLINK input files. VCF format of inputs will be converted to PLINK before performing QC."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "tamil-forestry",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook includes workflow for\n",
    "\n",
    "- Compute kinship matrix in sample and estimate related individuals\n",
    "- Genotype and sample QC: by MAF, missing data and HWE\n",
    "- LD pruning for follow up PCA analysis on genotype, as needed\n",
    "\n",
    "A potential limitation is that the workflow requires all samples and chromosomes to be merged as one single file, in order to perform both sample and variant level QC. However, in our experience using this pipeline with 200K exomes with 15 million variants, this pipeline works on the single merged PLINK file.\n",
    "\n",
    "## Methods\n",
    "\n",
    "Depending on the context of your problem, the workflow can be executed in two ways:\n",
    "\n",
    "1. Run `qc` command to perform genotype data QC and LD pruning to generate a subset of variants in preparation for analysis such as PCA.\n",
    "2. Run `king` first on either the original or a subset of common variants to identify unrelated individuals. The `king` pipeline will split samples to related and unrelated individuals. Then you perform `qc` on these individuals only and finally extract the same set of QC-ed variants for related individuals.\n",
    "\n",
    "## Input format\n",
    "\n",
    "The whole genome PLINK bim/bed/fam bundle. For input in VCF format and/or per-chromosome VCF or PLINK format, please use `vcf_to_plink` and `merge_plink` in [genotype formatting](https://cumc.github.io/xqtl-pipeline/code/data_preprocessing/genotype/genotype_formatting.html) pipeline to convert them to PLINK file bundle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bound-watson",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Default QC parameters\n",
    "\n",
    "- Kinship coefficient for related individuals: 0.0625\n",
    "- MAF default: 0\n",
    "    - Above default includes both common and are variant\n",
    "    - Recommand MAF for PCA: 0.01, [we should stick to common variants](https://bmcgenomdata.biomedcentral.com/articles/10.1186/s12863-020-0833-x)\n",
    "    - Recommand MAC for single variant analysis: 5\n",
    "- Variant level missingness threshold: 0.1\n",
    "- Sample level missingness threshold: 0.1\n",
    "- LD pruning via PLINK for PCA analysis:\n",
    "    - window 50 \n",
    "    - shift 10 \n",
    "    - r2 0.1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "brutal-tower",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "Minimal working example data-set as well as the singularity container `bioinfo.sif` can be downloaded from [Google Drive](https://drive.google.com/drive/u/0/folders/1ahIZGnmjcGwSd-BI91C9ayd_Ya8sB2ed).\n",
    "\n",
    "The `chr1_chr6` data-set was merged from `chr1` and `chr6` data, using `merge_plink` command from [genotype formatting](https://cumc.github.io/xqtl-pipeline/code/data_preprocessing/genotype/genotype_formatting.html) pipeline.\n",
    "\n",
    "\n",
    "### Example 1: perform QC on both rare and common variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-glasgow",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run GWAS_QC.ipynb qc_no_prune \\\n",
    "    --cwd output/genotype \\\n",
    "    --genoFile output/genotype/chr1_chr6.bed \\\n",
    "    --container container/bioinfo.sif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "traditional-grounds",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Example 2: QC common variants in unrelated individuals and extract those variants from related individuals\n",
    "\n",
    "Determine and split between related and unrelated individuals,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-prompt",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run GWAS_QC.ipynb king \\\n",
    "    --cwd output/genotype \\\n",
    "    --genoFile output/genotype/chr1_chr6.bed \\\n",
    "    --name 20220110 \\\n",
    "    --container container/bioinfo.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840744dd-3369-477b-8a7f-108dfc3d5902",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos run pipeline/GWAS_QC.ipynb king \\\n",
    "    --cwd output/ \\\n",
    "    --genoFile MWE.bed \\\n",
    "    --name test_output \\\n",
    "    --container containers/bioinfo.sif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "awful-camping",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "Variant level and sample level QC on unrelated individuals, in preparation for PCA analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-probability",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run GWAS_QC.ipynb qc \\\n",
    "    --cwd output/genotype \\\n",
    "    --genoFile output/genotype/chr1_chr6.20220110.unrelated.bed \\\n",
    "    --maf-filter 0.01 \\\n",
    "    --name for_pca \\\n",
    "    --container container/bioinfo.sif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "enhanced-commissioner",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Extract previously selected variants from related individuals in preparation for PCA, only applying missingness filter at sample level,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-scoop",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run GWAS_QC.ipynb qc_no_prune \\\n",
    "    --cwd output/genotype \\\n",
    "    --genoFile output/genotype/chr1_chr6.20220110.related.bed \\\n",
    "    --keep-variants output/genotype/chr1_chr6.20220110.unrelated.for_pca.filtered.prune.in \\\n",
    "    --maf-filter 0 --geno-filter 0 --mind-filter 0.1 --hwe-filter 0 \\\n",
    "    --name for_pca \\\n",
    "    --container container/bioinfo.sif"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dental-annex",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "supposed-memorabilia",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run GWAS_QC.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  king\n",
      "  qc_no_prune\n",
      "  qc\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        the output directory for generated files\n",
      "  --name ''\n",
      "                        A string to identify your analysis run\n",
      "  --genoFile  paths\n",
      "\n",
      "                        PLINK binary files\n",
      "  --remove-samples . (as path)\n",
      "                        The path to the file that contains the list of samples\n",
      "                        to remove (format FID, IID)\n",
      "  --keep-samples . (as path)\n",
      "                        The path to the file that contains the list of samples\n",
      "                        to keep (format FID, IID)\n",
      "  --keep-variants . (as path)\n",
      "                        The path to the file that contains the list of variants\n",
      "                        to keep\n",
      "  --exclude-variants . (as path)\n",
      "                        The path to the file that contains the list of variants\n",
      "                        to exclude\n",
      "  --kinship 0.0625 (as float)\n",
      "                        Kinship coefficient threshold for related individuals\n",
      "                        (e.g first degree above 0.25, second degree above 0.125,\n",
      "                        third degree above 0.0625)\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 20 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  king_1:               Inference of relationships in the sample to identify\n",
      "                        closely related individuals\n",
      "    Workflow Options:\n",
      "      --kin-maf 0.01 (as float)\n",
      "                        PLINK binary file\n",
      "  king_2:               Select a list of unrelated individual with an attempt to\n",
      "                        maximize the unrelated individuals selected from the\n",
      "                        data\n",
      "    Workflow Options:\n",
      "      --[no-]maximize-unrelated (default to False)\n",
      "                        If set to true, the unrelated individuals in a family\n",
      "                        will be kept without being reported. Otherwise (use\n",
      "                        `--no-maximize-unrelated`) the entire family will be\n",
      "                        removed Note that attempting to maximize unrelated\n",
      "                        individuals is computationally intensive on large data.\n",
      "  king_3:               Split genotype data into related and unrelated samples,\n",
      "                        if related individuals are detected\n",
      "  qc_no_prune, qc_1:    Filter SNPs and select individuals\n",
      "    Workflow Options:\n",
      "      --maf-filter 0.0 (as float)\n",
      "                        minimum MAF filter to use. 0 means do not apply this\n",
      "                        filter.\n",
      "      --maf-max-filter 0.0 (as float)\n",
      "                        maximum MAF filter to use. 0 means do not apply this\n",
      "                        filter.\n",
      "      --mac-filter 0.0 (as float)\n",
      "                        minimum MAC filter to use. 0 means do not apply this\n",
      "                        filter.\n",
      "      --mac-max-filter 0.0 (as float)\n",
      "                        maximum MAC filter to use. 0 means do not apply this\n",
      "                        filter.\n",
      "      --geno-filter 0.1 (as float)\n",
      "                        Maximum missingess per-variant\n",
      "      --mind-filter 0.1 (as float)\n",
      "                        Maximum missingness per-sample\n",
      "      --hwe-filter 1e-15 (as float)\n",
      "                        HWE filter -- a very lenient one\n",
      "      --other-args  (as list)\n",
      "                        Other PLINK arguments e.g snps_only, write-samples, etc\n",
      "      --[no-]meta-only (default to False)\n",
      "                        Only output SNP and sample list, rather than the PLINK\n",
      "                        binary format of subset data\n",
      "      --[no-]rm-dups (default to False)\n",
      "                        Remove duplicate variants\n",
      "  qc_2:                 LD prunning and remove related individuals (both ind of\n",
      "                        a pair) Plink2 has multi-threaded calculation for LD\n",
      "                        prunning\n",
      "    Workflow Options:\n",
      "      --window 50 (as int)\n",
      "                        Window size\n",
      "      --shift 10 (as int)\n",
      "                        Shift window every 10 snps\n",
      "      --r2 0.1 (as float)\n"
     ]
    }
   ],
   "source": [
    "sos run GWAS_QC.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-subsection",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# the output directory for generated files\n",
    "parameter: cwd = path(\"output\")\n",
    "# A string to identify your analysis run\n",
    "parameter: name = \"\"\n",
    "# PLINK binary files\n",
    "parameter: genoFile = paths\n",
    "# The path to the file that contains the list of samples to remove (format FID, IID)\n",
    "parameter: remove_samples = path('.')\n",
    "# The path to the file that contains the list of samples to keep (format FID, IID)\n",
    "parameter: keep_samples = path('.')\n",
    "# The path to the file that contains the list of variants to keep\n",
    "parameter: keep_variants = path('.')\n",
    "# The path to the file that contains the list of variants to exclude\n",
    "parameter: exclude_variants = path('.')\n",
    "# Kinship coefficient threshold for related individuals\n",
    "# (e.g first degree above 0.25, second degree above 0.125, third degree above 0.0625)\n",
    "parameter: kinship = 0.0625\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 20\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "if not container:\n",
    "    container = None\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f\"{cwd:a}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "south-hardware",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Estimate kinship in the sample\n",
    "\n",
    "The output is a list of related individuals, as well as the kinship matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-production",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Inference of relationships in the sample to identify closely related individuals\n",
    "[king_1]\n",
    "# PLINK binary file\n",
    "parameter: kin_maf = 0.01\n",
    "input: genoFile\n",
    "output: f'{cwd}/{_input:bn}{(\".\"+name) if name else \"\"}.kin0'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    plink2 \\\n",
    "      --bfile ${_input:n} \\\n",
    "      --make-king-table \\\n",
    "      --king-table-filter ${kinship} \\\n",
    "      ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} \\\n",
    "      ${('--remove %s' % remove_samples) if remove_samples.is_file() else \"\"} \\\n",
    "      --min-af ${kin_maf} \\\n",
    "      --max-af ${1-kin_maf} \\\n",
    "      --out ${_output:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6} \n",
    "    \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "        stdout=$[_output].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44e38a12-53ef-4662-b6be-1c89cc21edee",
   "metadata": {
    "kernel": "SoS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-hours",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Select a list of unrelated individual with an attempt to maximize the unrelated individuals selected from the data \n",
    "[king_2]\n",
    "# If set to true, the unrelated individuals in a family will be kept without being reported. \n",
    "# Otherwise (use `--no-maximize-unrelated`) the entire family will be removed\n",
    "# Note that attempting to maximize unrelated individuals is computationally intensive on large data.\n",
    "parameter: maximize_unrelated = False\n",
    "output: f'{_input:n}.related_id'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R:  container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    library(dplyr)\n",
    "    library(igraph)\n",
    "    # Remove related individuals while keeping maximum number of individuals\n",
    "    # this function is simplified from: \n",
    "    # https://rdrr.io/cran/plinkQC/src/R/utils.R\n",
    "    #' @param relatedness [data.frame] containing pair-wise relatedness estimates\n",
    "    #' (in column [relatednessRelatedness]) for individual 1 (in column\n",
    "    #' [relatednessIID1] and individual 2 (in column [relatednessIID1]). Columns\n",
    "    #' relatednessIID1, relatednessIID2 and relatednessRelatedness have to present,\n",
    "    #' while additional columns such as family IDs can be present. Default column\n",
    "    #' names correspond to column names in output of plink --genome\n",
    "    #' (\\url{https://www.cog-genomics.org/plink/1.9/ibd}). All original\n",
    "    #' columns for pair-wise highIBDTh fails will be returned in fail_IBD.\n",
    "    #' @param relatednessTh [double] Threshold for filtering related individuals.\n",
    "    #' Individuals, whose pair-wise relatedness estimates are greater than this\n",
    "    #' threshold are considered related.\n",
    "    relatednessFilter <- function(relatedness, \n",
    "                                  relatednessTh,\n",
    "                                  relatednessIID1=\"IID1\", \n",
    "                                  relatednessIID2=\"IID2\",\n",
    "                                  relatednessRelatedness=\"KINSHIP\") {\n",
    "        # format data\n",
    "        if (!(relatednessIID1 %in% names(relatedness))) {\n",
    "            stop(paste(\"Column\", relatednessIID1, \"for relatedness not found!\"))\n",
    "        }\n",
    "        if (!(relatednessIID2 %in% names(relatedness))) {\n",
    "            stop(paste(\"Column\", relatednessIID1, \"for relatedness not found!\"))\n",
    "        }\n",
    "        if (!(relatednessRelatedness %in% names(relatedness))) {\n",
    "            stop(paste(\"Column\", relatednessRelatedness,\n",
    "                       \"for relatedness not found!\"))\n",
    "        }\n",
    "\n",
    "        iid1_index <- which(colnames(relatedness) == relatednessIID1)\n",
    "        iid2_index <- which(colnames(relatedness) == relatednessIID2)\n",
    "\n",
    "        relatedness[,iid1_index] <- as.character(relatedness[,iid1_index])\n",
    "        relatedness[,iid2_index] <- as.character(relatedness[,iid2_index])\n",
    "\n",
    "        relatedness_names <- names(relatedness)\n",
    "        names(relatedness)[iid1_index] <- \"IID1\"\n",
    "        names(relatedness)[iid2_index] <- \"IID2\"\n",
    "        names(relatedness)[names(relatedness) == relatednessRelatedness] <- \"M\"\n",
    "\n",
    "        # Remove symmetric IID rows\n",
    "        relatedness_original <- relatedness\n",
    "        relatedness <- dplyr::select_(relatedness, ~IID1, ~IID2, ~M)\n",
    "\n",
    "        sortedIDs <- data.frame(t(apply(relatedness, 1, function(pair) {\n",
    "            c(sort(c(pair[1], pair[2])))\n",
    "            })), stringsAsFactors=FALSE)\n",
    "        keepIndex <- which(!duplicated(sortedIDs))\n",
    "\n",
    "        relatedness_original <- relatedness_original[keepIndex,]\n",
    "        relatedness <- relatedness[keepIndex,]\n",
    "\n",
    "        # individuals with at least one pair-wise comparison > relatednessTh\n",
    "        # return NULL to failIDs if no one fails the relatedness check\n",
    "        highRelated <- dplyr::filter_(relatedness, ~M > relatednessTh)\n",
    "        if (nrow(highRelated) == 0) {\n",
    "            return(list(relatednessFails=NULL, failIDs=NULL))\n",
    "        }\n",
    "\n",
    "        # all samples with related individuals\n",
    "        allRelated <- c(highRelated$IID1, highRelated$IID2)\n",
    "        uniqueIIDs <- unique(allRelated)\n",
    "\n",
    "        # Further selection of samples with relatives in cohort\n",
    "        multipleRelative <- unique(allRelated[duplicated(allRelated)])\n",
    "        singleRelative <- uniqueIIDs[!uniqueIIDs %in% multipleRelative]\n",
    "\n",
    "        highRelatedMultiple <- highRelated[highRelated$IID1 %in% multipleRelative |\n",
    "                                            highRelated$IID2 %in% multipleRelative,]\n",
    "        highRelatedSingle <- highRelated[highRelated$IID1 %in% singleRelative &\n",
    "                                           highRelated$IID2 %in% singleRelative,]\n",
    "\n",
    "        # Only one related samples per individual\n",
    "        if(length(singleRelative) != 0) {\n",
    "          # randomly choose one to exclude\n",
    "          failIDs_single <- highRelatedSingle[,1]\n",
    "            \n",
    "        } else {\n",
    "          failIDs_single <- NULL\n",
    "        }\n",
    "  \n",
    "        # An individual has multiple relatives\n",
    "        if(length(multipleRelative) != 0) {\n",
    "            relatedPerID <- lapply(multipleRelative, function(x) {\n",
    "                tmp <- highRelatedMultiple[rowSums(\n",
    "                    cbind(highRelatedMultiple$IID1 %in% x,\n",
    "                          highRelatedMultiple$IID2 %in% x)) != 0,1:2]\n",
    "                rel <- unique(unlist(tmp))\n",
    "                return(rel)\n",
    "            })\n",
    "            names(relatedPerID) <- multipleRelative\n",
    "\n",
    "            keepIDs_multiple <- lapply(relatedPerID, function(x) {\n",
    "                pairwise <- t(combn(x, 2))\n",
    "                index <- (highRelatedMultiple$IID1 %in% pairwise[,1] &\n",
    "                              highRelatedMultiple$IID2 %in% pairwise[,2]) |\n",
    "                    (highRelatedMultiple$IID1 %in% pairwise[,2] &\n",
    "                         highRelatedMultiple$IID2 %in% pairwise[,1])\n",
    "                combination <- highRelatedMultiple[index,]\n",
    "                combination_graph <- igraph::graph_from_data_frame(combination,\n",
    "                                                                   directed=FALSE)\n",
    "                all_iv_set <- igraph::ivs(combination_graph)\n",
    "                length_iv_set <- sapply(all_iv_set, function(x) length(x))\n",
    "\n",
    "                if (all(length_iv_set == 1)) {\n",
    "                    # check how often they occurr elsewhere\n",
    "                    occurrence <- sapply(x, function(id) {\n",
    "                        sum(sapply(relatedPerID, function(idlist) id %in% idlist))\n",
    "                    })\n",
    "                    # if occurrence the same everywhere, pick the first, else keep\n",
    "                    # the one with minimum occurrence elsewhere\n",
    "                    if (length(unique(occurrence)) == 1) {\n",
    "                        nonRelated <- sort(x)[1]\n",
    "                    } else {\n",
    "                        nonRelated <- names(occurrence)[which.min(occurrence)]\n",
    "                    }\n",
    "                } else {\n",
    "                    nonRelated <- all_iv_set[which.max(length_iv_set)]\n",
    "                }\n",
    "                return(nonRelated)\n",
    "            })\n",
    "            keepIDs_multiple <- unique(unlist(keepIDs_multiple))\n",
    "            failIDs_multiple <- c(multipleRelative[!multipleRelative %in%\n",
    "                                                       keepIDs_multiple])\n",
    "        } else {\n",
    "            failIDs_multiple <- NULL\n",
    "        }\n",
    "        allFailIIDs <- c(failIDs_single, failIDs_multiple)\n",
    "        relatednessFails <- lapply(allFailIIDs, function(id) {\n",
    "            fail_inorder <- relatedness_original$IID1 == id &\n",
    "                relatedness_original$M > relatednessTh\n",
    "            fail_inreverse <- relatedness_original$IID2 == id &\n",
    "                relatedness_original$M > relatednessTh\n",
    "            if (any(fail_inreverse)) {\n",
    "                inreverse <- relatedness_original[fail_inreverse, ]\n",
    "                id1 <- iid1_index\n",
    "                id2 <- iid2_index\n",
    "                inreverse[,c(id1, id2)] <- inreverse[,c(id2, id1)]\n",
    "                names(inreverse) <- relatedness_names\n",
    "            } else {\n",
    "                inreverse <- NULL\n",
    "            }\n",
    "            inorder <- relatedness_original[fail_inorder, ]\n",
    "            names(inorder) <- relatedness_names\n",
    "            return(rbind(inorder, inreverse))\n",
    "        })\n",
    "        relatednessFails <- do.call(rbind, relatednessFails)\n",
    "        if (nrow(relatednessFails) == 0) {\n",
    "            relatednessFails <- NULL\n",
    "            failIDs <- NULL\n",
    "        } else {\n",
    "            names(relatednessFails) <- relatedness_names\n",
    "            rownames(relatednessFails) <- 1:nrow(relatednessFails)\n",
    "            uniqueFails <- relatednessFails[!duplicated(relatednessFails[,iid1_index]),]\n",
    "            failIDs <- uniqueFails[,iid1_index]\n",
    "        }\n",
    "        return(list(relatednessFails=relatednessFails, failIDs=failIDs))\n",
    "    }\n",
    "    \n",
    "  \n",
    "    # main code\n",
    "    kin0 <- read.table(${_input:r}, header=F, stringsAsFactor=F)\n",
    "    colnames(kin0) <- c(\"FID1\",\"ID1\",\"FID2\",\"ID2\",\"NSNP\",\"HETHET\",\"IBS0\",\"KINSHIP\")\n",
    "    if (${\"TRUE\" if maximize_unrelated else \"FALSE\"}) {\n",
    "        rel <- relatednessFilter(kin0, ${kinship}, \"ID1\", \"ID2\", \"KINSHIP\")$failIDs\n",
    "        tmp1 <- kin0[,1:2]\n",
    "        tmp2 <- kin0[,3:4]\n",
    "        colnames(tmp1) = colnames(tmp2) = c(\"FID\", \"ID\")\n",
    "        # Get the family ID for these rels so there are two columns FID and IID in the output\n",
    "        lookup <- dplyr::distinct(rbind(tmp1,tmp2))\n",
    "        dat <- lookup[which(lookup[,2] %in% rel),]\n",
    "    } else {\n",
    "        rel <- kin0 %>% filter(KINSHIP >= ${kinship})\n",
    "        dat = rbind(rel[,c(\"FID1\",\"ID1\")],setNames(rel[,c(\"FID2\",\"ID2\")],c(\"FID1\",\"ID1\")))\n",
    "        dat = dat[!duplicated(dat),] ## This is to remove duplicated FID and IID caused by one sample being related to multiple samples\n",
    "       }    \n",
    "\n",
    "    cat(\"There are\", nrow(dat),\"related individuals using a kinship threshold of ${kinship}\\n\")\n",
    "    write.table(dat,${_output:r}, quote=FALSE, row.names=FALSE, col.names=FALSE)\n",
    "    \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "        stdout=$[_output].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-butter",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Split genotype data into related and unrelated samples, if related individuals are detected\n",
    "[king_3]\n",
    "input: output_from(2), genoFile\n",
    "output: unrelated_bed = f'{cwd}/{_input[0]:bn}.unrelated.bed',\n",
    "        related_bed = f'{cwd}/{_input[0]:bn}.related.bed'\n",
    "related_id = [x.strip() for x in open(_input[0]).readlines()]\n",
    "stop_if(len(related_id) == 0)\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash:  expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "    plink2 \\\n",
    "      --bfile ${_input[1]:n} \\\n",
    "      --remove ${_input[0]} \\\n",
    "      ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} \\\n",
    "      --make-bed \\\n",
    "      --out ${_output[0]:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6} --new-id-max-allele-len 1000 --set-all-var-ids chr@:#_\\$r_\\$a \n",
    "\n",
    "    plink2 \\\n",
    "      --bfile ${_input[1]:n} \\\n",
    "      --keep ${_input[0]} \\\n",
    "      --make-bed \\\n",
    "      --out ${_output[1]:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6} --new-id-max-allele-len 1000 --set-all-var-ids chr@:#_\\$r_\\$a \n",
    "        \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "standing-spencer",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Genotype and sample QC\n",
    "\n",
    "QC the genetic data based on MAF, sample and variant missigness and Hardy-Weinberg Equilibrium (HWE).\n",
    "\n",
    "In this step you may also provide a list of samples to keep, for example in the case when you would like to subset a sample based on their ancestries to perform independent analyses on each of these groups.\n",
    "\n",
    "The default parameters are set to reflect some suggestions in Table 1 of [this paper](https://dx.doi.org/10.1002%2Fmpr.1608)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "average-austin",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Filter SNPs and select individuals \n",
    "[qc_no_prune, qc_1 (basic QC filters)]\n",
    "# minimum MAF filter to use. 0 means do not apply this filter.\n",
    "parameter: maf_filter = 0.0\n",
    "# maximum MAF filter to use. 0 means do not apply this filter.\n",
    "parameter: maf_max_filter = 0.0\n",
    "# minimum MAC filter to use. 0 means do not apply this filter.\n",
    "parameter: mac_filter = 0.0\n",
    "# maximum MAC filter to use. 0 means do not apply this filter.\n",
    "parameter: mac_max_filter = 0.0 \n",
    "# Maximum missingess per-variant\n",
    "parameter: geno_filter = 0.1\n",
    "# Maximum missingness per-sample\n",
    "parameter: mind_filter = 0.1\n",
    "# HWE filter -- a very lenient one\n",
    "parameter: hwe_filter = 1e-15\n",
    "# Other PLINK arguments e.g snps_only, write-samples, etc\n",
    "parameter: other_args = []\n",
    "# Only output SNP and sample list, rather than the PLINK binary format of subset data\n",
    "parameter: meta_only = False\n",
    "# Remove duplicate variants\n",
    "parameter: rm_dups = False\n",
    "\n",
    "fail_if(not (keep_samples.is_file() or keep_samples == path('.')), msg = f'Cannot find ``{keep_samples}``')\n",
    "fail_if(not (keep_variants.is_file() or keep_variants == path('.')), msg = f'Cannot find ``{keep_variants}``')\n",
    "fail_if(not (remove_samples.is_file() or remove_samples == path('.')), msg = f'Cannot find ``{remove_samples}``')\n",
    "\n",
    "input: genoFile, group_by=1\n",
    "output: f'{cwd}/{_input:bn}{(\".\"+name) if name else \"\"}.plink_qc{\".extracted\" if keep_variants.is_file() else \"\"}{\".bed\" if not meta_only else \".snplist\"}'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, volumes=[f'{cwd}:{cwd}'], expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "    plink2 \\\n",
    "      --bfile ${_input:n} \\\n",
    "      ${('--maf %s' % maf_filter) if maf_filter > 0 else ''} \\\n",
    "      ${('--max-maf %s' % maf_max_filter) if maf_max_filter > 0 else ''} \\\n",
    "      ${('--mac %s' % mac_filter) if mac_filter > 0 else ''} \\\n",
    "      ${('--max-mac %s' % mac_max_filter) if mac_max_filter > 0 else ''} \\\n",
    "      ${('--geno %s' % geno_filter) if geno_filter > 0 else ''} \\\n",
    "      ${('--hwe %s' % hwe_filter) if hwe_filter > 0 else ''} \\\n",
    "      ${('--mind %s' % mind_filter) if mind_filter > 0 else ''} \\\n",
    "      ${('--keep %s' % keep_samples) if keep_samples.is_file() else \"\"} \\\n",
    "      ${('--remove %s' % remove_samples) if remove_samples.is_file() else \"\"} \\\n",
    "      ${('--exclude %s' % exclude_variants) if exclude_variants.is_file() else \"\"} \\\n",
    "      ${('--extract %s' % keep_variants) if keep_variants.is_file() else \"\"} \\\n",
    "      ${('--make-bed') if not meta_only else \"--write-snplist --write-samples\"} \\\n",
    "      ${(\"\") if not rm_dups else \"--rm-dup force-first 'list'\"} \\\n",
    "      ${paths([\"--%s\" % x for x in other_args]) if other_args else \"\"} \\\n",
    "      --out ${_output:n} \\\n",
    "      --threads ${numThreads} \\\n",
    "      --memory ${int(expand_size(mem) * 0.9)/1e6} --new-id-max-allele-len 1000 --set-all-var-ids chr@:#_\\$r_\\$a \n",
    "        \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "numerous-literature",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# LD prunning and remove related individuals (both ind of a pair)\n",
    "# Plink2 has multi-threaded calculation for LD prunning\n",
    "[qc_2 (LD pruning)]\n",
    "# Window size\n",
    "parameter: window = 50\n",
    "# Shift window every 10 snps\n",
    "parameter: shift = 10\n",
    "parameter: r2 = 0.1\n",
    "stop_if(r2==0)\n",
    "output: bed=f'{cwd}/{_input:bn}.prune.bed', prune=f'{cwd}/{_input:bn}.prune.in'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout'\n",
    "    plink2 \\\n",
    "    --bfile ${_input:n} \\\n",
    "    --indep-pairwise ${window} ${shift} ${r2}  \\\n",
    "    --out ${_output[\"prune\"]:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e6}\n",
    "   \n",
    "    plink2 \\\n",
    "    --bfile ${_input:n} \\\n",
    "    --extract ${_output['prune']} \\\n",
    "    --make-bed \\\n",
    "    --out ${_output['bed']:n} \\\n",
    "    --threads ${numThreads} \\\n",
    "    --memory ${int(expand_size(mem) * 0.9)/1e6}\n",
    "    \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "0.22.4"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "sos",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "0.22.6"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "sos",
     "op": "patch"
    }
   ]
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "panel": {
    "displayed": true,
    "height": 0
   },
   "version": "0.23.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
