{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broken-billy",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Gene Coordinate Annotation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-combine",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e69234",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "We use a gene coordinate annotation pipeline based on [`pyqtl`, as demonstrated here](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py). This adds genomic coordinate annotations to gene-level molecular phenotype files generated in `gct` format and converts them to `bed` format for downstreams analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c5d2f-53df-45b7-bf78-8c449c02f97e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Alternative implementation\n",
    "\n",
    "Previously we use `biomaRt` package in R instead of code from `pyqtl`. The core function calls are:\n",
    "\n",
    "```r\n",
    "    ensembl = useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "    ensembl_df <- getBM(attributes=c(\"ensembl_gene_id\",\"chromosome_name\", \"start_position\", \"end_position\"),mart=ensembl)\n",
    "```\n",
    "\n",
    "We require ENSEMBL version to be specified explicitly in this pipeline. As of 2021 for the Brain xQTL project, we use ENSEMBL version 103."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-surge",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "1. Molecular phenotype data in `gct` format, with the first column being ENSEMBL ID and other columns being sample names. \n",
    "2. GTF for collapsed gene model\n",
    "    - the gene names must be consistent with the molecular phenotype data matrices (eg ENSG00000000003 vs. ENSG00000000003.1 will not work) \n",
    "3. (Optional) Meta-data to match between sample names in expression data and genotype files\n",
    "    - Tab delimited with header\n",
    "    - Only 2 columns: first column is sample name in expression data, 2nd column is sample name in genotype data\n",
    "    - **must contains all the sample name in expression matrices even if they don't existing in genotype data**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-naples",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "Molecular phenotype data in `bed` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-patch",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps\n",
    "\n",
    "The MWE is uploaded to [Synapse](https://www.synapse.org/#!Synapse:syn36416559/files/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b384e-e1d8-4952-8b38-5289aba54366",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### i. Cooridnate Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69943fd9-1e32-4c89-9a9b-000e30c90c80",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### a. Gene Expression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe0f2b4-d7a2-458e-af98-a3d4df043d97",
   "metadata": {
    "kernel": "SoS"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cd30d7e-4252-4ec5-bb47-a2f6cbe1a43a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: <1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0b7ce8-9360-4b20-a8fd-82577aaefaf0",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mannotate_coord_gene\u001b[0m: \n",
      "INFO: tc987661f8bb5f0b1 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 3690363 (\"job_tc987661f8bb5f0b1\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mannotate_coord_gene\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/phenotype_by_chrom/MWE.log2cpm.mol_phe.bed.bed.gz /restricted/projectnb/xqtl/xqtl_protocol/output_test/phenotype_by_chrom/MWE.log2cpm.mol_phe.bed.region_list.txt\u001b[0m\n",
      "INFO: Workflow annotate_coord_gene (ID=w90bd1b9b7fb38fca) is executed successfully with 1 completed step and 1 completed task.\n"
     ]
    }
   ],
   "source": [
    "!sos run gene_annotation.ipynb annotate_coord_gene \\\n",
    "    --cwd ../../../output_test/phenotype_by_chrom \\\n",
    "    --phenoFile ../../../mwe_data/xQTL_discovery/MWE.log2cpm.mol_phe.bed.gz \\\n",
    "    --annotation-gtf ../../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf \\\n",
    "    --container  oras://ghcr.io/cumc/rna_quantification_apptainer:latest --phenotype-id-type gene_name \\\n",
    "    -c ../../csg.yml  -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b914a3-175d-47be-b81c-0e67c705232a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### b. Proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867be70-4327-4536-9f73-596512fb80cd",
   "metadata": {
    "kernel": "SoS"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08a79d4f-2b71-44ac-82c8-a3e1973f00d6",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Timing: <1 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enormous-reduction",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mannotate_coord_protein\u001b[0m: \n",
      "INFO: tbecf36ea3e901ea1 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: tbecf36ea3e901ea1 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 3961996 (\"job_tbecf36ea3e901ea1\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mannotate_coord_protein\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/phenotype_by_chrom/protocol_example.protein.bed.gz /restricted/projectnb/xqtl/xqtl_protocol/output_test/phenotype_by_chrom/protocol_example.protein.region_list.txt\u001b[0m\n",
      "INFO: Workflow annotate_coord_protein (ID=w75d21004c6718efd) is executed successfully with 1 completed step and 1 completed task.\n"
     ]
    }
   ],
   "source": [
    "!sos run gene_annotation.ipynb annotate_coord_protein \\\n",
    "    --cwd ../../../output_test/phenotype_by_chrom \\\n",
    "    --phenoFile ../../../mwe_data/protocol_data/input/xqtl_association/protocol_example.protein.csv \\\n",
    "    --annotation-gtf ../../../reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf \\\n",
    "    --sample-participant-lookup ../../../mwe_data/protocol_data/output/protocol_example.protein.sample_overlap.txt \\\n",
    "    --phenotype-id-type gene_name \\\n",
    "    --container  oras://ghcr.io/cumc/rna_quantification_apptainer:latest --sep \",\"  \\\n",
    "    -c ../../csg.yml  -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cdb9b-a5a1-4656-ab72-da43bfd55ed4",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3886247b-3e47-4d8f-90be-580c98cfa311",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-state",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "christian-uganda",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run gene_annotation.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  region_list_generation\n",
      "  annotate_coord_gene\n",
      "  annotate_coord_protein\n",
      "  annotate_coord_biomart\n",
      "  map_leafcutter_cluster_to_gene\n",
      "  annotate_leafcutter_isoforms\n",
      "  annotate_psichomics_isoforms\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        Work directory & output directory\n",
      "  --annotation-gtf VAL (as path, required)\n",
      "                        gene gtf annotation table\n",
      "  --phenoFile VAL (as path, required)\n",
      "                        Molecular phenotype matrix\n",
      "  --phenotype-id-type 'gene_id'\n",
      "                        Whether the input data is named by gene_id or gene_name.\n",
      "                        By default it is gene_id, if not, please change it to\n",
      "                        gene_name\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 1 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "  --sep '\\t'\n",
      "                        delimiter of phenoFile\n",
      "  --entrypoint  ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
      "\n",
      "\n",
      "Sections\n",
      "  region_list_generation:\n",
      "  annotate_coord_gene:\n",
      "    Workflow Options:\n",
      "      --sample-participant-lookup . (as path)\n",
      "                        A file to map sample ID from expression to genotype,\n",
      "                        must contain two columns, sample_id and participant_id,\n",
      "                        mapping IDs in the expression files to IDs in the\n",
      "                        genotype (these can be the same).\n",
      "  annotate_coord_protein:\n",
      "    Workflow Options:\n",
      "      --sample-participant-lookup . (as path)\n",
      "                        A file to map sample ID from expression to genotype,\n",
      "                        must contain two columns, sample_id and participant_id,\n",
      "                        mapping IDs in the expression files to IDs in the\n",
      "                        genotype (these can be the same).\n",
      "      --protein-name-index . (as path)\n",
      "      --protein-ID-type SOMAseqID\n",
      "  annotate_coord_biomart:\n",
      "    Workflow Options:\n",
      "      --ensembl-version VAL (as int, required)\n",
      "  map_leafcutter_cluster_to_gene:\n",
      "    Workflow Options:\n",
      "      --intron-count VAL (as path, required)\n",
      "                        Extract the code in case psichromatic needs to be\n",
      "                        processed the same way PheoFile in this step is the\n",
      "                        intron_count file\n",
      "  annotate_leafcutter_isoforms:\n",
      "    Workflow Options:\n",
      "      --sample-participant-lookup . (as path)\n",
      "  annotate_psichomics_isoforms:\n",
      "    Workflow Options:\n",
      "      --sample-participant-lookup . (as path)\n"
     ]
    }
   ],
   "source": [
    "!sos run gene_annotation.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae37de-fac0-44d3-b180-28fb1afbdab0",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-climb",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path(\"output\")\n",
    "#  gene gtf annotation table\n",
    "parameter: annotation_gtf = path\n",
    "# Molecular phenotype matrix\n",
    "parameter: phenoFile = path\n",
    "# Whether the input data is named by gene_id or gene_name. By default it is gene_id, if not, please change it to gene_name\n",
    "parameter: phenotype_id_type = 'gene_id'\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "parameter: container = \"\"\n",
    "## delimiter of phenoFile\n",
    "parameter: sep = \"\\t\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "parameter: strip_id = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-speech",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Region List generation\n",
    "\n",
    "To partitioning the data by genes require a region list file which:\n",
    "\n",
    "1. have 5 columns: chr,start,end,gene_id,gene_name\n",
    "2. have the same gene as or less gene than that of the bed file\n",
    "\n",
    "Input:\n",
    "\n",
    "1. A gtf file used to generated the bed\n",
    "2. A phenotype bed file, must have a gene_id column indicating the name of genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-defeat",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[region_list_generation]\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bnn}.region_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    # get the five column data\n",
    "    bed_template_df_id = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_id\" )\n",
    "    bed_template_df_name = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_name\" )\n",
    "    bed_template_df = bed_template_df_id.merge(bed_template_df_name, on = [\"chr\",\"start\",\"end\"])\n",
    "    bed_template_df.columns = [\"#chr\",\"start\",\"end\",\"gene_id\",\"gene_name\"]\n",
    "    pheno = pd.read_csv(${_input[0]:r}, sep = \"\\t\")\n",
    "    # Retaining only the genes in the data\n",
    "    region_list = bed_template_df[bed_template_df.${phenotype_id_type}.isin(pheno.gene_id)]\n",
    "    region_list.to_csv(\"${_output}\", sep = \"\\t\",index = False)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alleged-saskatchewan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Implementation using `pyqtl`\n",
    "\n",
    "Implementation based on [GTEx pipeline](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py).\n",
    "\n",
    "Following step serves to annotate cood for gene expression file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-corruption",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_gene]\n",
    "# A file to map sample ID from expression to genotype, must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path()\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bn}.bed.gz', f'{cwd:a}/{_input[0]:bn}.region_list.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "\n",
    "    def prepare_bed(df, bed_template_df, chr_subset=None):\n",
    "        bed_df = pd.merge(bed_template_df, df, left_index=True, right_index=True)\n",
    "        bed_df = bed_df.groupby('#chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "        if chr_subset is not None:\n",
    "            bed_df = bed_df[bed_df.chr.isin(chr_subset)]\n",
    "        return bed_df\n",
    "\n",
    "    def load_and_preprocess_data(input_path, drop_columns):\n",
    "        df = pd.read_csv(input_path, sep=\"${sep}\", skiprows=0)\n",
    "        dc = [col for col in df.columns if col in drop_columns] # Take interscet between df.columns and drop_columns\n",
    "        df = df.drop(dc,axis = 1) # drop the intersect\n",
    "        if len(df.columns) < 2:\n",
    "            raise ValueError(\"There are too few columns in the loaded dataframe, please check the delimiter of the input file. The default delimiter is tab\")\n",
    "        return df\n",
    "\n",
    "    # convert first column to second column: convert ProjID to SampleID\n",
    "    def rename_samples_using_lookup(df, lookup_path):\n",
    "        if Path(lookup_path).is_file():\n",
    "            # Read the CSV without headers and specify dtype for both columns as string\n",
    "            sample_participant_lookup = pd.read_csv(lookup_path, sep=\",\", header=None, dtype={0: str, 1: str})\n",
    "            # Create a rename dictionary using the first column as keys and the second column as values\n",
    "            rename_dict = dict(zip(sample_participant_lookup[0], sample_participant_lookup[1]))\n",
    "            # Rename columns in df based on the rename_dict\n",
    "            df.rename(columns=rename_dict, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def load_bed_template(input_path, phenotype_id_type):\n",
    "        if sum(qtl.io.gtf_to_tss_bed(input_path, feature='gene', phenotype_id = \"gene_id\").index.duplicated()) > 0:\n",
    "            raise valueerror(f\"gtf file {input_path} needs to be collapsed into gene model by reference data processing module\")\n",
    "\n",
    "        bed_template_df_id = qtl.io.gtf_to_tss_bed(input_path, feature='transcript', phenotype_id=\"gene_id\")\n",
    "        bed_template_df_name = qtl.io.gtf_to_tss_bed(input_path, feature='transcript', phenotype_id=\"gene_name\")\n",
    "        bed_template_df = bed_template_df_id.merge(bed_template_df_name, on=[\"chr\", \"start\", \"end\"])\n",
    "        bed_template_df.columns = [\"#chr\", \"start\", \"end\", \"gene_id\", \"gene_name\"]\n",
    "        bed_template_df = bed_template_df.set_index(phenotype_id_type, drop=False)\n",
    "\n",
    "        return bed_template_df\n",
    "\n",
    "    df = load_and_preprocess_data(${_input[0]:ar}, [\"#chr\",\"chr\", \"start\", \"end\",\"stop\",\"annot.seqnames\",\"annot.start\",\"annot.end\"]) # This function will drop any columns from the DF based on the drop_columns list, so it is good to make the list comprehensive to accomodate different source of inputs.    df.set_index(df.columns[0], inplace=True)\n",
    "    df.set_index(df.columns[0], inplace=True)\n",
    "    df = rename_samples_using_lookup(df, \"${sample_participant_lookup:a}\")\n",
    "    if ${strip_id}:  \n",
    "        # Update the index by stripping the pattern \"A | B\" to \"B\"\n",
    "        # and removing any leading or trailing spaces around \"B\"\n",
    "        df.index = df.index.map(lambda x: x.split('|')[-1].strip() if '|' in x else x)\n",
    "    bed_template_df = load_bed_template(${_input[1]:ar}, \"${phenotype_id_type}\")\n",
    "    bed_df = prepare_bed(df, bed_template_df).drop(\"gene_name\", axis=1)\n",
    "    bed_df = bed_df.drop_duplicates(\"gene_id\", keep=False)\n",
    "    bed_df = bed_df.rename(columns={'gene_id': 'ID'})\n",
    "    qtl.io.write_bed(bed_df, ${_output[0]:r})\n",
    "    bed_df[[\"#chr\",\"start\",\"end\",\"ID\"]].assign(path = ${_output[0]:r}).to_csv(${_output[1]:r},\"\\t\",index = False) \n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-journal",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Following step serves to annotate cood for proteiomics data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-regular",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_protein]\n",
    "# A file to map sample ID from expression to genotype, must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path()\n",
    "parameter: protein_name_index = path()\n",
    "parameter: protein_ID_type = \"SOMAseqID\"\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bn}.bed.gz', f'{cwd:a}/{_input[0]:bn}.region_list.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "\n",
    "    def prepare_bed(df, bed_template_df, chr_subset=None):\n",
    "        bed_df = pd.merge(bed_template_df, df, left_index=True, right_index=True)\n",
    "        bed_df = bed_df.groupby('#chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "        if chr_subset is not None:\n",
    "            bed_df = bed_df[bed_df.chr.isin(chr_subset)]\n",
    "        return bed_df\n",
    "\n",
    "    def load_and_preprocess_data(input_path, drop_columns):\n",
    "        df = pd.read_csv(input_path, sep=\"${sep}\", skiprows=0)\n",
    "        dc = [col for col in df.columns if col in drop_columns] # Take interscet between df.columns and drop_columns\n",
    "        df = df.drop(dc,axis = 1) # drop the intersect\n",
    "        if len(df.columns) < 2:\n",
    "            raise ValueError(\"There are too few columns in the loaded dataframe, please check the delimiter of the input file. The default delimiter is tab\")\n",
    "        return df\n",
    "\n",
    "    def rename_samples_using_lookup(df, lookup_path):\n",
    "        sample_participant_lookup = Path(lookup_path)\n",
    "        if sample_participant_lookup.is_file():\n",
    "            sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=1, dtype={0:str,1:str})\n",
    "            df.rename(columns=sample_participant_lookup_s.to_dict()[\"genotype_id\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def load_bed_template(input_path, phenotype_id_type):\n",
    "        if sum(qtl.io.gtf_to_tss_bed(input_path, feature='gene',phenotype_id = \"gene_id\").index.duplicated()) > 0:\n",
    "            raise valueerror(f\"gtf file {input_path} needs to be collapsed into gene model by reference data processing module\")\n",
    "\n",
    "        bed_template_df_id = qtl.io.gtf_to_tss_bed(input_path, feature='transcript', phenotype_id=\"gene_id\")\n",
    "        bed_template_df_name = qtl.io.gtf_to_tss_bed(input_path, feature='transcript', phenotype_id=\"gene_name\")\n",
    "        bed_template_df = bed_template_df_id.merge(bed_template_df_name, on=[\"chr\", \"start\", \"end\"])\n",
    "        bed_template_df.columns = [\"#chr\", \"start\", \"end\", \"gene_id\", \"gene_name\"]\n",
    "        bed_template_df = bed_template_df.set_index(phenotype_id_type, drop=False)\n",
    "\n",
    "        return bed_template_df\n",
    "\n",
    "    df = load_and_preprocess_data(${_input[0]:ar}, [\"#chr\",\"chr\", \"start\", \"end\",\"stop\",\"annot.seqnames\",\"annot.start\",\"annot.end\"]) # This function will drop any columns from the DF based on the drop_columns list, so it is good to make the list comprehensive to accomodate different source of inputs.\n",
    "    protein_ID = df.columns.values[0]\n",
    "    protein_name_index = Path(\"${protein_name_index:a}\")\n",
    "    if protein_name_index.is_file():\n",
    "        df_info = pd.read_csv(protein_name_index).rename(columns={'${protein_ID_type}': protein_ID, 'EntrezGeneSymbol':'gene_name'})[['gene_name',protein_ID,'UniProt']]\n",
    "        df = df_info.merge(df, on=protein_ID).drop(protein_ID,axis=1)\n",
    "    else:\n",
    "        df[[protein_ID, 'UniProt']] = df[protein_ID].astype(str).str.split('|', 1, expand=True)\n",
    "    df.set_index(df.columns[0], inplace=True)\n",
    "    df = rename_samples_using_lookup(df, \"${sample_participant_lookup:a}\")\n",
    "    bed_template_df = load_bed_template(${_input[1]:ar}, \"${phenotype_id_type}\")\n",
    "    bed_df = prepare_bed(df, bed_template_df)\n",
    "    bed_df[\"ID\"] = bed_df[\"gene_id\"] + \"_\" + bed_df[\"UniProt\"]\n",
    "    bed_df = bed_df.drop_duplicates(\"ID\", keep=False)[[\"#chr\",\"start\",\"end\",\"ID\"] + df.drop([\"UniProt\"],axis=1).columns.values.tolist()]\n",
    "    qtl.io.write_bed(bed_df, ${_output[0]:r})\n",
    "    bed_df[[\"#chr\",\"start\",\"end\",\"ID\"]].assign(path = ${_output[0]:r}).to_csv(${_output[1]:r},\"\\t\",index = False) \n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce221f4-fc32-4f20-a96c-50e7eee0e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Following code annotate the atac_seq peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe5499-8601-495f-9e32-1f1b81fed389",
   "metadata": {},
   "outputs": [],
   "source": [
    "[annotate_coord_ATAC]\n",
    "# A file to map sample ID from expression to genotype, must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path()\n",
    "parameter: peak_annotation = path\n",
    "input: phenoFile, peak_annotation\n",
    "output: f'{cwd:a}/{_input[0]:bn}.bed.gz', f'{cwd:a}/{_input[0]:bn}.region_list.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "\n",
    "    def prepare_bed(df, bed_template_df, chr_subset=None):\n",
    "        bed_df = pd.merge(bed_template_df, df, left_index=True, right_index=True)\n",
    "        bed_df = bed_df.groupby('#chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "        if chr_subset is not None:\n",
    "            bed_df = bed_df[bed_df.chr.isin(chr_subset)]\n",
    "        return bed_df\n",
    "\n",
    "    def load_and_preprocess_data(input_path, drop_columns):\n",
    "        df = pd.read_csv(input_path, sep=\"${sep}\", skiprows=0)\n",
    "        dc = [col for col in df.columns if col in drop_columns] # Take interscet between df.columns and drop_columns\n",
    "        df = df.drop(dc,axis = 1) # drop the intersect\n",
    "        if len(df.columns) < 2:\n",
    "            raise ValueError(\"There are too few columns in the loaded dataframe, please check the delimiter of the input file. The default delimiter is tab\")\n",
    "        return df\n",
    "\n",
    "    def rename_samples_using_lookup(df, lookup_path):\n",
    "        sample_participant_lookup = Path(lookup_path)\n",
    "        if sample_participant_lookup.is_file():\n",
    "            sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=1, dtype={0:str,1:str})\n",
    "            df.rename(columns=sample_participant_lookup_s.to_dict()[\"genotype_id\"], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def load_bed_template(input_path, phenotype_id_type):\n",
    "        if sum(qtl.io.gtf_to_tss_bed(input_path, feature='gene',phenotype_id = \"gene_id\").index.duplicated()) > 0:\n",
    "            raise valueerror(f\"gtf file {input_path} needs to be collapsed into gene model by reference data processing module\")\n",
    "\n",
    "        bed_template_df_id = qtl.io.gtf_to_tss_bed(input_path, feature='transcript', phenotype_id=\"gene_id\")\n",
    "        bed_template_df_name = qtl.io.gtf_to_tss_bed(input_path, feature='transcript', phenotype_id=\"gene_name\")\n",
    "        bed_template_df = bed_template_df_id.merge(bed_template_df_name, on=[\"chr\", \"start\", \"end\"])\n",
    "        bed_template_df.columns = [\"#chr\", \"start\", \"end\", \"gene_id\", \"gene_name\"]\n",
    "        bed_template_df = bed_template_df.set_index(phenotype_id_type, drop=False)\n",
    "\n",
    "        return bed_template_df\n",
    "\n",
    "    df = load_and_preprocess_data(${_input[0]:ar}, [\"#chr\",\"chr\", \"start\", \"end\",\"stop\",\"annot.seqnames\",\"annot.start\",\"annot.end\"]) # This function will drop any columns from the DF based on the drop_columns list, so it is good to make the list comprehensive to accomodate different source of inputs.    df.set_index(df.columns[0], inplace=True)\n",
    "    df.set_index(df.columns[0], inplace=True)\n",
    "    df = rename_samples_using_lookup(df, \"${sample_participant_lookup:a}\")\n",
    "    bed_template_df = pd.read_csv(\"${_input[1]}\",sep = \"\\t\").set_index(\"ID\")\n",
    "    bed_template_df = bed_template_df.assign(ID = bed_template_df.index )\n",
    "    bed_df = prepare_bed(df, bed_template_df)\n",
    "    bed_df = bed_df.drop_duplicates(\"ID\", keep=False)\n",
    "    qtl.io.write_bed(bed_df, ${_output[0]:r})\n",
    "    bed_df[[\"#chr\",\"start\",\"end\",\"ID\"]].assign(path = ${_output[0]:r}).to_csv(${_output[1]:r},\"\\t\",index = False) \n",
    "\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-hindu",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Implementation using biomaRt\n",
    "This workflow adds the annotations of chr pos(TSS where start = end -1) and gene_ID to the `bed` file. **This workflow is obsolete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-toner",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_biomart]\n",
    "parameter: ensembl_version=int\n",
    "input: phenoFile\n",
    "output: f'{cwd:a}/{_input:bn}.bed.gz',\n",
    "        f'{cwd:a}/{_input:bn}.region_list.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "R:  expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout' ,container = container, entrypoint=entrypoint\n",
    "\n",
    "    library(biomaRt)\n",
    "    library(dplyr)\n",
    "    library(readr)\n",
    "\n",
    "    # Clear biomart cache\n",
    "    biomartCacheClear()\n",
    "\n",
    "    # Read gene expression data\n",
    "    gene_exp <- read_delim(\"$[_input[0]]\", delim = \"\\t\")\n",
    "\n",
    "    # If column \"#chr\" exists, remove the first 3 columns (chr, start, and end)\n",
    "    if(\"#chr\" %in% colnames(gene_exp)){\n",
    "      gene_exp <- gene_exp[, 4:ncol(gene_exp)]\n",
    "    }\n",
    "\n",
    "    # Connect to Ensembl biomart\n",
    "    ensembl <- useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "\n",
    "    # Get gene annotations from Ensembl\n",
    "    ensembl_df <- getBM(attributes = c(\"ensembl_gene_id\", \"chromosome_name\", \"start_position\", \"end_position\"), mart = ensembl)\n",
    "\n",
    "    # Filter and rename columns\n",
    "    my_genes_ann <- ensembl_df %>%\n",
    "      filter(ensembl_gene_id %in% gene_exp$gene_ID, chromosome_name %in% 1:23) %>%\n",
    "      rename(\"#chr\" = chromosome_name, \n",
    "            \"start\" = start_position, \n",
    "            \"end\" = end_position, \n",
    "            \"gene_ID\" = ensembl_gene_id) %>%\n",
    "      filter(gene_ID != \"NA\")\n",
    "\n",
    "    # Save the annotations\n",
    "    my_genes_ann %>%\n",
    "      select(`#chr`, start, end, gene_ID) %>%\n",
    "      write_delim(path = \"$[_output[1]]\", \"\\t\")\n",
    "\n",
    "    # Merge the annotations with the gene expression data, after modifying 'end' to be 'start + 1'\n",
    "    my_gene_bed <- my_genes_ann %>%\n",
    "      mutate(end = start + 1) %>%\n",
    "      inner_join(gene_exp, by = \"gene_ID\") %>%\n",
    "      arrange(`#chr`, start)\n",
    "\n",
    "    # Save the final merged data\n",
    "    write_tsv(my_gene_bed, path = \"$[_output[0]:n]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")\n",
    "\n",
    "\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container, entrypoint=entrypoint\n",
    "    bgzip -f $[_output[0]:n]\n",
    "    tabix -p bed $[_output[0]] -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-affect",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annotation of leafcutter isoform\n",
    "The following steps processed the output files of leafcutter so that they are TensorQTL ready. Shown below are three intemediate files\n",
    "\n",
    "Exon list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brief-positive",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "chr   |  start  |  end    |  strand  | gene_id | gene_name\n",
    "------|---------|---------|----------|----------|--------------\n",
    "chr1  |  29554  |  30039  |  +       | ENSG00000243485 | MIR1302-2HG\n",
    "chr1  |  30564  |  30667  |  +       | ENSG00000243485 | MIR1302-2HG\n",
    "chr1  |  30976  |  31097  |  +       | ENSG00000243485 | MIR1302-2HG\n",
    "chr1  |  35721  |  36081  |  -       | ENSG00000237613 | FAM138A\n",
    "chr1  |  35277  |  35481  |  -       | ENSG00000237613 | FAM138A\n",
    "chr1  |  34554  |  35174  |  -       | ENSG00000237613 | FAM138A\n",
    "chr1  |  65419  |  65433  |  +       | ENSG00000186092 | OR4F5\n",
    "chr1  |  65520  |  65573  |  +       | ENSG00000186092 | OR4F5\n",
    "chr1  |  69037  |  71585  |  +       | ENSG00000186092 | OR4F5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-munich",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "clusters_to_genes\n",
    "\n",
    "\n",
    "|clu    | genes |\n",
    "|--------|---------- |\n",
    "|1:clu_1_+  |    ENSG00000116288|\n",
    "|1:clu_10_+ |    ENSG00000143774|\n",
    "|1:clu_11_+ |    ENSG00000143774|\n",
    "|1:clu_12_+ |    ENSG00000143774|\n",
    "|1:clu_14_- |    ENSG00000126709|\n",
    "|1:clu_15_- |    ENSG00000121753|\n",
    "|1:clu_16_- |    ENSG00000121753|\n",
    "|1:clu_17_- |    ENSG00000116560|\n",
    "|1:clu_18_- |    ENSG00000143549|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-hobby",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "phenotype_group\n",
    "\n",
    "|X1|X2|\n",
    "|-------------|---|\n",
    "| 7:102476270:102478811:clu_309_-:ENSG00000005075 | ENSG00000005075 | \n",
    "| 7:102476270:102478808:clu_309_-:ENSG00000005075 | ENSG00000005075 |\n",
    "| X:47572961:47574002:clu_349_-:ENSG00000008056   | ENSG00000008056 |\n",
    "| X:47572999:47574002:clu_349_-:ENSG00000008056   | ENSG00000008056 |\n",
    "| 8:27236905:27239971:clu_322_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27239279:27239971:clu_322_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27241262:27241677:clu_323_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27241262:27242397:clu_323_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27241757:27242397:clu_323_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 1:35558223:35559107:clu_4_+:ENSG00000020129     | ENSG00000020129 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "every-market",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The gtf used here should be the collapsed gtf, i.e. the final output of reference_data gtf processing and the one used to called rnaseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-hydrogen",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[map_leafcutter_cluster_to_gene]\n",
    "## Extract the code in case psichromatic needs to be processed the same way\n",
    "## PheoFile in this step is the intron_count file\n",
    "parameter: intron_count = path\n",
    "# Defines the mapping strategy options: 'site' or 'region', with 'site' as the default. \n",
    "# The 'site' strategy maps introns to the start and end of each exon. \n",
    "# The 'region' strategy, to be recommended in leafcutter2, maps each intron based on it overlapping more than overlap_ratio of a gene's region.\n",
    "parameter: map_stra = \"site\" \n",
    "# Define the overlap ratio as the proportion of the cluster length that intersects with a gene, used to determine mapping to the gene.\n",
    "parameter: overlap_ratio = 0.8\n",
    "input: intron_count, annotation_gtf\n",
    "output: f'{cwd}/{_input[1]:b}.exon_list', f'{cwd}/{_input[0]:b}.leafcutter.clusters_to_genes.txt'\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import qtl.annotation\n",
    "    # Load data\n",
    "    annot = qtl.annotation.Annotation(${_input[1]:r})\n",
    "    exon_df = pd.DataFrame([[g.chr, e.start_pos, e.end_pos, g.strand, g.id, g.name]\n",
    "                        for g in annot.genes for e in g.transcripts[0].exons],\n",
    "                       columns=['chr', 'start', 'end', 'strand', 'gene_id', 'gene_name'])\n",
    "    exon_df.to_csv(${_output[0]:r}, sep='\\t', index=False)\n",
    "    \n",
    "    \n",
    "R:expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    suppressMessages(library(dplyr, quietly=TRUE))\n",
    "    suppressMessages(library(stringr, quietly=TRUE))\n",
    "    suppressMessages(library(foreach, quietly=TRUE))\n",
    "    # leafcutter functions:\n",
    "    \n",
    "    #' Make a data.frame of meta data about the introns\n",
    "    #' @param introns Names of the introns\n",
    "    #' @return Data.frame with chr, start, end, cluster id\n",
    "    #' @export\n",
    "    get_intron_meta <- function(introns) {\n",
    "      intron_meta <- do.call(rbind, strsplit(introns, \":\"))  \n",
    "      if (ncol(intron_meta) == 5) {\n",
    "        colnames(intron_meta) <- c(\"chr\", \"start\", \"end\", \"clu\", \"category\")\n",
    "      } else {\n",
    "        colnames(intron_meta) <- c(\"chr\", \"start\", \"end\", \"clu\")\n",
    "      }\n",
    "\n",
    "      intron_meta <- as.data.frame(intron_meta, stringsAsFactors = FALSE)\n",
    "      intron_meta$start <- as.numeric(intron_meta$start)\n",
    "      intron_meta$end <- as.numeric(intron_meta$end)\n",
    "\n",
    "      return(intron_meta)\n",
    "    }\n",
    "    #' Work out which gene each cluster belongs to. Note the chromosome names used in the two inputs must match.\n",
    "    #' @param intron_meta Data frame describing the introns, usually from get_intron_meta\n",
    "    #' @param exons_table Table of exons, see e.g. /data/gencode19_exons.txt.gz\n",
    "    #' @return Data.frame with cluster ids and genes separated by commas\n",
    "    #' @import dplyr\n",
    "    #' @export\n",
    "    map_clusters_to_genes_site <- function(intron_meta, exons_table) {\n",
    "      suppressMessages(library(foreach, quietly=TRUE))\n",
    "      gene_df <- foreach (chr=sort(unique(intron_meta$chr)), .combine=rbind) %dopar% {\n",
    "    \n",
    "        intron_chr <- intron_meta[ intron_meta$chr==chr, ]\n",
    "        exons_chr <- exons_table[exons_table$chr==chr, ]\n",
    "    \n",
    "        exons_chr$temp <- exons_chr$start\n",
    "        intron_chr$temp <- intron_chr$end\n",
    "        three_prime_matches <- inner_join( intron_chr, exons_chr, by=\"temp\")\n",
    "    \n",
    "        exons_chr$temp <- exons_chr$end\n",
    "        intron_chr$temp <- intron_chr$start\n",
    "        five_prime_matches <- inner_join( intron_chr, exons_chr, by=\"temp\")\n",
    "    \n",
    "        all_matches <- rbind(three_prime_matches, five_prime_matches)[ , c(\"clu\", \"gene_name\")]\n",
    "    \n",
    "        all_matches <- all_matches[!duplicated(all_matches),]\n",
    "    \n",
    "        if (nrow(all_matches)==0) return(NULL)\n",
    "        all_matches$clu <- paste(chr,all_matches$clu,sep=':')\n",
    "        all_matches\n",
    "      }\n",
    "    \n",
    "      clu_df <- gene_df %>% group_by(clu) %>% summarize(genes=paste(gene_name, collapse = \",\"))\n",
    "      class(clu_df) <- \"data.frame\"\n",
    "      clu_df\n",
    "    }\n",
    "    \n",
    "    map_clusters_to_genes_region <- function(intron_meta, exons_table, f) {\n",
    "        #combine the exon position to get gene region\n",
    "        merged_df <- exons_table %>%\n",
    "            group_by(chr, gene_id) %>%\n",
    "            summarize(start = min(start), end = max(end), .groups = 'drop') %>% as.data.frame()%>%\n",
    "            .[,c(\"chr\",\"start\",\"end\",\"gene_id\")]\n",
    "        # use bedtools to map the intron region to gene region \n",
    "         options(bedtools.path = \"/home/rf2872/software/bedtools2/bin/\") #Fixme: build bedtools in leafcutter sif\n",
    "                map_res <- bedtoolsr::bt.intersect(a = intron_meta, b = merged_df,f = f, wa = TRUE, wb=TRUE)\n",
    "        #organize output of mapped file\n",
    "        exon_map <- map_res %>% \n",
    "            mutate(clu = paste(V1, V4, sep=\":\"), \n",
    "                   genes = .[[paste0(\"V\", 4 + ncol(intron_meta))]]) %>%\n",
    "                select(clu,genes)\n",
    "          return(exon_map)\n",
    "    }\n",
    "\n",
    "    cat(\"LeafCutter: mapping clusters to genes\\n\")\n",
    "    intron_counts <- read.table(${_input[0]:r}, header=TRUE, check.names=FALSE, row.names=1)\n",
    "    intron_meta <- get_intron_meta(rownames(intron_counts))\n",
    "    exon_table <- read.table(${_output[0]:r}, header=TRUE, stringsAsFactors=FALSE)\n",
    "    if(!str_detect(intron_meta$chr[1],\"chr\")) {\n",
    "        exon_table = exon_table %>% mutate(chr = str_remove_all(chr,\"chr\"))\n",
    "    } else if (!any(str_detect(exon_table$chr[1],\"chr\"))) {\n",
    "        exon_table = exon_table %>% mutate(chr = paste0(\"chr\",chr))\n",
    "    } else {exon_table = exon_table}\n",
    "    stopifnot(is.element('gene_id', colnames(exon_table)))\n",
    "    exon_table[, 'gene_name'] <- exon_table[, 'gene_id']\n",
    "    if(\"${map_stra}\" == \"site\"){\n",
    "      cat(\"mapping clusters to genes by splicing site\\n\")\n",
    "      m <- map_clusters_to_genes_site(intron_meta, exon_table)\n",
    "      } else if(\"${map_stra}\" == \"region\"){\n",
    "      cat(\"mapping clusters to genes by overlapping gene region\\n\")\n",
    "      m <- map_clusters_to_genes_region(intron_meta, exon_table, f = ${overlap_ratio})\n",
    "    } else {\n",
    "      stop(\"Map strategy should only be 'site' or 'region'.\")\n",
    "    }\n",
    "    write.table(m, ${_output[1]:r}, sep = \"\\t\", quote=FALSE, row.names=FALSE)\n",
    "  \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-concept",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_leafcutter_isoforms]\n",
    "parameter: sample_participant_lookup = path()\n",
    "input: phenoFile, annotation_gtf,output_from(\"map_leafcutter_cluster_to_gene\")\n",
    "output: f'{cwd:a}/{_input[0]:bn}.formated.bed.gz', f'{cwd:a}/{_input[0]:bn}.phenotype_group.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Load data\n",
    "    tss_df = qtl.io.gtf_to_tss_bed(${_input[1]:r})\n",
    "    bed_df = pd.read_csv(${_input[0]:ar}, sep='\\t', skiprows=0)\n",
    "    bed_df.columns.values[0] = \"#chr\" # Temporary\n",
    "    sample_participant_lookup = Path(\"${sample_participant_lookup:a}\")\n",
    "    cluster2gene_dict = pd.read_csv(${_input[3]:r}, sep='\\t', index_col=0).to_dict()\n",
    "    cluster2gene_dict = cluster2gene_dict['genes']\n",
    "    print('    ** assigning introns to gene mapping(s)')\n",
    "    n = 0\n",
    "    gene_bed_df = []\n",
    "    group_s = {}\n",
    "    for _,r in bed_df.iterrows():\n",
    "        s = r['ID'].split(':')\n",
    "        cluster_id = s[0]+':'+s[3]\n",
    "        if cluster_id in cluster2gene_dict:\n",
    "            gene_ids = cluster2gene_dict[cluster_id].split(',')\n",
    "            for g in gene_ids:\n",
    "                gi = r['ID']+':'+g\n",
    "                gene_bed_df.append(tss_df.loc[g, ['chr', 'start', 'end']].tolist() + [gi] + r.iloc[4:].tolist())\n",
    "                group_s[gi] = g\n",
    "        else:\n",
    "            n += 1\n",
    "    if n > 0:\n",
    "        print(f'    ** discarded {n} introns without a gene mapping')\n",
    "\n",
    "    print('  * writing BED files for QTL mapping')\n",
    "    gene_bed_df = pd.DataFrame(gene_bed_df, columns=bed_df.columns)\n",
    "    # sort by TSS\n",
    "    gene_bed_df = gene_bed_df.groupby('#chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "    #rename the samples if they named by file name (simply pick the first element with [.])\n",
    "    gene_bed_df.columns = list(gene_bed_df.columns[:4]) + [name.split('.')[0] if 'junc' in name else name for name in gene_bed_df.columns[4:]]\n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup.is_file():\n",
    "        sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=0, dtype={0:str,1:str})\n",
    "        #gene_bed_df.rename(columns=sample_participant_lookup_s.to_dict(), inplace=True)\n",
    "        # Create a dictionary mapping from sample_id to participant_id\n",
    "        column_mapping = dict(zip(sample_participant_lookup_s.index, sample_participant_lookup_s['participant_id']))\n",
    "        # Get the column names to be replaced\n",
    "        column_names = gene_bed_df.columns[4:]\n",
    "        # Replace the column names using the mapping dictionary\n",
    "        #new_column_names = [column_mapping.get(col, 'missing_data') for col in column_names]\n",
    "        #it should overlap with genotype in downstream anyways\n",
    "        new_column_names = [column_mapping.get(col, col) for col in column_names]\n",
    "        gene_bed_df.rename(columns=dict(zip(column_names, new_column_names)), inplace=True)\n",
    "\n",
    "    gene_bed_df = gene_bed_df.drop_duplicates()\n",
    "    qtl.io.write_bed(gene_bed_df, ${_output[0]:r})\n",
    "    gene_bed_df[['start', 'end']] = gene_bed_df[['start', 'end']].astype(np.int32)\n",
    "    gene_bed_df[gene_bed_df.columns[4:]] = gene_bed_df[gene_bed_df.columns[4:]].astype(np.float32)\n",
    "    group_s_df =  pd.Series(group_s).sort_values().reset_index()\n",
    "    group_s_df.columns = ['ID', 'gene']\n",
    "    group_s_df.to_csv(${_output[1]:r}, sep='\\t', index=False, header=True)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-happening",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Processing of psichomics output\n",
    "It occurs that the psichomatic by default grouped the isoforms by gene name, so only thing needs to be done is to extract this information and potentially renamed the gene symbol into ENSG ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-spelling",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_psichomics_isoforms]\n",
    "parameter: sample_participant_lookup = path()\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bn}.formated.bed.gz', f'{cwd:a}/{_input[0]:bn}.phenotype_group.txt'\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "    tss_df = qtl.io.gtf_to_tss_bed(${_input[1]:r}, feature='gene',phenotype_id = \"gene_id\" )\n",
    "    bed_df = pd.read_csv(${_input[0]:ar}, sep='\\t', skiprows=0)\n",
    "    bed_df[\"gene_id\"]  = [x[-1] for x in bed_df.ID.str.split(\"_\")]\n",
    "    sample_participant_lookup = Path(\"${sample_participant_lookup:a}\")\n",
    "    if \"start\" in  bed_df.columns:\n",
    "        bed_df = bed_df.drop([\"#Chr\",\"start\",\"end\"],axis = 1)\n",
    "    output = tss_df.merge(bed_df, left_on = [\"gene_id\"], right_on = [\"gene_id\"],how = \"right\").sort_values([\"chr\",\"start\"])\n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup.is_file():\n",
    "        sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=0, dtype={0:str,1:str})\n",
    "        output.rename(columns=sample_participant_lookup_s.to_dict(), inplace=True)\n",
    "\n",
    "    # Old code grouping by each gene\n",
    "    #bed_output = output.drop(\"gene_id\" , axis = 1)\n",
    "    #phenotype_group = output[[\"ID\",\"gene_id\"]]\n",
    "\n",
    "    bed_output = output.drop(\"gene_id\" , axis = 1)\n",
    "\n",
    "    # R version \n",
    "    #output = output%>%mutate(gene_type_id = map_chr(str_split( ID ,\"_\"),~paste0(.x[length(.x)],\"_\",.x[1] )) )\n",
    "    \n",
    "    # corrected in python\n",
    "    output = output.assign(gene_type_id = output['ID'].str.split('_').map(lambda x: x[-1] + '_' + x[0]))\n",
    "    phenotype_group = output[[\"ID\",\"gene_type_id\"]]\n",
    "\n",
    "    bed_output.to_csv(${_output[0]:nr},\"\\t\",index = False)\n",
    "    phenotype_group.to_csv(${_output[1]:r},\"\\t\",index = False,header=False)\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    bgzip -f ${_output[0]:n}\n",
    "    tabix ${_output[0]}\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container, entrypoint=entrypoint\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
