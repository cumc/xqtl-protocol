{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assigned-calculator",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Fine-mapping with PolyFun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-clock",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polar-learning",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "The purpose of this notebook is to demonstrate a functionally-informed fine-mapping workflow using the PolyFun method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-thermal",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Methods Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-problem",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Input "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-outside",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) GWAS summary statistics including the following variables: \n",
    "\n",
    "    - variant_id - variant ID \n",
    "    - P - p-value \n",
    "    - CHR - chromosome number \n",
    "    - BP - base pair position\n",
    "    - A1 - The effect allele (i.e., the sign of the effect size is with respect to A1)\n",
    "    - A2 - the second allele \n",
    "    - MAF - minor allele frequency \n",
    "    - BETA - effect size \n",
    "    - SE - effect size standard error\n",
    "\n",
    "\n",
    "2) Functional annotation files including the following columns: \n",
    "\n",
    "    - CHR - chromosome number\n",
    "    - BP base pair position (in hg19 coordinates)\n",
    "    - SNP - dbSNP reference number \n",
    "    - A1 - The effect allele \n",
    "    - A2 - the second allele\n",
    "    - Arbitrary additional columns representing annotations\n",
    "\n",
    "\n",
    "3) A `.l2.M` white-space delimited file containing a single line with the sums of the columns of each annotation\n",
    "\n",
    "4) LD-score files \n",
    "\n",
    "    - Strongly recommended that LD-score files include A1,A2 columns\n",
    "\n",
    "\n",
    "5) LD information, taken from one of three possible data sources:\n",
    "\n",
    "    - plink files with genotypes from a reference panel\n",
    "    - bgen file with genotypes from a reference panel\n",
    "    - pre-computed LD matrix\n",
    "\n",
    "    Optional if (4) is obtained and no plans to compute prior causal probabilities non-parametrically \n",
    "\n",
    "6) Ld-score weights files.\n",
    "\n",
    "    - Strongly recommended that weight files include A1,A2 columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-essence",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Output\n",
    "\n",
    "A `.gz` file containing input summary statistics columns and additionally the following columns:\n",
    "\n",
    "- PIP - posterior causal probability\n",
    "- BETA_MEAN - posterior mean of causal effect size (in standardized genotype scale)\n",
    "- BETA_SD - posterior standard deviation of causal effect size (in standardized genotype scale)\n",
    "- CREDIBLE_SET - the index of the first (typically smallest) credible set that the SNP belongs to (0 means none).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-asian",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-recipient",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Step 1 and 2 are optional if using pre-computed prior causal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-manhattan",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 1: Obtain functional annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-lawyer",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "For each chromosome, the following files need to be obtained: \n",
    "\n",
    "1) A `.gz` or `.parquet` annotations file containing the following columns:\n",
    "\n",
    "- CHR - chromosome number\n",
    "- BP base pair position\n",
    "- SNP - dbSNP reference number \n",
    "- A1 - The effect allele \n",
    "- A2 - the second allele\n",
    "- Arbitrary additional columns representing annotations \n",
    "\n",
    "2) A `.l2.M` white-space delimited file containing a single line with the sums of the columns of each annotation\n",
    "\n",
    "3) (Optional) A `l2.M_5_50` file that is the `.l2.M` file but only containing common SNPS (MAF between 5% and 50%) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pressing-baker",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The above files can be obtained either by using existing function annotation files, or by creating your own through other software such as `TORUS`.\n",
    "\n",
    "Existing function annotation files example: functional annotations for ~19 million UK Biobank imputed SNPs with MAF>0.1%, based on the baseline-LF 2.2.UKB annotations.\n",
    "\n",
    "Download (30G): https://data.broadinstitute.org/alkesgroup/LDSCORE/baselineLF_v2.2.UKB.polyfun.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-transition",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 2: Compute LD-scores for annotations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-master",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Precomputed LD-score files can be used. LD-score files can also be generated through the methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-touch",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Compute with reference panel of sequenced individuals "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-prize",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Reference panel should have at least 3000 sequenced individuals from target population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-capability",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "parameter: container = \"/mnt/mfs/statgen/containers/xqtl_pipeline_sif/polyfun.sif\"\n",
    "parameter: wd = path(\"./\")\n",
    "parameter: exe_dir = \"/usr/local/bin/\"\n",
    "parameter: name = \"demo\"\n",
    "parameter: genoFile = path(\"./\")\n",
    "parameter: annot_file = path(\"./\")\n",
    "parameter: sumstats = path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-study",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stretch-living",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    " sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb munged_sumstats  --sumstats \"./sumstat_demo.gz\"\n",
    " sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb ld_score  --genoFile XXX --annoFile XXX\n",
    "\n",
    "    \n",
    "    sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb prior_causal_prob  --sumstats \"/mnt/mfs/statgen/xqtl_workflow_testing/demo/polyfun/demo.sumstats_munged.parquet\"\n",
    "\n",
    "nohup sos run ~/GIT/xqtl-pipeline/pipeline/integrative_analysis/SuSiE_Ann/polyfun.ipynb L2_SLDSC \\\n",
    "--sumstats \"/home/at3535/polyfun/GCST90012877_buildGRCh37_colrenamed.txt.gz\" \\\n",
    "--ref_ld  \"/mnt/mfs/statgen/tl3030/baselineLF2.2.UKB/baselineLF2.2.UKB.1.l2.ldscore.parquet\" \\\n",
    "--ref_wgt  \"/mnt/mfs/statgen/tl3030/weights.UKB.l2.ldscore/weights.UKB.1.l2.ldscore.parquet\" -J 200 -q csg -c /home/hs3163/GIT/ADSPFG-xQTL/code/csg.yml &\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-april",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score]\n",
    "parameter: genoFile = path\n",
    "parameter: annot_file = path\n",
    "input: annot_file, genoFile\n",
    "output: f'{wd:a}/{name}.ref.ldscore.parquet'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/compute_ldscores.py \\\n",
    "        --bfile $[_input[1]:n] \\\n",
    "        --annot $[_input[0]] \\\n",
    "        --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-surface",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute with pre-computed UK Biobank LD matrices "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-linux",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Matrices download: https://data.broadinstitute.org/alkesgroup/UKBB_LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-loading",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_uk]\n",
    "parameter: annot_file = path\n",
    "input: annot_file\n",
    "output: f'{wd:a}/{name}.ukb.ldscore.parquet'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/compute_ldscores_from_ld.py \\\n",
    "        --annot $[_input[0]] \\\n",
    "        --ukb \\\n",
    "        --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-clerk",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 3: Compute with own pre-computed LD matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-hotel",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Own pre-computed LD matrices should be in `.bcor` format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-quest",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_score_own]\n",
    "parameter: annot_file = path\n",
    "parameter: sample_size = int\n",
    "parameter: bcor_files = paths\n",
    "input: annot_file,bcor_files\n",
    "output: f'{wd:a}/{name}.original.ldscore.parquet'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/compute_ldscores_from_ld.py $[_input[1]] \\\n",
    "        --annot $[_input[0]] \\\n",
    "        --out $[_output] \\\n",
    "        --n $[sample_size] \\\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-scotland",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 3: Compute Prior Causal Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-stake",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 1: Use precomputed prior causal probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-parallel",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Use precomputed prior causal probabilities of 19 million imputed UK Biobank SNPs with MAF>0.1%, based on a meta-analysis of 15 UK Biobank traits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-lingerie",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[prior_causal_prob]\n",
    "parameter: sumstats = path\n",
    "input: sumstats\n",
    "output: f'{wd:a}/{name}.pcp.gz'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/extract_snpvar.py \\\n",
    "        --sumstats $[_input] \\\n",
    "        --out $[_output] \\\n",
    "        --allow-missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-maker",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 2: Compute via L2-regularized extension of S-LDSC (preferred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-sword",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Compute via an L2-regularized extension of stratified LD-score regression (S-LDSC). Use the annotation and LD-score files produced in Step1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-password",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) Create a munged summary statistics file in a PolyFun-friendly parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-shareware",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[munged_sumstats]\n",
    "parameter: sumstats = path\n",
    "parameter: sample_size = 472868 \n",
    "parameter: min_info = 0.6\n",
    "parameter: min_maf = 0.01\n",
    "input: sumstats\n",
    "output: f'{wd:a}/{name}.sumstats_munged.parquet'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '127G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/munge_polyfun_sumstats.py \\\n",
    "      --sumstats $[_input] \\\n",
    "      --n $[sample_size] \\\n",
    "      --out $[_output] \\\n",
    "      --min-info $[min_info] \\\n",
    "      --min-maf $[min_maf]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-lincoln",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "2) Run PolyFun with L2-regularized S-LDSC\n",
    "- Require at least 45 GB of mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-worse",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[L2_SLDSC]\n",
    "# a ld score file with surfix l2.ldscore.parquet\n",
    "parameter: ref_ld = path\n",
    "# another ld score file with surfix l2.ldscore.parquet, different from ref_ld\n",
    "parameter: ref_wgt = path\n",
    "parameter: partitions = \"\"\n",
    "input: ref_ld, ref_wgt,output_from(\"munged_sumstats\")\n",
    "# parameter: sumstat = _input[2]\n",
    "output: f'{wd:a}/{name}.ldsldsc.parquent'\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '24h',  mem = '127G', tags = f'{step_name}_{_output[0]:bn}'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/polyfun.py \\\n",
    "        --compute-h2-L2 \\\n",
    "        --output-prefix $[_output] \\\n",
    "        --sumstats $[_input[2]] \\\n",
    "        --ref-ld-chr $[_input[0]:nnnn].\\\n",
    "        --w-ld-chr $[_input[1]:nnnn]. \\\n",
    "        --allow-missing $[\"\" if partitions else \"--no-partitions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-chain",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Method 3: Compute Non-parametrically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-omega",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "1) Create a munged summary statistics file in a PolyFun-friendly parquet format.\n",
    "Duplicated cells are commented out, the input of [ld_snpbin] is the output from [L2_regu_SLDSC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focal-wagon",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[munged_sumstats2]\n",
    "#parameter: sumstats = AD_sumstats_Jansenetal_2019sept.txt.gz\n",
    "#parameter: sample_size = int\n",
    "#parameter: container = none\n",
    "#bash: container = container \n",
    "#    mkdir -p SLDSC_output\n",
    "#    python munge_polyfun_sumstats.py \\\n",
    "#      --sumstats sumstats \\\n",
    "#      --n sample_size \\\n",
    "#      --out /SLDSC_output/sumstats_munged.parquet \\\n",
    "#      --min-info 0 \\\n",
    "#      --min-maf 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-portfolio",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "2) Run PolyFun with L2-regularized S-LDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-cooler",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# [L2_regu_SLDSC2]\n",
    "# \n",
    "# parameter: container = none\n",
    "# paramter: ref_ld = example_data/annotations.\n",
    "# parameter: ref_wgt = example_data/weights.\n",
    "# bash: container=container\n",
    "#     python polyfun.py \\\n",
    "#     --compute-h2-L2 \\\n",
    "#     --output-prefix output/testrun \\\n",
    "#     --sumstats example_data/sumstats.parquet \\\n",
    "#     --ref-ld-chr ref_ld \\\n",
    "#     --w-ld-chr ref_wgt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-headline",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "3) Compute LD-scores for each SNP bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "casual-nepal",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ld_snpbin]\n",
    "depends: sos_step(\"L2_regu_SLDSC\")\n",
    "parameter: genoFile = path\n",
    "parameter: chrom = int\n",
    "input: annot_file, genoFile\n",
    "output: f'{wd:a}/{name}.snpbin.ldscore.parquet'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "     python $[exe_dir]/polyfun.py \\\n",
    "        --compute-ldscores \\\n",
    "        --bfile-chr $[_input[1]:n] \\\n",
    "        --output-prefix $[_output] \\\n",
    "        --chr $[chrom]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-genre",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "4) Re-estimate per-SNP heritabilities via S-LDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-mount",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#[re_SLDSC]\n",
    "#bash:\n",
    "#    python polyfun.py \\\n",
    "#    --compute-h2-bins \\\n",
    "#    --output-prefix output/testrun \\\n",
    "#    --sumstats example_data/sumstats.parquet \\\n",
    "#    --w-ld-chr example_data/weights.\n",
    "\n",
    "[L2_SLDSC_bins]\n",
    "paramter: ref_ld = path\n",
    "parameter: ref_wgt = path\n",
    "parameter: partitions = \"\"\n",
    "input: ref_ld, ref_wgt,output_from(\"munged_sumstats\")\n",
    "output: f'{wd:a}/{name}.txt.gz'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/polyfun.py \\\n",
    "        --compute-h2-bins \\\n",
    "        --output-prefix $[_output] \\\n",
    "        --sumstats $[_input[2]] \\\n",
    "        --ref-ld-chr $[_input[0]]\\\n",
    "        --w-ld-chr $[_input[1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-sweden",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Step 4: Functionally informed fine mapping with finemapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-importance",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Input summary statistics file must have `SNPVAR` column (per-SNP heritability) to perform functionally-informed fine-mapping. To fine-map without annotations, use additional parameter `--non-funct`. The summary statistical file then will not require the `SNPVAR` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-barcelona",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fine_mapping]\n",
    "parameter: genoFile = path#example_data/chr1\n",
    "parameter: sumstat = path#example_data/chr1.finemap_sumstats.txt.gz\n",
    "parameter: sample_size = 383290\n",
    "parameter: chrom = 1\n",
    "parameter: start = 46000001\n",
    "parameter: end = 49000001\n",
    "parameter: output_path = \"output/finemap.1.46000001.49000001.gz\"\n",
    "parameter: max_num_causal = 5\n",
    "input: genoFile,sumstat\n",
    "output: f'{wd:a}/output/finemap.{chrom}.{start}.{end}.gz'\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout' , container = container\n",
    "    python $[exe_dir]/finemapper.py \\\n",
    "        --geno $[_input[0]] \\\n",
    "        --sumstats $[_input[1]]  \\\n",
    "        --n $[sample_size] \\\n",
    "        --chr $[chrom] \\\n",
    "        --start $[start] \\\n",
    "        --end $[end] \\\n",
    "        --method susie \\\n",
    "        --max-num-causal $[max_num_causal] \\\n",
    "        --cache-dir $[_output:d]/cache \\\n",
    "        --out $[_output]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-cookie",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "## Minimal Working Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-flavor",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "# FIXME: Run examples and summaries through. Uplaod all example data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-barrier",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Example 1: Functionally-informed fine-mapping using summary statistics file with precomputed prior causal probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-scholar",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "bash: container = '/mnt/mfs/statgen/tl3030/SIF/polyfun_ninth.sif'\n",
    "    mkdir output\n",
    "    python /home/at3535/polyfun/extract_snpvar.py \\\n",
    "        --sumstats /home/at3535/polyfun/AD_sumstats_Jansenetal_2019sept.txt.gz \\\n",
    "        --out output/AD_snps_with_var.gz \\\n",
    "        --allow-missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expensive-newark",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "bash: container = '/mnt/mfs/statgen/tl3030/SIF/polyfun_ninth.sif'\n",
    "    python /home/at3535/polyfun/finemapper.py \\\n",
    "    --geno /home/at3535/polyfun/UKB_expandedwhiteonly_phenotypeindepqc_410905indiv_528206snps_102720_chr1 \\\n",
    "    --sumstats output/AD_snps_with_var.gz \\\n",
    "    --n 410905 \\\n",
    "    --chr 1 \\\n",
    "    --start 46000001 \\\n",
    "    --end 49000001 \\\n",
    "    --method susie \\\n",
    "    --max-num-causal 5 \\\n",
    "    --allow-missing \\\n",
    "    --cache-dir LD_cache \\\n",
    "    --out output/finemap.1.46000001.49000001.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informed-collect",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Example 2: Functionally-informed fine-mapping using summary statistics file generated from pre-obtained annotation and LD-score files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-sydney",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "bash: container = '/mnt/mfs/statgen/tl3030/SIF/polyfun_ninth.sif'\n",
    "    mkdir output\n",
    "    python /home/at3535/polyfun/munge_polyfun_sumstats.py \\\n",
    "      --sumstats /home/at3535/polyfun/example_data/boltlmm_sumstats.gz \\\n",
    "      --n 472868 \\\n",
    "      --out output/sumstats_munged.parquet \\\n",
    "      --min-info 0.6 \\\n",
    "      --min-maf 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-gates",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "bash: container = '/mnt/mfs/statgen/tl3030/SIF/polyfun_ninth.sif'\n",
    "    python /home/at3535/polyfun/polyfun.py \\\n",
    "    --compute-h2-L2 \\\n",
    "    --no-partitions \\\n",
    "    --output-prefix output/testrun \\\n",
    "    --sumstats output/sumstats_munged.parquet \\\n",
    "    --ref-ld-chr /home/at3535/polyfun/example_data/annotations. \\\n",
    "    --w-ld-chr /home/at3535/polyfun/example_data/weights. \\\n",
    "    --allow-missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-lyric",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "bash: container = '/mnt/mfs/statgen/tl3030/SIF/polyfun_ninth.sif't\n",
    "    python /home/at3535/polyfun/finemapper.py \\\n",
    "    --geno /home/at3535/polyfun/UKB_expandedwhiteonly_phenotypeindepqc_410905indiv_528206snps_102720_chr1 \\\n",
    "    --sumstats output/testrun.1.snpvar_ridge_constrained.gz \\\n",
    "    --n 410905 \\\n",
    "    --chr 1 \\\n",
    "    --start 46000000 \\\n",
    "    --end 490000010 \\\n",
    "    --method susie \\\n",
    "    --max-num-causal 5 \\\n",
    "    --allow-missing \\\n",
    "    --cache-dir LD_cache \\\n",
    "    --out output/finemap.1.46000000.49000000.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-standard",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-sussex",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "#### Example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "super-warrant",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHR\tSNP\tBP\tA1\tA2\tSNPVAR\tZ\tN\tP\tPIP\tBETA_MEAN\tBETA_SD\tDISTANCE_FROM_CENTER\tCREDIBLE_SET\n",
      "1\trs2088102\t46032974\tT\tC\t1.70060e-06\t1.25500e+01\t383290\t3.97510e-36\t1.00000e+00\t-2.03917e-02\t1.61901e-03\t1456799\t1\n",
      "1\trs7528714\t47966058\tG\tA\t1.18040e-06\t5.14320e+00\t383290\t2.70098e-07\t9.97870e-01\t7.42146e-03\t1.62305e-03\t476285\t2\n",
      "1\trs7528075\t47870271\tG\tA\t1.18040e-06\t4.40160e+00\t383290\t1.07456e-05\t9.76545e-01\t-5.98945e-03\t1.81667e-03\t380498\t3\n",
      "1\trs212968\t48734666\tG\tA\t1.70060e-06\t-3.01130e+00\t383290\t2.60132e-03\t3.75823e-01\t-1.56305e-03\t2.23942e-03\t1244893\t0\n",
      "1\trs2622911\t47837404\tC\tA\t1.70060e-06\t3.12520e+00\t383290\t1.77684e-03\t3.71804e-01\t1.54312e-03\t2.22812e-03\t347631\t0\n",
      "1\trs4511165\t48293181\tG\tA\t1.70060e-06\t-1.18940e+00\t383290\t2.34282e-01\t5.75970e-02\t1.60630e-04\t7.52226e-04\t803408\t0\n",
      "1\trs3766196\t47284526\tC\tA\t6.93040e-06\t-5.92360e-02\t383290\t9.52764e-01\t4.89776e-02\t-5.06776e-07\t3.48039e-04\t205247\t0\n",
      "1\trs12567716\t48197570\tT\tC\t1.18040e-06\t2.14810e+00\t383290\t3.17058e-02\t4.45128e-02\t-1.28281e-04\t6.81457e-04\t707797\t0\n",
      "1\trs4927234\t48384796\tG\tA\t1.70060e-06\t-7.59600e-01\t383290\t4.47494e-01\t3.41123e-02\t7.74978e-05\t5.04755e-04\t895023\t0\n"
     ]
    }
   ],
   "source": [
    "bash:\n",
    "    gzcat output/finemap.1.46000001.49000001.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "variable-nebraska",
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number of variants with PIP > 0.5: 3\\n', 'Number of variants with PIP > 0.95: 3\\n', 'Number of variants that have credible sets: 3\\n', 'Number of unique credible sets: 3\\n', 'Average number of variants per credible set: 1.0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('output/finemap.1.46000001.49000001.gz', sep=\"\\t\")\n",
    "\n",
    "data.head(5)\n",
    "    \n",
    "num_var_cs = np.count_nonzero(data['CREDIBLE_SET'])\n",
    "total_cs = len(data.CREDIBLE_SET.unique())- 1\n",
    "avg_var_cs = float(num_var_cs) / total_cs\n",
    "pip50 = sum(1 for i in data['PIP'] if i >0.5)\n",
    "pip95 = sum(1 for i in data['PIP'] if i >0.95)\n",
    "\n",
    "result = \"Number of variants with PIP > 0.5: \" + str(pip50) + \"\\n\" + \"Number of variants with PIP > 0.95: \" + str(pip95) + \"\\n\" \\\n",
    "    + \"Number of variants that have credible sets: \" + str(num_var_cs) + \"\\n\" \\\n",
    "    + \"Number of unique credible sets: \" + str(total_cs) + \"\\n\" \\\n",
    "    + \"Average number of variants per credible set: \" + str(avg_var_cs) \n",
    "\n",
    "\n",
    "with open('results.txt', 'a') as the_file:\n",
    "    the_file.write(result)\n",
    "\n",
    "with open('results.txt') as f:\n",
    "    contents = f.readlines()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-seventh",
   "metadata": {
    "kernel": "Python3"
   },
   "source": [
    "#### Example 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrong-cliff",
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [],
   "source": [
    "bash:\n",
    "    gzcat output/finemap.1.460000010.49000000.gz | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "together-feeding",
   "metadata": {
    "kernel": "Python3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Number of variants with PIP > 0.5: 3\\n', 'Number of variants with PIP > 0.95: 3\\n', 'Number of variants that have credible sets: 3\\n', 'Number of unique credible sets: 3\\n', 'Average number of variants per credible set: 1.0']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('output/finemap.1.46000000.49000000.gz', sep=\"\\t\")\n",
    "\n",
    "data.head(5)\n",
    "    \n",
    "num_var_cs = np.count_nonzero(data['CREDIBLE_SET'])\n",
    "total_cs = len(data.CREDIBLE_SET.unique())- 1\n",
    "avg_var_cs = float(num_var_cs) / total_cs\n",
    "pip50 = sum(1 for i in data['PIP'] if i >0.5)\n",
    "pip95 = sum(1 for i in data['PIP'] if i >0.95)\n",
    "\n",
    "result = \"Number of variants with PIP > 0.5: \" + str(pip50) + \"\\n\" + \"Number of variants with PIP > 0.95: \" + str(pip95) + \"\\n\" \\\n",
    "    + \"Number of variants that have credible sets: \" + str(num_var_cs) + \"\\n\" \\\n",
    "    + \"Number of unique credible sets: \" + str(total_cs) + \"\\n\" \\\n",
    "    + \"Average number of variants per credible set: \" + str(avg_var_cs) \n",
    "\n",
    "\n",
    "with open('results.txt', 'a') as the_file:\n",
    "    the_file.write(result)\n",
    "\n",
    "with open('results.txt') as f:\n",
    "    contents = f.readlines()\n",
    "    print(contents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
