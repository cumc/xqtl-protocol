{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "described-restoration",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Reference data standardization\n",
    "\n",
    "This module provides reference data download, indexing and preprocessing (if necessary), in preparation for use throughout the pipeline.\n",
    "\n",
    "We have included the PDF document compiled by data standardization subgroup in the [on Google Drive](https://drive.google.com/file/d/1R5sw5o8vqk_mbQQb4CGmtH3ldu1T3Vu0/view?usp=sharing) as well as on [ADSP Dashboard](https://www.niagads.org/adsp/content/adspgcadgenomeresources-v2pdf). It contains the reference data to use for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-prescription",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This module is based on the [TOPMed workflow from Broad](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md). The reference data after we process it (details see Methods section and the rest of the analysis) can be found [in this folder on Google Drive](https://drive.google.com/drive/folders/19fmoII8yS7XE7HFcMU4OfvC2bL1zMD_P). \n",
    "\n",
    "### Processed reference file for RNA-seq based expression quantification\n",
    "\n",
    "**We have decided to use these preprocessed reference files for RNA-seq expression quantification. They may not be applicable to other molecular phenotypes.**\n",
    "\n",
    "Specifically, the list of reference files to be used are:\n",
    "\n",
    "1. `GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.{dict,fasta,fasta.fai}`\n",
    "2. `Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf` for stranded protocol, and `Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf` for unstranded protocol.\n",
    "3. Everything under `STAR_Index` folder\n",
    "4. Everything under `RSEM_Index` folder\n",
    "5. Optionally, for quality control, `gtf_ref.flat`\n",
    "\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "Workflows implemented include:\n",
    "\n",
    "### Convert transcript feature file gff3 to gtf\n",
    "\n",
    "- Input: an uncompressed gff3 file.(i.e. can be view via cat)\n",
    "- Output: a gtf file.\n",
    "\n",
    "### Collapse transcript features into genes\n",
    "\n",
    "- Input: a gtf file.\n",
    "- Output: a gtf file with collapesed gene model.\n",
    "\n",
    "### Generate STAR index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of STAR index.\n",
    "\n",
    "### Generate RSEM index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of RSEM index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-running",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Example commands\n",
    "\n",
    "To download reference data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-suite",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb download_hg_reference --cwd reference_data    &\n",
    "sos run pipeline/reference_data.ipynb download_gene_annotation --cwd reference_data &\n",
    "sos run pipeline/reference_data.ipynb download_ercc_reference --cwd reference_data &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-blood",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format reference data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-average",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb hg_reference \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-reference reference_data/ERCC92.fa \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.fa \\\n",
    "    --container container/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-reputation",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "\n",
    "sos run pipeline/reference_data.ipynb hg_gtf \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-gtf /mnt/mfs/statgen/snuc_pseudo_bulk/data/reference_data/genes.gtf \\\n",
    "    --hg-reference data/reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --containers containers/rna_quantification.sif -J 1 -q csg -c csg.yml &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-karaoke",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format gene feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-colonial",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb gene_annotation \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-gtf reference_data/ERCC92.gtf \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.gtf \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --container container/rna_quantification.sif --stranded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-valuable",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Notice that for un-stranded RNA-seq protocol please use switch `--no-stranded` to the command above instead of `--stranded`. More details can be found later in the document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-whole",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate STAR index using the GTF annotation file before gene model collapse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-nancy",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb STAR_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container containers/rna_quantification.sif \\\n",
    "    --mem 40G "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-vulnerability",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Notice that command above requires at least 40G of memory, and takes quite a while to complete**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-sperm",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate RSEM index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-advice",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb RSEM_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container container/rna_quantification.sif \\\n",
    "    --mem 40G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27548a-5475-4dd9-83d0-38a7aeb0b1f7",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate SUPPA annotation for psichomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583a351-d2ab-4408-bdce-107acd2acba8",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb SUPPA_annotation \\\n",
    "    --hg_gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformated.gtf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-export",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regular-minority",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run reference_data.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  download_hg_reference\n",
      "  download_gene_annotation\n",
      "  download_ercc_reference\n",
      "  gff3_to_gtf\n",
      "  hg_reference\n",
      "  hg_gtf\n",
      "  ercc_gtf\n",
      "  gene_annotation\n",
      "  STAR_index\n",
      "  RSEM_indexing\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        The output directory for generated files.\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  download_hg_reference:\n",
      "  download_gene_annotation:\n",
      "  download_ercc_reference:\n",
      "  gff3_to_gtf:\n",
      "    Workflow Options:\n",
      "      --gff3-file VAL (as path, required)\n",
      "  hg_reference_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "                        Path to HG reference file\n",
      "  hg_reference_2:\n",
      "    Workflow Options:\n",
      "      --ercc-reference VAL (as path, required)\n",
      "  hg_reference_3:\n",
      "  hg_gtf_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "      --hg-gtf VAL (as path, required)\n",
      "  hg_gtf_2:\n",
      "    Workflow Options:\n",
      "      --[no-]collapse-only (default to False)\n",
      "                        Use this for stranded protocol (optional)\n",
      "  ercc_gtf:\n",
      "    Workflow Options:\n",
      "      --ercc-gtf VAL (as path, required)\n",
      "  gene_annotation:\n",
      "  STAR_index:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hg-reference VAL (as path, required)\n",
      "      --sjdbOverhang 100 (as int)\n",
      "                        Specifies the length of the genomic sequence around the\n",
      "                        annotated junction to be used in constructing the splice\n",
      "                        junctions database. Ideally, this length should be equal\n",
      "                        to the ReadLength-1, where ReadLength is the length of\n",
      "                        the reads. Default choice follows from TOPMed pipeline\n",
      "                        recommendation.\n",
      "  RSEM_indexing:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hg-reference VAL (as path, required)\n"
     ]
    }
   ],
   "source": [
    "sos run reference_data.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-advocacy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files.\n",
    "parameter: cwd = path(\"output\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "cwd = path(f'{cwd:a}')\n",
    "from sos.utils import expand_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-delay",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-framing",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_hg_reference]\n",
    "output: f\"{cwd:a}/GRCh38_full_analysis_set_plus_decoy_hla.fa\"\n",
    "download: dest_dir = cwd\n",
    "    ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-liabilities",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_gene_annotation]\n",
    "output: f\"{cwd:a}/Homo_sapiens.GRCh38.103.chr.gtf\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    http://ftp.ensembl.org/pub/release-103/gtf/homo_sapiens/Homo_sapiens.GRCh38.103.chr.gtf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-ground",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_ercc_reference]\n",
    "output: f\"{cwd:a}/ERCC92.gtf\", f\"{cwd:a}/ERCC92.fa\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    https://tools.thermofisher.com/content/sfs/manuals/ERCC92.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-paradise",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## GFF3 to GTF formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-teens",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gff3_to_gtf]\n",
    "parameter: gff3_file = path\n",
    "input: gff3_file\n",
    "output: f'{cwd}/{_input:bn}.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "        gffread ${_input} -T -o ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-attraction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## HG reference file preprocessing\n",
    "1. Remove the HLA/ALT/Decoy record from the fasta -- because none of the downstreams RNA-seq calling pipeline component can handle them properly.\n",
    "2. Adding in ERCC information to the fasta file -- even if ERCC is not included in the RNA-seq library it does not harm to add them.\n",
    "3. Generating index for the fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-herald",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_1 (HLA ALT Decoy removal)]\n",
    "# Path to HG reference file\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference\n",
    "output:  f'{cwd}/{_input:bn}.noALT_noHLA_noDecoy.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}', 'r') as fasta:\n",
    "        contigs = fasta.read()\n",
    "        contigs = contigs.split('>')\n",
    "        contig_ids = [i.split(' ', 1)[0] for i in contigs]\n",
    "\n",
    "        # exclude ALT, HLA and decoy contigs\n",
    "        filtered_fasta = '>'.join([c for i,c in zip(contig_ids, contigs)\n",
    "        if not (i[-4:]=='_alt' or i[:3]=='HLA' or i[-6:]=='_decoy')])\n",
    "    \n",
    "    with open('${_output}', 'w') as fasta:\n",
    "        fasta.write(filtered_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-screening",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_2 (merge with ERCC reference)]\n",
    "parameter: ercc_reference = path\n",
    "output: f'{cwd}/{_input:bn}_ERCC.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    sed 's/ERCC-/ERCC_/g' ${ercc_reference} >  ${ercc_reference:n}.patched.fa\n",
    "    cat ${_input} ${ercc_reference:n}.patched.fa > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-table",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[hg_reference_3 (index the fasta file)]\n",
    "output: f'{cwd}/{_input:bn}.dict'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    samtools faidx ${_input}\n",
    "    java -jar /opt/picard-tools/picard.jar \\\n",
    "    CreateSequenceDictionary \\\n",
    "    R=${_input} \\\n",
    "    O=${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-glass",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Transcript and gene model reference processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-colors",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step modify the `gtf` file for following reasons:\n",
    "\n",
    "1. RSEM require GTF input to have the same chromosome name format (with `chr` prefix) as the fasta file. **although for STAR, this problem can be solved by the now commented --sjdbGTFchrPrefix \"chr\" option, we have to add `chr` to it for use with RSEM**. \n",
    "2. Gene model collapsing script `collapse_annotation.py` from GTEx require the gtf have `transcript_type` instead `transcript_biotype` in its annotation. We rename it here, although **this problem can also be solved by modifying the collapse_annotation.py while building the docker, since we are doing 1 above we think it is better to add in another customization here.**\n",
    "3. Adding in ERCC information to the `gtf` reference.\n",
    "\n",
    "We may reimplement 1 and 2 if the problem with RSEM is solved, or when RSEM is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-camel",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_1 (add chr prefix to gtf file)]\n",
    "parameter: hg_reference = path\n",
    "parameter: hg_gtf = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f'{cwd}/{_input[1]:bn}.reformatted.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    options(scipen = 999)\n",
    "    con <- file(\"${_input[0]}\",\"r\")\n",
    "    fasta <- readLines(con,n=1)\n",
    "    close(con)\n",
    "    gtf = read_delim(\"${_input[1]}\", \"\\t\",  col_names  = F, comment = \"#\", col_types=\"ccccccccc\")\n",
    "    if(!str_detect(fasta,\">chr\")) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = str_remove_all(X1,\"chr\"))\n",
    "    } else if (!any(str_detect(gtf$X1[1],\"chr\"))) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = paste0(\"chr\",X1))\n",
    "    } else (gtf_mod = gtf)\n",
    "    if(any(str_detect(gtf_mod$X9, \"transcript_biotype\"))) {\n",
    "      gtf_mod = gtf_mod%>%mutate(X9 = str_replace_all(X9,\"transcript_biotype\",\"transcript_type\"))\n",
    "    }\n",
    "    gtf_mod%>%write.table(\"${_output}\",sep = \"\\t\",quote = FALSE,col.names = F,row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-notebook",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**Text below is taken from https://github.com/broadinstitute/gtex-pipeline/tree/master/gene_model**\n",
    "\n",
    "\n",
    "Gene-level expression and eQTLs from the GTEx project are calculated based on a collapsed gene model (i.e., combining all isoforms of a gene into a single transcript), according to the following rules:\n",
    "\n",
    "1. Transcripts annotated as “retained_intron” or “read_through” are excluded. Additionally, transcripts that overlap with annotated read-through transcripts may be blacklisted (blacklists for GENCODE v19, 24 & 25 are provided in this repository; no transcripts were blacklisted for v26).\n",
    "2. The union of all exon intervals of each gene is calculated.\n",
    "3. Overlapping intervals between genes are excluded from all genes.\n",
    "\n",
    "\n",
    "The purpose of step 3 is primarily to exclude overlapping regions from genes annotated on both strands, which can't be unambiguously quantified from unstranded RNA-seq (GTEx samples were sequenced using an unstranded protocol). For stranded protocols, this step can be skipped by adding the `--collapse_only` flag.\n",
    "\n",
    "Further documentation is available on the [GTEx Portal](https://gtexportal.org/home/documentationPage#staticTextAnalysisMethods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-criticism",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_2 (collapsed gene model)]\n",
    "parameter: stranded = bool\n",
    "output: f'{_input:n}{\".collapse_only\" if stranded else \"\"}.gene.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    collapse_annotation.py ${\"--collapse_only\" if stranded else \"\"} ${_input} ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-conducting",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ercc_gtf (Preprocess ERCC gtf file)]\n",
    "parameter: ercc_gtf = path\n",
    "input: ercc_gtf\n",
    "output: f'{cwd}/{_input:bn}.genes.patched.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}') as exon_gtf, open('${_output}', 'w') as gene_gtf:\n",
    "        for line in exon_gtf:\n",
    "            f = line.strip().split('\\t')\n",
    "            f[0] = f[0].replace('-','_')  # required for RNA-SeQC/GATK (no '-' in contig name)\n",
    "        \n",
    "            attr = f[8]\n",
    "            if attr[-1]==';':\n",
    "                attr = attr[:-1]\n",
    "            attr = dict([i.split(' ') for i in attr.replace('\"','').split('; ')])\n",
    "            # add gene_name, gene_type\n",
    "            attr['gene_name'] = attr['gene_id']\n",
    "            attr['gene_type'] = 'ercc_control'\n",
    "            attr['gene_status'] = 'KNOWN'\n",
    "            attr['level'] = 2\n",
    "            for k in ['id', 'type', 'name', 'status']:\n",
    "                attr['transcript_'+k] = attr['gene_'+k]\n",
    "        \n",
    "            attr_str = []\n",
    "            for k in ['gene_id', 'transcript_id', 'gene_type', 'gene_status', 'gene_name',\n",
    "                'transcript_type', 'transcript_status', 'transcript_name']:\n",
    "                attr_str.append('{0:s} \"{1:s}\";'.format(k, attr[k]))\n",
    "            attr_str.append('{0:s} {1:d};'.format('level', attr['level']))\n",
    "            f[8] = ' '.join(attr_str)\n",
    "        \n",
    "            # write gene, transcript, exon\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['gene']+f[3:])+'\\n')\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['transcript']+f[3:])+'\\n')\n",
    "            f[8] = ' '.join(attr_str[:2])\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['exon']+f[3:])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-applicant",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gene_annotation]\n",
    "input: output_from(\"hg_gtf_1\"), output_from(\"hg_gtf_2\"), output_from(\"ercc_gtf\")\n",
    "output: f'{cwd}/{_input[0]:bn}.ERCC.gtf', f'{cwd}/{_input[1]:bn}.ERCC.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container\n",
    "    cat ${_input[0]} ${_input[2]} > ${_output[0]}\n",
    "    cat ${_input[1]} ${_input[2]} > ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-needle",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Generating index file for `STAR` \n",
    "\n",
    "This step generate the index file for STAR alignment. This file just need to generate once and can be re-used. \n",
    "\n",
    "**At least 40GB of memory is needed**.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. Both of them needs to be unzipped. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads. We use 100 here as recommended by the TOPMed pipeline. See here for [some additional discussions](https://groups.google.com/g/rna-star/c/h9oh10UlvhI/m/BfSPGivUHmsJ). \n",
    "\n",
    "### Step Output\n",
    "\n",
    "* Indexing file stored in `{cwd}/STAR_index`, which will be used by `STAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-karen",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_index]\n",
    "parameter: hg_gtf = path\n",
    "parameter: hg_reference = path\n",
    "# Specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "# Default choice follows from TOPMed pipeline recommendation.\n",
    "parameter: sjdbOverhang = 100\n",
    "fail_if(expand_size(mem) < expand_size('40G'), msg = \"At least 40GB of memory is required for this step\")\n",
    "input: hg_reference, hg_gtf\n",
    "output: f\"{cwd}/STAR_Index/chrName.txt\", \n",
    "        f\"{cwd}/STAR_Index/transcriptInfo.tab\", f\"{cwd}/STAR_Index/exonGeTrInfo.tab\", \n",
    "        f\"{cwd}/STAR_Index/SAindex\", f\"{cwd}/STAR_Index/SA\", f\"{cwd}/STAR_Index/genomeParameters.txt\", \n",
    "        f\"{cwd}/STAR_Index/chrStart.txt\", f\"{cwd}/STAR_Index/sjdbList.out.tab\", \n",
    "        f\"{cwd}/STAR_Index/exonInfo.tab\", f\"{cwd}/STAR_Index/sjdbList.fromGTF.out.tab\", \n",
    "        f\"{cwd}/STAR_Index/chrLength.txt\", f\"{cwd}/STAR_Index/sjdbInfo.txt\", \n",
    "        f\"{cwd}/STAR_Index/Genome\", f\"{cwd}/STAR_Index/chrNameLength.txt\", \n",
    "        f\"{cwd}/STAR_Index/geneInfo.tab\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[1]:n}.stderr', stdout = f'{_output[1]:n}.stdout'\n",
    "    STAR --runMode genomeGenerate \\\n",
    "         --genomeDir ${_output[0]:d} \\\n",
    "         --genomeFastaFiles ${_input[0]} \\\n",
    "         --sjdbGTFfile ${_input[1]} \\\n",
    "         --runThreadN ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-proxy",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Generating index file for `RSEM`\n",
    "\n",
    "This step generate the indexing file for `RSEM`. This file just need to generate once.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "\n",
    "### Step Outputs\n",
    "* Indexing file stored in `RSEM_index_dir`, which will be used by `RSEM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-oregon",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[RSEM_index]\n",
    "parameter: hg_gtf = path\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f\"{cwd}/RSEM_Index/rsem_reference.n2g.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.grp\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.ti\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.chrlist\", f\"{cwd}/RSEM_Index/rsem_reference.seq\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.transcripts.fa\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[1]:n}.stderr', stdout = f'{_output[1]:n}.stdout'\n",
    "    rsem-prepare-reference \\\n",
    "            ${_input[0]} \\\n",
    "            ${_output[1]:n} \\\n",
    "            --gtf ${_input[1]} \\\n",
    "            --num-threads ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-lotus",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Generation of RefFlat file \n",
    "This file is needed for picard CollectRnaSeqMetrics module, which in turn \n",
    ">produces metrics describing the distribution of the bases within the transcripts. It calculates the total numbers and the fractions of nucleotides within specific genomic regions including untranslated regions (UTRs), introns, intergenic sequences (between discrete genes), and peptide-coding sequences (exons). This tool also determines the numbers of bases that pass quality filters that are specific to Illumina data (PF_BASES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-blank",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[RefFlat_generation]\n",
    "parameter: hg_gtf = path\n",
    "input: hg_gtf\n",
    "output: f'{_input:n}.ref.flat'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    gtfToGenePred ${_input}  ${_output}.tmp -genePredExt -geneNameAsName2\n",
    "    awk -F'\\t' -v OFS=\"\\t\" '{$1=$12 OFS $1;}7' ${_output}.tmp | cut -f 1-11 > ${_output}\n",
    "    rm ${_output}.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae142156-b455-4a04-8c61-604949abd5c7",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Generation of SUPPA annotation for psichomics.\n",
    "The generation of custom alternative splicing annotation is based on [this tutorial](https://rpubs.com/nuno-agostinho/preparing-AS-annotation). The way to generate the local alternative splicing suppa output is documented on [SUPPA github page](https://github.com/comprna/SUPPA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0925b02-a448-4276-ae51-614e47101d4c",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb SUPPA_annotation \\\n",
    "    --hg_gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformated.gtf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb074fe-4236-4c15-b815-9aec345e674d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SUPPA_annotation_1]\n",
    "parameter: hg_gtf = path\n",
    "input: hg_gtf\n",
    "output: f'{cwd}/hg38.{_input:bn}_SE_strict.ioe' # The stderr file must not shared the same start with the output file\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{cwd}/{_input:bn}.stderr', stdout = f'{cwd}/{_input:bn}.stdout'\n",
    "    python ~/GIT/SUPPA/suppa.py generateEvents -i ${_input} -o ${cwd}/hg38.${_input:bn} -f ioe -e SE SS MX RI FL\n",
    "[SUPPA_annotation_2]\n",
    "parameter: hg_gtf = path\n",
    "output: f'{cwd}/{hg_gtf:bn}.SUPPA_annotation.rds'\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    library(\"psichomics\")\n",
    "    suppa <- parseSuppaAnnotation(\"${_input:d}\", genome=\"hg38\") \n",
    "    annot <- prepareAnnotationFromEvents(suppa)\n",
    "    saveRDS(annot, file=${_output:r})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
