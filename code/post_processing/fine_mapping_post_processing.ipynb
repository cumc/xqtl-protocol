{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "legislative-cylinder",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# SuSIE results post process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-robertson",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This notebook is to post-process the susie results into different text file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thirty-jurisdiction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Extracting susie results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-routine",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_to_tsv \\\n",
    "    --cwd output/test --rds_path `ls output/test/cache/*rds | head ` --region-list <(head -50  ./dlpfc_region_list) --container containers/stephenslab.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-speed",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Extracting susie_rss results for ADGWAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unavailable-dayton",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_to_tsv \\\n",
    "    --cwd output/ADGWAS_finemapping_extracted/Bellenguez/ --rds_path `ls GWAS_Finemapping_Results/Bellenguez/ADGWAS2022*rds ` \\\n",
    "    --region-list ~/1300_hg38_EUR_LD_blocks_orig.tsv \\\n",
    "    --container containers/stephenslab.sif \n",
    "\n",
    "sos run pipeline/SuSiE_post_processing.ipynb susie_tsv_collapse \\\n",
    "    --cwd output/ADGWAS_finemapping_extracted --tsv_path `ls output/ADGWAS_finemapping_extracted/*lbf.tsv` \\\n",
    "    --container containers/stephenslab.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-index",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Extracting fsusie results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-broadcast",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_haQTL_pos --rds_path `ls output/f_susie_tad_haQTL_pos/cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-remark",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos_selected/ --rds_path `ls output/f_susie_tad_meQTL_pos_selected//cache/*1204*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-embassy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos_2/ --rds_path `ls output/f_susie_tad_meQTL_pos_2//cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-rwanda",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos/ --rds_path `ls output/f_susie_tad_meQTL_pos//cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-retailer",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_haQTL_pos_check_pure_2 --rds_path `ls output/f_susie_tad_haQTL_pos_check_pure_2/cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-shepherd",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb fsusie_to_tsv \\\n",
    "    --cwd output/f_susie_tad_meQTL_pos_2/ --rds_path `ls output/f_susie_tad_meQTL_pos_2//cache/*rds ` \\\n",
    "    --region-list ../eqtl/dlpfc_tad_list \\\n",
    "    --container containers/stephenslab.sif -s build"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe78e94-87f7-4de7-b46d-fae572aa1194",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Exporting cis_analysis susie_twas results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6c5448-ef4d-4e09-8a55-f87aabf8da1c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "meta file is produced by:\n",
    "```\n",
    "get_condition <- function(conditions, Author, qtl_type){\n",
    "    strings <- c()\n",
    "    for(condition in conditions){\n",
    "        string =  paste(condition, Author, qtl_type, sep = \"_\")  \n",
    "        string = paste(unique(unlist(strsplit(string, \"_\"))), collapse = \"_\")\n",
    "        strings <- c(strings, string)\n",
    "    }\n",
    "    return(strings)\n",
    "}\n",
    "\n",
    "raw_name<- c(\"Mic\",\"Ast\",\"Oli\",\"OPC\",\"Exc\",\"Inh\",\"DLPFC\",\"PCC\",\"AC\")\n",
    "raw_name_kellis<- c(\"Mic_Kellis\",\"Ast_Kellis\",\"Oli_Kellis\",\"OPC_Kellis\",\"Exc_Kellis\",\"Inh_Kellis\",\"Ast.10\",\"Mic.12\",\"Mic.13\")\n",
    "\n",
    "dejager_name <- get_condition(raw_name, \"De_Jager\",\"eQTL\")\n",
    "kellis_name <- get_condition(raw_name_kellis, \"Kellis\",\"eQTL\")\n",
    "eQTL_meta <- data.frame(raw_name = c(raw_name, raw_name_kellis), new_name = c(dejager_name, kellis_name))\n",
    "write_delim(eQTL_meta, \"/mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/eQTL_meta.tsv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd09e57-ee13-4822-a4a7-8a3fc640f999",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_name new_name\n",
      "Mic Mic_De_Jager_eQTL\n",
      "Ast Ast_De_Jager_eQTL\n",
      "Oli Oli_De_Jager_eQTL\n",
      "OPC OPC_De_Jager_eQTL\n",
      "Exc Exc_De_Jager_eQTL\n",
      "Inh Inh_De_Jager_eQTL\n",
      "DLPFC DLPFC_De_Jager_eQTL\n",
      "PCC PCC_De_Jager_eQTL\n",
      "AC AC_De_Jager_eQTL\n"
     ]
    }
   ],
   "source": [
    "head /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/eQTL_meta.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137961b5-1c6c-4c4b-b29a-a27643d5b1d1",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run ~/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  cis_results_export \\\n",
    "    --condition_meta /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024_new/eQTL_meta.tsv \\\n",
    "    --region_list ~/codes/fungen-xqtl-analysis/resource/TADB_enhanced_cis.protein_coding.bed \\\n",
    "    --file_path ./rds_files \\\n",
    "    --prefix ROSMAP_eQTL ROSMAP_eQTL_Kellis \\\n",
    "    --suffix susie_weights_db.rds \\\n",
    "    --name ROSMAP_eQTL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f3b184-552a-4ca0-b11d-0e56516ed6ff",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Export gwas data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6b547-b86a-4014-9377-07bbad828d62",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run ~/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  gwas_results_export \\\n",
    "    --rds-files ` ls /mnt/vast/hpc/csg/hs3393/RSS_QC/GWAS_finemapping_result/SuSiE_RSS/*rds` \\\n",
    "    --name ADGWAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bcc90-94ef-4d9b-b3eb-57abaacfd5c9",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Overlapped gwas data and eQTL data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0efd99d-cd04-4f8d-9d2b-418b67df5d3e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run ~/codes/xqtl-pipeline/pipeline/fine_mapping_post_processing.ipynb  overlap_qtl_gwas \\\n",
    "    --qtl-files `ls /mnt/vast/hpc/csg/rf2872/Work/Multivariate/susie_2024/data/*/*export.rds  ` \\\n",
    "    --gwas-union-file  /mnt/vast/hpc/csg/rf2872/Work/Multivariate/gwas/output/ADGWAS.union_export.tsv.gz \\\n",
    "    --name ROSMAP_eQTL \\\n",
    "    -c ~/env_files/csg.yml -J 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-measure",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Plotting the pip plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-secretary",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_pip_landscape_plot \\\n",
    "    --cwd output/test/ --plot_list plot_recipe --annot_tibble ~/Annotatr_builtin_annotation_tibble.tsv -s force &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-blackjack",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/SuSiE_post_processing.ipynb susie_upsetR_plot \\\n",
    "    --cwd output/test/ --plot_list plot_recipe_1  -s force &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-drilling",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The required input for this step is a tab-delimited plot_recipe file that specifies the path to each of the variant.tsv files generated from this module. Each column represents a molecular phenotype, and each row indicates the files that share common variants. Since one TAD may correspond to multiple genes, additional eQTL are permitted. If there are additional molecular phenotypes or ADGWAS datasets, additional columns can be appended.\n",
    "\n",
    "The built-in Annotatr_builtin_annotation_tibble.tsv can be downloaded from [synapse](https://www.synapse.org/#!Synapse:syn51198526), please download it and specify the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nuclear-chancellor",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haQTL\tmQTL\teQTL\teQTL\tADGWAS\n",
      "/mnt/vast/hpc/csg/molecular_phenotype_calling/QTL_fine_mapping/output/f_susie_tad_meQTL_pos//meQTL.yuqi_mqtl.tad100.uni_Fsusie.mixture_normal_per_scale.variant.tsv\t/mnt/vast/hpc/csg/molecular_phenotype_calling/QTL_fine_mapping/output/f_susie_tad_haQTL_pos//haQTL.rosmap_haqtl.tad100.uni_Fsusie.mixture_normal_per_scale.variant.tsv\t/mnt/vast/hpc/csg/molecular_phenotype_calling/eqtl//output/susie_per_gene_tad//demo.ENSG00000117322.unisusie.fit.variant.tsv\t/mnt/vast/hpc/csg/molecular_phenotype_calling/eqtl//output/susie_per_gene_tad//demo.ENSG00000203710.unisusie.fit.variant.tsv\t/mnt/vast/hpc/csg/xqtl_workflow_testing/susie_rss/output/ADGWAS_finemapping_extracted/Bellenguez/ADGWAS2022.chr1.sumstat.chr1_205972031_208461272.unisusie_rss.fit.variant.tsv\n"
     ]
    }
   ],
   "source": [
    "cat /mnt/vast/hpc/csg/molecular_phenotype_calling/QTL_fine_mapping/plot_recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-fields",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "import glob\n",
    "import pandas as pd\n",
    "# A region list file documenting the chr_pos_ref_alt of a susie_object\n",
    "parameter: cwd = path(\"output\")\n",
    "parameter: name = \"demo\"\n",
    "\n",
    "## Path to work directory where output locates\n",
    "## Containers that contains the necessary packages\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 50\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"96h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"6G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 2\n",
    "parameter: windows = 1000000\n",
    "# use this function to edit memory string for PLINK input\n",
    "from sos.utils import expand_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "genetic-campbell",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_to_tsv_1]\n",
    "# Input\n",
    "# For complete susie, region_list or tad_list, for susie_rss , LD region list \n",
    "parameter: region_list = path\n",
    "region_tbl = pd.read_csv(region_list,sep = \"\\t\")\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "output: f\"{cwd}/{_input:bn}.variant.tsv\",f\"{cwd}/{_input:bn}.lbf.tsv\",f\"{cwd}/{_input:bn}.effect.tsv\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"susieR\")\n",
    "    extract_lbf = function(susie_obj){\n",
    "    \n",
    "    if(\"variants\" %in% names(susie_obj) ){\n",
    "    ss_bf = tibble(snps = susie_obj$variants, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    }\n",
    "      else \n",
    "      {\n",
    "    ss_bf = tibble(snps = susie_obj$variable_name, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${_input:bnnn}\")\n",
    "     }\n",
    "    \n",
    "    ss_bf = ss_bf%>%cbind(susie_obj$lbf_variable%>%t)%>%as_tibble()\n",
    "    \n",
    "    return(ss_bf)\n",
    "    }\n",
    "    \n",
    "    extract_variants_pip = function(susie_obj,region_list){\n",
    "    susie_tb = tibble( variants =  names(susie_obj$pip)[which( susie_obj$pip >= 0)],\n",
    "                           snps_index = which(( susie_obj$pip >= 0))) %>%\n",
    "        mutate(chromosome = map_chr(variants, ~read.table(text = .x, sep = \":\")$V1%>%str_replace(\"chr\",\"\") ),\n",
    "                position  = map_chr(variants, ~read.table(text = .x, sep = \":\")$V2  ),\n",
    "                ref = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V2  ),\n",
    "                alt = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V3  ),\n",
    "                position  = map_dbl(position , ~read.table(text = .x, sep = \"_\",as.is = T)$V1  )\n",
    "                             )\n",
    "      susie_tb = susie_tb%>%mutate(cs_order =(map(susie_tb$snps_index , ~tryCatch(which(pmap(list( a= susie_obj$sets$cs) , function(a) .x %in% a )%>%unlist()), error = function(e) return(0) )  ))%>%as.character%>%str_replace(\"integer\\\\(0\\\\)\",\"0\"),\n",
    "                         cs_id = map_chr(cs_order,~ifelse(.x ==\"0\", \"None\" ,names(susie_obj$sets$cs)[.x%>%str_split(\":\")%>%unlist%>%as.numeric] ) ),\n",
    "                         log10_base_factor = map_chr(snps_index,~paste0( susie_obj$lbf_variable[,.x],  collapse = \";\")),\n",
    "                         pip = susie_obj$pip,\n",
    "                         posterior_mean = coef.susie(susie_obj)[-1],\n",
    "                         posterior_sd = susie_get_posterior_sd(susie_obj),\n",
    "                         z = posterior_mean/posterior_sd)\n",
    "    \n",
    "          susie_tb =  susie_tb%>%mutate(  molecular_trait_id = region_list$molecular_trait_id,\n",
    "                             finemapped_region_start = region_list$finemapped_region_start,\n",
    "                             finemapped_region_end = region_list$finemapped_region_end)\n",
    "          return(susie_tb)    }\n",
    "          \n",
    "        \n",
    "\n",
    "     extract_effect_pip = function(susie_obj,region_list,susie_tb){\n",
    "      result_tb =  tibble(phenotype = susie_obj$name,\n",
    "        V = susie_obj$V,effect_id = paste0(\"L\",1:length(V) ) ,\n",
    "        cs_log10bf = susie_obj$lbf)\n",
    "        if(is.null(susie_obj$sets$cs)){\n",
    "            cs_min_r2 = cs_avg_r2 =  coverage =  0 \n",
    "            cs = \"None\"} else {         cs = map_chr(susie_obj$sets$cs[result_tb$effect_id],~susie_tb$variants[.x]%>%paste0(collapse = \";\"))\n",
    "        coverage = map(result_tb$effect_id, ~susie_obj$sets$coverage[which(names(susie_obj$sets$cs) == .x )])%>%as.numeric%>%replace_na(0)\n",
    "        cs_min_r2  = (susie_obj$sets$purity[result_tb$effect_id,1])%>%as.numeric%>%replace_na(0)  \n",
    "        cs_avg_r2  = (susie_obj$sets$purity[result_tb$effect_id,2])%>%as.numeric%>%replace_na(0) }\n",
    "        result_tb = result_tb%>%mutate(cs_min_r2 = cs_min_r2,cs_avg_r2 = cs_avg_r2 ,coverage = coverage%>%unlist,cs = cs )            \n",
    "      return(result_tb)\n",
    "      }\n",
    "       \n",
    "  \n",
    "    susie_obj = readRDS(\"${_input:a}\")\n",
    "    if(\"variants\" %in% names(susie_obj) ){susie_obj$variants = susie_obj$variants%>%str_replace(\"_\",\":\")}\n",
    "    if(is.null(names(susie_obj$pip ))){names(susie_obj$pip) = susie_obj$variants}\n",
    "    lbf = extract_lbf(susie_obj)\n",
    "    region_list = read_delim(\"${region_list}\",\"\\t\")\n",
    "    if(ncol(region_list) == 3 ){   region_list =  region_list%>%mutate(`#chr` = `#chr`%>%str_remove_all(\" \") , ID = paste0(`#chr`,\"_\",start,\"_\",end) ) } # LD_list \n",
    "    if(region_list$start[1] - region_list$end[1]  == -1 ){ \n",
    "        region_list = region_list%>%mutate( start = start - ${windows} ,end = start +${windows}) # region_list for fix cis windows  \n",
    "          } \n",
    "      if(\"gene_id\" %in% colnames(region_list)){region_list = region_list%>%mutate(ID = gene_id)  } # region_list for gene\n",
    "    region_list = region_list%>%select(molecular_trait_id = ID, chromosome  = `#chr`,finemapped_region_start = start ,finemapped_region_end = end)  # Formatting\n",
    "    region_list = region_list%>%filter(molecular_trait_id == \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    variants_pip = extract_variants_pip( susie_obj , region_list)\n",
    "    effect_pip = extract_effect_pip( susie_obj , region_list,variants_pip)\n",
    "    lbf%>%write_delim(\"${_output[1]}\",\"\\t\")\n",
    "    variants_pip%>%write_delim(\"${_output[0]}\",\"\\t\")\n",
    "    effect_pip%>%write_delim(\"${_output[2]}\",\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-florist",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[sQTL_susie_to_tsv_1]\n",
    "# Input\n",
    "# For complete susie, region_list or tad_list, for susie_rss , LD region list \n",
    "parameter: region_list = path\n",
    "region_tbl = pd.read_csv(region_list,sep = \"\\t\")\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "input_name=f\"{_input:bn}\"\n",
    "input_name=input_name.replace('*', 'N') # \"*\" in leafcutter2 would be ignored in shell and cause error \n",
    "output: f\"{cwd}/{input_name}.variant.tsv\",f\"{cwd}/{input_name}.lbf.tsv\",f\"{cwd}/{input_name}.effect.tsv\"\n",
    "tags = f'{step_name}_{_output[0]:bn}'\n",
    "tags = tags.replace(':', '_').replace('+', 'ps') # also for other symbols in tag id \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = tags\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"susieR\")\n",
    "    extract_lbf = function(susie_obj){\n",
    "    \n",
    "    if(\"variants\" %in% names(susie_obj) ){\n",
    "    ss_bf = tibble(snps = susie_obj$variants, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    }\n",
    "      else \n",
    "      {\n",
    "    ss_bf = tibble(snps = susie_obj$variable_name, cs_index = ifelse(is.null(susie_obj$sets$cs_index), 0, paste0(susie_obj$sets$cs_index,collapse =\",\")),names = \"${_input:bnnn}\")\n",
    "     }\n",
    "    \n",
    "    ss_bf = ss_bf%>%cbind(susie_obj$lbf_variable%>%t)%>%as_tibble()\n",
    "    \n",
    "    return(ss_bf)\n",
    "    }\n",
    "    \n",
    "    extract_variants_pip = function(susie_obj,region_list){\n",
    "    susie_tb = tibble( variants =  names(susie_obj$pip)[which( susie_obj$pip >= 0)],\n",
    "                           snps_index = which(( susie_obj$pip >= 0))) %>%\n",
    "        mutate(chromosome = map_chr(variants, ~read.table(text = .x, sep = \":\")$V1%>%str_replace(\"chr\",\"\") ),\n",
    "                position  = map_chr(variants, ~read.table(text = .x, sep = \":\")$V2  ),\n",
    "                ref = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V2  ),\n",
    "                alt = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V3  ),\n",
    "                position  = map_dbl(position , ~read.table(text = .x, sep = \"_\",as.is = T)$V1  )\n",
    "                             )\n",
    "      susie_tb = susie_tb%>%mutate(cs_order =(map(susie_tb$snps_index , ~tryCatch(which(pmap(list( a= susie_obj$sets$cs) , function(a) .x %in% a )%>%unlist()), error = function(e) return(0) )  ))%>%as.character%>%str_replace(\"integer\\\\(0\\\\)\",\"0\"),\n",
    "                         cs_id = map_chr(cs_order,~ifelse(.x ==\"0\", \"None\" ,names(susie_obj$sets$cs)[.x%>%str_split(\":\")%>%unlist%>%as.numeric] ) ),\n",
    "                         log10_base_factor = map_chr(snps_index,~paste0( susie_obj$lbf_variable[,.x],  collapse = \";\")),\n",
    "                         pip = susie_obj$pip,\n",
    "                         posterior_mean = coef.susie(susie_obj)[-1],\n",
    "                         posterior_sd = susie_get_posterior_sd(susie_obj),\n",
    "                         z = posterior_mean/posterior_sd)\n",
    "    \n",
    "          susie_tb =  susie_tb%>%mutate(  molecular_trait_id = region_list$molecular_trait_id,\n",
    "                             finemapped_region_start = region_list$finemapped_region_start,\n",
    "                             finemapped_region_end = region_list$finemapped_region_end)\n",
    "          return(susie_tb)    }\n",
    "          \n",
    "        \n",
    "\n",
    "     extract_effect_pip = function(susie_obj,region_list,susie_tb){\n",
    "      result_tb =  tibble(phenotype = susie_obj$name,\n",
    "        V = susie_obj$V,effect_id = paste0(\"L\",1:length(V) ) ,\n",
    "        cs_log10bf = susie_obj$lbf)\n",
    "        if(is.null(susie_obj$sets$cs)){\n",
    "            cs_min_r2 = cs_avg_r2 =  coverage =  0 \n",
    "            cs = \"None\"} else {         cs = map_chr(susie_obj$sets$cs[result_tb$effect_id],~susie_tb$variants[.x]%>%paste0(collapse = \";\"))\n",
    "        coverage = map(result_tb$effect_id, ~susie_obj$sets$coverage[which(names(susie_obj$sets$cs) == .x )])%>%as.numeric%>%replace_na(0)\n",
    "        cs_min_r2  = (susie_obj$sets$purity[result_tb$effect_id,1])%>%as.numeric%>%replace_na(0)  \n",
    "        cs_avg_r2  = (susie_obj$sets$purity[result_tb$effect_id,2])%>%as.numeric%>%replace_na(0) }\n",
    "        result_tb = result_tb%>%mutate(cs_min_r2 = cs_min_r2,cs_avg_r2 = cs_avg_r2 ,coverage = coverage%>%unlist,cs = cs )            \n",
    "      return(result_tb)\n",
    "      }\n",
    "       \n",
    "  \n",
    "    susie_obj = readRDS(\"${_input:a}\")\n",
    "    if(\"variants\" %in% names(susie_obj) ){susie_obj$variants = susie_obj$variants%>%str_replace(\"_\",\":\")}\n",
    "    if(is.null(names(susie_obj$pip ))){names(susie_obj$pip) = susie_obj$variants}\n",
    "    lbf = extract_lbf(susie_obj)\n",
    "    region_list = read_delim(\"${region_list}\",\"\\t\")\n",
    "    if(ncol(region_list) == 3 ){   region_list =  region_list%>%mutate(`#chr` = `#chr`%>%str_remove_all(\" \") , ID = paste0(`#chr`,\"_\",start,\"_\",end) ) } # LD_list \n",
    "    if(region_list$start[1] - region_list$end[1]  == -1 ){ \n",
    "        region_list = region_list%>%mutate( start = start - ${windows} ,end = start +${windows}) # region_list for fix cis windows  \n",
    "          } \n",
    "      if(\"gene_id\" %in% colnames(region_list)){region_list = region_list%>%mutate(ID = gene_id)  } # region_list for gene\n",
    "    region_list = region_list%>%select(molecular_trait_id = ID, chromosome  = `#chr`,finemapped_region_start = start ,finemapped_region_end = end)  # Formatting\n",
    "    mole_id = \"${f'{_input:br}'.split('.')[-4]}\"%>%gsub(\"_N:\",\"_*:\",.)#for sQTL\n",
    "    region_list = region_list%>%filter(molecular_trait_id == mole_id)\n",
    "    variants_pip = extract_variants_pip( susie_obj , region_list)\n",
    "    effect_pip = extract_effect_pip( susie_obj , region_list,variants_pip)\n",
    "    lbf%>%write_delim(\"${_output[1]}\",\"\\t\")\n",
    "    variants_pip%>%write_delim(\"${_output[0]}\",\"\\t\")\n",
    "    effect_pip%>%write_delim(\"${_output[2]}\",\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-wichita",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fsusie_to_tsv_1]\n",
    "# Input\n",
    "# For complete susie, region_list or tad_list, for susie_rss , LD region list \n",
    "parameter: region_list = path\n",
    "region_tbl = pd.read_csv(region_list,sep = \"\\t\")\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "output: f\"{cwd}/{_input:bn}.variant.tsv\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]:nn}.stdout\", stderr = f\"{_output[0]:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"susieR\")\n",
    "\n",
    "    extract_variants_pip = function(susie_obj,region_list){\n",
    "        susie_tb = tibble( variants =  names(susie_obj$csd_X),\n",
    "                               snps_index = which(( susie_obj$pip >= 0))) %>%\n",
    "            mutate(chromosome = map_chr(variants, ~read.table(text = .x, sep = \":\")$V1%>%str_replace(\"chr\",\"\") ),\n",
    "                    position  = map_chr(variants, ~read.table(text = .x, sep = \":\")$V2  ),\n",
    "                    ref = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V2  ),\n",
    "                    alt = map_chr(position , ~read.table(text = .x, sep = \"_\",colClasses = \"character\")$V3  ),\n",
    "                    position  = map_dbl(position , ~read.table(text = .x, sep = \"_\",as.is = T)$V1  )\n",
    "                                 )\n",
    "          susie_tb = susie_tb%>%mutate(cs_order =(map(susie_tb$snps_index , ~tryCatch(which(pmap(list( a= susie_obj$cs) , function(a) .x %in% a )%>%unlist()), error = function(e) return(0) )  ))%>%as.character%>%str_replace(\"integer\\\\(0\\\\)\",\"0\"),\n",
    "                             pip = susie_obj$pip)\n",
    "          susie_tb =  susie_tb%>%mutate(  molecular_trait_id = region_list$tad_index,\n",
    "                                 finemapped_region_start = region_list$start,\n",
    "                                 finemapped_region_end = region_list$end)\n",
    "          if(\"purity\" %in% names(susie_obj)){\n",
    "              susie_tb = susie_tb%>%mutate(purity = map_dbl(susie_tb$cs_order, ~ifelse(.x%>%as.numeric > 0, susie_obj$purity[[as.numeric(.x)]], NA ) ), is_dummy = as.numeric(purity < 0.5)  )\n",
    "              }\n",
    "    susie_tb = susie_tb%>%mutate(effect_peak_pos = map_dbl(cs_order, ~ifelse(.x%>%as.numeric > 0, susie_obj$outing_grid[which(abs(susie_obj$fitted_func[[as.numeric(.x)]]) == max(abs(susie_obj$fitted_func[[as.numeric(.x)]])))] , NA ) )) \n",
    "    susie_tb_lbf = cbind(susie_tb%>%select(molecular_trait_id,variants,cs_order),Reduce(cbind, susie_obj$lBF)%>%as.tibble%>%`colnames<-`(1:length(susie_obj$lBF)))\n",
    "          return(list(susie_tb, susie_tb_lbf))    }\n",
    "    susie_obj = readRDS(\"${_input:a}\")\n",
    "    region_list = read_delim(\"${region_list}\",\"\\t\")\n",
    "    region_list = region_list%>%filter(tad_index == \"${f'{_input:br}'.split('.')[-4]}\")\n",
    "    variants_pip = extract_variants_pip( susie_obj , region_list)[[1]]\n",
    "    variants_lbf = extract_variants_pip( susie_obj , region_list)[[2]]\n",
    "    print(paste0(\"fsusie run time is \", round(susie_obj$runtime[[3]]/60),\"min\"))\n",
    "    variants_pip%>%write_delim(\"${_output}\",\"\\t\")\n",
    "    variants_pip%>%write_delim(\"${_output:nn}.lbf.tsv\",\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-extent",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[*_to_tsv_2]\n",
    "parameter: name = f'{_input[0]:b}'.split(\".\")[0]\n",
    "input: group_by = \"all\"\n",
    "output: f\"{cwd}/{name}.all_variants.tsv\"\n",
    "bash: expand = '${ }', stdout = f\"{_output:n}.stdout\", stderr = f\"{_output:n}.stderr\", container = container, entrypoint = entrypoint\n",
    "    head -1 ${_input[0]} > ${_output}\n",
    "    cat ${_input[0]:d}/*variant.tsv | grep -v cs_order >> ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-festival",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_tsv_collapse]\n",
    "parameter: tsv_path = paths # TSV needs have the name ends with  *.chr1_2_3.unisusie(_rss).lbf.tsv\n",
    "tsv_list  = pd.DataFrame({\"lbf_path\" : [str(x) for x in tsv_path]})\n",
    "chromosome = list(set([f'{x.split(\".\")[-5].split(\"_\")[0].replace(\"chr\",\"\")}'  for x in tsv_list.lbf_path ])) ## Add chr if there is no chr prefix. This is to accomodata chr XY and M\n",
    "input: tsv_path, for_each = \"chromosome\"\n",
    "output: f'{cwd}/{_input[0]:bnnnnnnn}.chr{_chromosome}.unisusie_rss.lbf.tsv'\n",
    "bash: expand = '${ }', stdout = f\"{_output}.stdout\", stderr = f\"{_output}.stderr\", container = container, entrypoint = entrypoint\n",
    "        head -1 ${_input[0]} > ${_output}\n",
    "        cat ${_input[0]:d}/*.chr${_chromosome}_*lbf.tsv | grep -v cs_index >> ${_output}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reliable-praise",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_pip_landscape_plot]\n",
    "parameter: plot_list = path\n",
    "parameter: annot_tibble = path(\"~/Annotatr_builtin_annotation_tibble.tsv\")\n",
    "import pandas as pd\n",
    "plot_list  = pd.read_csv(plot_list,sep = \"\\t\")\n",
    "file_type = plot_list.columns.values.tolist()\n",
    "file_type = [x.split(\".\")[0] for x in file_type ]\n",
    "plot_list = plot_list.to_dict(\"records\")\n",
    "input: plot_list, group_by = len(file_type)\n",
    "output: f'{cwd}/{\"_\".join(file_type)}.{str(_input[0]).split(\".\")[-5]}.pip_landscape_plot.rds',f'{cwd}/{\"_\".join(file_type)}.{str(_input[0]).split(\".\")[-5]}.pip_landscape_plot.pdf'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    color = c(\"black\", \"dodgerblue2\", \"green4\", \"#6A3D9A\", \n",
    "          \"#FF7F00\", \"gold1\", \"skyblue2\", \"#FB9A99\", \"palegreen2\",\n",
    "          \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\",\n",
    "          \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\", \"green1\", \n",
    "          \"yellow4\", \"yellow3\",\"darkorange4\",\"brown\",\"navyblue\",\"#FF0000\",\n",
    "          \"darkgreen\",\"#FFFF00\",\"purple\",\"#00FF00\",\"pink\",\"#0000FF\",\n",
    "          \"orange\",\"#FF00FF\",\"cyan\",\"#00FFFF\",\"#FFFFFF\")\n",
    "    extract_table = function(variant_df,type){ \n",
    "    if(\"purity\" %in% colnames(variant_df) ){\n",
    "      variant_df$purity[is.na(variant_df$purity)] = 0\n",
    "      variant_df[abs(variant_df$purity) < 0.5,7] = 0\n",
    "      }\n",
    "     variant_df = variant_df%>%mutate(CS = (cs_order%>%as.factor%>%as.numeric-1)%>%as.factor)%>%\n",
    "          select( y = pip ,snp = variants,pos = position , CS, molecular_trait_id)%>%mutate(molecular_trait_id = paste0(type,\"_\",molecular_trait_id ) )\n",
    "    return(variant_df)\n",
    "    }\n",
    "    plot_recipe = tibble( type =  c('${\"','\".join(file_type) }'), path = c(${_input:r,}))\n",
    "    plot_list = map2(plot_recipe$type,plot_recipe$path, ~read_delim(.y, guess_max = 10000000)%>%extract_table(.x) )\n",
    "    plot_df = Reduce(rbind,plot_list)\n",
    "    plot_range = (plot_df%>%group_by(molecular_trait_id)%>%summarize(start = (min(pos)), end = (max(pos)))%>%mutate(start = median(start),end = median(end)))[1,c(2,3)]%>%as.matrix\n",
    "    plot_chr = (plot_df$snp[1]%>%stringr::str_split(\":\"))[[1]][1]\n",
    "    plot_df = plot_df%>%mutate(Shared = as.logical(map(snp, ~(plot_df%>%filter( snp ==.x ,   CS%>%as.numeric !=  1 )%>%nrow()) > 1  )))\n",
    "    pip_plot <- plot_df%>%ggplot2::ggplot(aes(y = y, x = pos,\n",
    "                                  col =  CS, shape = Shared )) + facet_grid(molecular_trait_id ~.)+\n",
    "      geom_point(size = 7) +\n",
    "      scale_color_manual(\"CS\",values = color) +\n",
    "      theme(axis.ticks.x = element_blank()) +\n",
    "     ylab(\"Posterior Inclusion Probability (PIP)\")+xlim(plot_range)+\n",
    "      theme(axis.ticks.x = element_blank()) +\n",
    "            theme(strip.text.y.right = element_text(angle = 0))+\n",
    "            xlab(\"\") + \n",
    "            theme(text = element_text(size = 30))+ggtitle(\"Overview of fine-mapping\")\n",
    "  \n",
    "    annot = read_delim(\"${annot_tibble}\")\n",
    "    annot = annot%>%filter(seqnames == plot_chr, start > plot_range[1], end < plot_range[2])\n",
    "    annot_plot   = annot%>%filter(!type%in%c(\"hg38_genes_introns\",\"hg38_genes_1to5kb\"))%>%\n",
    "                        ggplot(aes())+\n",
    "                        geom_segment( aes(x = start,xend = end, y = \"Regulartory Element\", yend = \"Regulartory Element\", color = type ), linewidth =10)+\n",
    "                        ylab(\"\")+xlab(\"\")+xlim(plot_range)+theme(axis.text.x=element_blank(),text = element_text(size = 20))+scale_color_brewer(palette=\"Dark2\")\n",
    "    gene_plot = annot%>%filter(type%in%c(\"hg38_genes_1to5kb\"))%>%group_by(symbol)%>%\n",
    "                            summarise(start = min(start), end = max(end))%>%na.omit%>%\n",
    "                            ggplot(aes())+geom_segment( aes(x = start,xend = end, y = \"Gene\", yend = \"Gene\", color = symbol ), linewidth =10)+\n",
    "                            geom_label(aes(x = (start+end)/2,y = \"Gene\", label = symbol ),size = 5)+ylab(\"\")+xlab(\"POS\")+\n",
    "                            theme(legend.position=\"none\")+theme(text = element_text(size = 20))+xlim(plot_range)\n",
    "      \n",
    "    list(pip_plot,plot_df,annot_plot,gene_plot)%>%saveRDS(\"${_output[0]}\")\n",
    "    cowplot::plot_grid(plotlist = list(pip_plot,annot_plot,gene_plot),ncol = 1, align = \"v\",axis = \"tlbr\",rel_heights = c(8,1,1))%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width = 30, height = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "juvenile-leone",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_upsetR_plot]\n",
    "parameter: plot_list = path\n",
    "import pandas as pd\n",
    "plot_list  = pd.read_csv(plot_list, sep = \"\\t\")\n",
    "file_type = plot_list.columns.values.tolist()\n",
    "file_type = [x.split(\".\")[0] for x in file_type ]\n",
    "plot_list = plot_list.to_dict(\"records\")\n",
    "input: plot_list\n",
    "output: f'{cwd}/{\"_\".join(file_type)}.UpSetR.rds',f'{cwd}/{\"_\".join(file_type)}.UpSetR.pdf'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"UpSetR\")\n",
    "    library(\"ComplexUpset\")\n",
    "    plot_recipe = tibble( type =  c('${\"','\".join(file_type) }'), path = c(${_input:r,}))\n",
    "    plot_list = map2(plot_recipe$type,plot_recipe$path, ~read_delim(.y, guess_max = 10000000)%>%mutate(cs = cs_order != 0 )%>%filter(cs > 0)%>%select(variants,cs)%>%`colnames<-`(c(\"variants\",.x))%>%distinct() )\n",
    "    cs_sharing = Reduce(full_join,plot_list)\n",
    "    cs_upsetR_sharing = cs_sharing\n",
    "    cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]%>%mutate_all(as.numeric)-> cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]\n",
    "    a = upset(cs_upsetR_sharing%>%as.data.frame,intersect = colnames(cs_upsetR_sharing[2:ncol(cs_upsetR_sharing)]),\n",
    "      keep_empty_groups = F,\n",
    "          base_annotations=list(`Intersection size` = intersection_size( bar_number_threshold = 1, position = position_dodge(0.5), width = 0.3 ,text = list(size = 5)   )  ) ,\n",
    "              themes=upset_default_themes(axis.text=element_text(size=30))     ,\n",
    "              min_degree = 1)\n",
    "    \n",
    "    a%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width=18.5, height=10.5)\n",
    "    list(cs_upsetR_sharing)%>%saveRDS(\"${_output[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intense-prompt",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[susie_upsetR_cs_plot]\n",
    "parameter: plot_list = path\n",
    "import pandas as pd\n",
    "plot_list  = pd.read_csv(plot_list, sep = \"\\t\")\n",
    "file_type = plot_list.columns.values.tolist()\n",
    "file_type = [x.split(\".\")[0] for x in file_type ]\n",
    "plot_list = plot_list.to_dict(\"records\")\n",
    "parameter: trait_to_select =  1 \n",
    "input: plot_list\n",
    "output: f'{cwd}/{\"_\".join(file_type)}.UpSetR_{file_type[trait_to_select-1]}_cs.rds',f'{cwd}/{\"_\".join(file_type)}.UpSetR_{file_type[trait_to_select-1]}_cs.pdf'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"UpSetR\")\n",
    "    library(\"ComplexUpset\")\n",
    "\n",
    "    cs_sharing_identifer = function(upsetR_input,df){\n",
    "    inner_join(upsetR_input, df%>%select(variants,molecular_trait_id, cs_order)%>%filter(cs_order != 0))%>%select(-variants)-> dfL_CS_sharing\n",
    "    dfL_CS_sharing[is.na(dfL_CS_sharing)] = FALSE\n",
    "    dfL_CS_sharing = dfL_CS_sharing%>%group_by(molecular_trait_id,cs_order)%>%summarize(across(everything(), list(mean))   )\n",
    "    dfL_CS_sharing = dfL_CS_sharing%>%mutate(across(colnames(dfL_CS_sharing)[3:ncol(dfL_CS_sharing)], ~.x != 0  ))%>%`colnames<-`(c(\"molecular_trait_id\",\"cs_order\",colnames(cs_sharing)[2:ncol(cs_sharing)]))\n",
    "    }\n",
    "  \n",
    "  \n",
    "    plot_recipe = tibble( type =  c('${\"','\".join(file_type) }'), path = c(${_input:r,}))\n",
    "    plot_list = map2(plot_recipe$type,plot_recipe$path, ~read_delim(.y, guess_max = 10000000)%>%mutate(cs = cs_order != 0 )%>%filter(cs > 0)%>%select(variants,cs)%>%`colnames<-`(c(\"variants\",.x))%>%distinct() )\n",
    "    cs_sharing = Reduce(full_join,plot_list)\n",
    "    cs_upsetR_sharing = cs_sharing\n",
    "    cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]%>%mutate_all(as.numeric)-> cs_upsetR_sharing[,2:ncol(cs_upsetR_sharing)]\n",
    "\n",
    "    df = read_delim(plot_recipe$path[[${trait_to_select}]]) \n",
    "    \n",
    "    cs_sharing_df = cs_sharing_identifer(cs_upsetR_sharing,df)\n",
    "\n",
    "    a = upset(cs_sharing_df%>%as.data.frame,intersect = colnames(cs_sharing_df[3:ncol(cs_sharing_df)]),\n",
    "          keep_empty_groups = F,\n",
    "          base_annotations=list(`Intersection size` = intersection_size( bar_number_threshold = 1, position = position_dodge(0.5), width = 0.3 ,text = list(size = 8)   )  ),\n",
    "          themes=upset_default_themes(axis.text=element_text(size=30)),\n",
    "          set_size = F  ,  min_degree = 1,wrap = T) + ggtitle( paste0(plot_recipe$type[[${trait_to_select}]],'CS shared with other phenotypes') )   + theme(plot.title = element_text(size = 40, face = \"bold\"))\n",
    "  \n",
    "    a%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width=18.5, height=10.5)\n",
    "    list(cs_sharing_df)%>%saveRDS(\"${_output[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-apartment",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[tmp_annotatation_of_snps_1]\n",
    "parameter: SNP_list = path\n",
    "parameter: annotation_rds = path\n",
    "input: SNP_list\n",
    "output: f'{cwd}/{_input:b}.annotated.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output}.stdout\", stderr = f\"{_output}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"purrr\")\n",
    "    library(\"stringr\")\n",
    "    sharing_snp = readRDS(\"${_input}\")\n",
    "    sharing_snp_fsusie = sharing_snp[[1]]%>%filter(haQTL == 1 | mQTL == 1)\n",
    "    sharing_snp_fsusie = sharing_snp_fsusie%>%mutate(X1 =  read.table(text = sharing_snp_fsusie$variants, sep = \":\")$V1, X2 = read.table(text = read.table(text = sharing_snp_fsusie$variants, sep = \":\")$V2 , sep = \"_\")$V1  )\n",
    "    sharing_snp_fsusie = sharing_snp_fsusie%>%select(variants,chr = X1, pos = X2)\n",
    "    annotation = readRDS(\"${annotation_rds}\")\n",
    "    print(\"data loaded\")\n",
    "    result = sharing_snp_fsusie%>%mutate(annot = map2( chr,pos , ~ annotation%>%filter(X1 == .x, X2 <= .y, X3 >= .y)%>%pull(X5)))%>%mutate(annot = map_chr(annot, ~paste0(.x ,collapse = \",\")) )\n",
    "    print(\"snp annotated\")\n",
    "    result%>%saveRDS(\"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-transport",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[tmp_annotatation_of_snps_2]\n",
    "parameter: SNP_list = path\n",
    "parameter: annotation_rds = path\n",
    "output: f'{cwd}/{_input:b}.annotated_rev.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output}.stdout\", stderr = f\"{_output}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"purrr\")\n",
    "    library(\"stringr\")\n",
    "    result = readRDS(${_input:r})\n",
    "    result_rev = tibble(annot = unique(annotation$X5))%>%mutate(variants = map(annot, ~  result%>%filter( str_detect(annot,.x))%>%pull(variants)) )%>%mutate( variants = map_chr(variants,~paste0(.x ,collapse = \",\"))  )\n",
    "    result_rev%>%saveRDS(\"${_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-workplace",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fsusie_extract_effect]\n",
    "parameter: rds_path = paths\n",
    "parameter: annot_tibble = path(\"~/Annotatr_builtin_annotation_tibble.tsv\")\n",
    "input: rds_path, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.estimated_effect.tsv',f'{cwd}/{_input:bn}.estimated_effect.pdf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    color = c(\"black\", \"dodgerblue2\", \"green4\", \"#6A3D9A\", \n",
    "          \"#FF7F00\", \"gold1\", \"skyblue2\", \"#FB9A99\", \"palegreen2\",\n",
    "          \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\",\n",
    "          \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\", \"green1\", \n",
    "          \"yellow4\", \"yellow3\",\"darkorange4\",\"brown\",\"navyblue\",\"#FF0000\",\n",
    "          \"darkgreen\",\"#FFFF00\",\"purple\",\"#00FF00\",\"pink\",\"#0000FF\",\n",
    "          \"orange\",\"#FF00FF\",\"cyan\",\"#00FFFF\",\"#FFFFFF\")\n",
    "    \n",
    "    effect_extract = function(fsusie){\n",
    "        plot_df = fsusie$fitted_func%>%as_tibble(.name_repair = \"universal\")%>%mutate(pos =  fsusie$outing_grid)%>%`colnames<-`(c(paste0(\"Effect_\",1:length(fsusie$cs)),\"pos\"))%>%mutate(`#chr` = str_split(names(fsusie$csd_X)[[1]],\":\")[[1]][[1]] )%>%select(`#chr`, pos, everything())\n",
    "        plot=  plot_df%>%pivot_longer(cols = 3:ncol(plot_df),names_to = \"effect\", values_to = \"values\"   ) %>%ggplot(aes(x = pos, y = values,color = effect),linewidth = 7)+\n",
    "        geom_line()+ylab(\"Estimated Effect\") + xlab(\"POS\")+facet_grid(effect~. )+\n",
    "            scale_color_manual(\"Credible set\",values = color[2:length(color)])+geom_line(aes(y = 0), color = \"black\")\n",
    "            theme(strip.text.y.right = element_text(angle = 0))+\n",
    "            xlab(\"\") + \n",
    "            ylab(\"Estimated Effect\")+ \n",
    "            theme(text = element_text(size = 50))+\n",
    "            ggtitle(paste0( \"Estimated effect for ${f'{_input:br}'.split('.')[-4]}\"))\n",
    "        return(list(plot_df,plot))\n",
    "        }\n",
    "    susie_obj =  readRDS(\"${_input}\")\n",
    "    output = effect_extract(susie_obj)\n",
    "    effect_tbl = output[[1]]\n",
    "    annot = read_delim(\"${annot_tibble}\")\n",
    "    annot = annot%>%filter(seqnames == (effect_tbl$`#chr`)[[1]], start > min(effect_tbl$pos), end < max(effect_tbl$pos))\n",
    "    plot_range = c(min(effect_tbl$pos),  max(effect_tbl$pos))\n",
    "    annot_plot   = annot%>%filter(!type%in%c(\"hg38_genes_introns\",\"hg38_genes_1to5kb\"))%>%\n",
    "                        ggplot(aes())+\n",
    "                        geom_segment( aes(x = start,xend = end, y = \"Regulartory Element\", yend = \"Regulartory Element\", color = type ), linewidth =10)+\n",
    "                        ylab(\"\")+xlab(\"\")+xlim(plot_range)+theme(axis.text.x=element_blank(),text = element_text(size = 20))+scale_color_brewer(palette=\"Dark2\")\n",
    "    gene_plot = annot%>%filter(type%in%c(\"hg38_genes_1to5kb\"))%>%group_by(symbol)%>%\n",
    "                            summarise(start = min(start), end = max(end))%>%na.omit%>%\n",
    "                            ggplot(aes())+geom_segment( aes(x = start,xend = end, y = \"Gene\", yend = \"Gene\", color = symbol ), linewidth =10)+\n",
    "                            geom_label(aes(x = (start+end)/2,y = \"Gene\", label = symbol ),size = 5)+ylab(\"\")+xlab(\"POS\")+\n",
    "                            theme(legend.position=\"none\")+theme(text = element_text(size = 20))+xlim(plot_range)\n",
    "    cowplot::plot_grid(plotlist = list(output[[2]]),ncol = 1, align = \"v\",axis = \"tlbr\",rel_heights = c(8))%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width = 30, height = 30)\n",
    "    effect_tbl%>%write_delim(\"${_output[0]}\",\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-disabled",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[fsusie_affected_region]\n",
    "parameter: rds_path = paths\n",
    "input: rds_path, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.affected_region.tsv',f'{cwd}/{_input:bn}.affected_region.pdf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output[0]}.stdout\", stderr = f\"{_output[0]}.stderr\", container = container, entrypoint = entrypoint\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"readr\") \n",
    "    library(\"ggplot2\")\n",
    "    library(\"purrr\")\n",
    "    library(\"tidyr\")\n",
    "    library(susiF.alpha)\n",
    "    library(ashr)\n",
    "    library(wavethresh)\n",
    "    color = c(\"black\", \"dodgerblue2\", \"green4\", \"#6A3D9A\", \n",
    "          \"#FF7F00\", \"gold1\", \"skyblue2\", \"#FB9A99\", \"palegreen2\",\n",
    "          \"#CAB2D6\", \"#FDBF6F\", \"gray70\", \"khaki2\", \"maroon\", \"orchid1\",\n",
    "          \"deeppink1\", \"blue1\", \"steelblue4\", \"darkturquoise\", \"green1\", \n",
    "          \"yellow4\", \"yellow3\",\"darkorange4\",\"brown\",\"navyblue\",\"#FF0000\",\n",
    "          \"darkgreen\",\"#FFFF00\",\"purple\",\"#00FF00\",\"pink\",\"#0000FF\",\n",
    "          \"orange\",\"#FF00FF\",\"cyan\",\"#00FFFF\",\"#FFFFFF\")\n",
    "    ## Define Function\n",
    "    update_cal_credible_band2 <- function(susiF.obj )\n",
    "    {\n",
    "    \n",
    "    \n",
    "    \n",
    "      if(sum( is.na(unlist(susiF.obj$alpha))))\n",
    "      {\n",
    "        stop(\"Error: some alpha value not updated, please update alpha value first\")\n",
    "      }\n",
    "      temp <- wavethresh::wd(rep(0, susiF.obj$n_wac))\n",
    "    \n",
    "    \n",
    "      for ( l in 1:susiF.obj$L)\n",
    "      {\n",
    "        Smat <-  susiF.obj$fitted_wc2[[l]]\n",
    "        W1   <- ((wavethresh::GenW(n=  ncol(Smat )  , filter.number = 10, family = \"DaubLeAsymm\")))\n",
    "        tt   <- diag( W1%*%diag(c(susiF.obj$alpha[[l]]%*%Smat ))%*% t(W1 ))\n",
    "    \n",
    "        up                       <-  susiF.obj$fitted_func[[l]]+ 3*sqrt(tt)\n",
    "        low                      <-  susiF.obj$fitted_func[[l]]- 3*sqrt(tt)\n",
    "        susiF.obj$cred_band[[l]] <- rbind(up, low)\n",
    "      }\n",
    "    \n",
    "    \n",
    "    \n",
    "      return(susiF.obj)\n",
    "    }\n",
    "    \n",
    "    affected_reg <- function( susiF.obj){\n",
    "      outing_grid <- susiF.obj$outing_grid\n",
    "    \n",
    "      reg <-  list()\n",
    "      h <- 1\n",
    "      for (   l in 1:length(susiF.obj$cs)){\n",
    "    \n",
    "        pos_up <-  which(susiF.obj$cred_band[[l]][1,]<0)\n",
    "        pos_low <- which(susiF.obj$cred_band[[l]][2,]>0)\n",
    "    \n",
    "    \n",
    "        reg_up <- split( pos_up,cumsum(c(1,diff( pos_up)!=1)))\n",
    "    \n",
    "        reg_low <- split( pos_low,cumsum(c(1,diff( pos_low)!=1)))\n",
    "        for( k in 1:length(reg_up)){\n",
    "          reg[[h]] <- c(l, outing_grid[reg_up[[k]][1]], outing_grid[reg_up[[k]][length(reg_up[[k]])]])\n",
    "    \n",
    "          h <- h+1\n",
    "        }\n",
    "        for( k in 1:length(reg_low )){\n",
    "          reg[[h]] <- c(l, outing_grid[reg_low [[k]][1]], outing_grid[reg_low [[k]][length(reg_low [[k]])]])\n",
    "    \n",
    "          h <- h+1\n",
    "        }\n",
    "    \n",
    "    \n",
    "      }\n",
    "      reg <-  do.call(rbind, reg)\n",
    "      colnames(reg) <- c(\"CS\", \"Start\",\"End\")\n",
    "      return(reg)\n",
    "    }\n",
    "    \n",
    "    \n",
    "    susiF_obj =  readRDS(\"${_input}\")\n",
    "    susiF_obj = update_cal_credible_band2(susiF_obj)\n",
    "    affected_tbl = affected_reg(susiF_obj)\n",
    "    affected_tbl = affected_tbl%>%as_tibble%>%mutate(analysis = susiF_obj$name,\n",
    "                          chr = (names(susiF_obj$csd_X)[[1]]%>%stringr::str_split(\":\"))[[1]][[1]],\n",
    "                          molecular_trait_id =  \"${f'{_input:br}'.split('.')[-4]}\", \n",
    "                          purity  = purrr::map_dbl(CS,~susiF_obj$purity[[.x]] )  )\n",
    "    \n",
    "    affected_tbl%>%as_tibble%>%write_delim(\"${_output[0]}\",\"\\t\")\n",
    "    plt = plot_susiF(susiF_obj, cred.band = T)\n",
    "    plt%>%ggsave(filename = \"${_output[1]}\",device = \"pdf\",dpi = \"retina\",width = 30, height = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-bangkok",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Into VCF format\n",
    "\n",
    "FIXME: These codes were moved from earlier workflows. To be cleaned up and tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greatest-montana",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#[uni_susie_2]\n",
    "input: group_with = \"genoFile\"\n",
    "output: f\"{_input:n}.vcf.bgz\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:nn}.stdout\", stderr = f\"{_output:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "   ## Define create_vcf function\n",
    "           create_vcf = function (chrom, pos, nea, ea, snp = NULL, ea_af = NULL, effect = NULL, \n",
    "        se = NULL, pval = NULL, name = NULL,cs = NULL, pip = NULL) \n",
    "    {\n",
    "        stopifnot(length(chrom) == length(pos))\n",
    "        if (is.null(snp)) {\n",
    "            snp <- paste0(chrom, \":\", pos)\n",
    "        }\n",
    "        snp <- paste0(chrom, \":\", pos)\n",
    "        nsnp <- length(chrom)\n",
    "        gen <- list()\n",
    "        ## Setupt data content for each sample column\n",
    "        if (!is.null(ea_af)) \n",
    "            gen[[\"AF\"]] <- matrix(ea_af, nsnp)\n",
    "        if (!is.null(effect)) \n",
    "            gen[[\"ES\"]] <- matrix(effect, nsnp)\n",
    "        if (!is.null(se)) \n",
    "            gen[[\"SE\"]] <- matrix(se, nsnp)\n",
    "        if (!is.null(pval)) \n",
    "            gen[[\"LP\"]] <- matrix(-log10(pval), nsnp)\n",
    "        if (!is.null(cs)) \n",
    "            gen[[\"CS\"]] <- matrix(cs, nsnp)\n",
    "        if (!is.null(pip)) \n",
    "            gen[[\"PIP\"]] <- matrix(pip, nsnp)\n",
    "        gen <- S4Vectors::SimpleList(gen)\n",
    "        \n",
    "      ## Setup snps info for the fix columns\n",
    "        gr <- GenomicRanges::GRanges(chrom, IRanges::IRanges(start = pos, \n",
    "            end = pos + pmax(nchar(nea), nchar(ea)) - 1, names = snp))\n",
    "         coldata <- S4Vectors::DataFrame(Studies = name, row.names = name)\n",
    "    ## Setup header informations\n",
    "        hdr <- VariantAnnotation::VCFHeader(header = IRanges::DataFrameList(fileformat = S4Vectors::DataFrame(Value = \"VCFv4.2\", \n",
    "            row.names = \"fileformat\")), sample = name)\n",
    "        VariantAnnotation::geno(hdr) <- S4Vectors::DataFrame(Number = c(\"A\", \n",
    "            \"A\", \"A\", \"A\", \"A\", \"A\"), Type = c(\"Float\", \"Float\", \n",
    "            \"Float\", \"Float\", \"Float\", \"Float\"), Description = c(\"Effect size estimate relative to the alternative allele\", \n",
    "            \"Standard error of effect size estimate\", \"-log10 p-value for effect estimate\",  \n",
    "            \"Alternate allele frequency in the association study\",\n",
    "            \"The CS this variate are captured, 0 indicates not in any cs\", \"The posterior inclusion probability to a CS\"), \n",
    "            row.names = c(\"ES\", \"SE\", \"LP\", \"AF\", \"CS\", \"PIP\"))\n",
    "      ## Save only the meta information in the sample columns \n",
    "        VariantAnnotation::geno(hdr) <- subset(VariantAnnotation::geno(hdr), \n",
    "            rownames(VariantAnnotation::geno(hdr)) %in% names(gen))\n",
    "      ## Save VCF \n",
    "        vcf <- VariantAnnotation::VCF(rowRanges = gr, colData = coldata, \n",
    "            exptData = list(header = hdr), geno = gen)\n",
    "        VariantAnnotation::alt(vcf) <- Biostrings::DNAStringSetList(as.list(ea))\n",
    "        VariantAnnotation::ref(vcf) <- Biostrings::DNAStringSet(nea)\n",
    "      ## Add fixed values\n",
    "        VariantAnnotation::fixed(vcf)$FILTER <- \"PASS\"\n",
    "          return(sort(vcf))\n",
    "        }\n",
    "    library(\"susieR\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"stringr\")\n",
    "    \n",
    "    # Get list of cs snps\n",
    "    susie_list = readRDS(${_input:r})\n",
    "    susie_tb_ls = list()\n",
    "    for (i in 1:length(susie_list)){\n",
    "        susie_tb = tibble( snps =  names(susie_list[[1]]$pip)[which( susie_list[[i]]$pip >= 0)], snps_index = which(( susie_list[[i]]$pip >= 0))  )\n",
    "        susie_tb_ls[[i]]= susie_tb%>%mutate( cs = map(snps_index,~which( susie_list[[i]]$sets$cs %in% .x))%>%as.numeric%>%replace_na(0),\n",
    "                                 pip = map_dbl(snps_index,~( susie_list[[i]]$pip[.x])),\n",
    "                                 coef = map_dbl(snps_index,~(coef.susie( susie_list[[i]])[.x+1])))\n",
    "        }\n",
    "    if(length(susie_tb_ls) >= 2){ \n",
    "      for(i in 2:length(susie_tb_ls)){\n",
    "          susie_tb_ls[[i]] = full_join(susie_tb_ls[[i-1]],susie_tb_ls[[i]], by = \"snps\") \n",
    "        }\n",
    "    }\n",
    "    m = c(\"cs\",\"pip\",\"coef\")    \n",
    "    output = list()\n",
    "    for(i in m){\n",
    "    output[[i]] = susie_tb_ls[[length(susie_tb_ls)]]%>%select(contains(i))%>%as.matrix\n",
    "    }\n",
    "    snps_tb = susie_tb_ls[[length(susie_tb_ls)]]%>%mutate(\n",
    "                         chr = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = T)$V1),\n",
    "                         pos_alt_ref = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = TRUE)$V2),\n",
    "                         pos = map_dbl(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE)$V1),\n",
    "                         alt = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V2),\n",
    "                         ref = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V3))\n",
    "    \n",
    "    snps_tb = snps_tb%>%filter(str_detect(ref, \"[ACTG]\") & str_detect(alt, \"[ACTG]\"))\n",
    "    output_vcf = create_vcf(\n",
    "            chrom = snps_tb$chr,\n",
    "             pos = snps_tb$pos,\n",
    "             ea = snps_tb$alt,\n",
    "             nea = snps_tb$ref,\n",
    "             effect = snps_tb%>%select(contains(\"coef\"))%>%as.matrix ,\n",
    "             pip = snps_tb%>%select(contains(\"pip\"))%>%as.matrix,\n",
    "             cs = snps_tb%>%select(contains(\"cs\"))%>%as.matrix,\n",
    "             name = names(susie_list))\n",
    "    VariantAnnotation::writeVcf(output_vcf,${_output:nr},index = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-harvest",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[mv_susie_2]\n",
    "input: group_with = \"genoFile\"\n",
    "output: f\"{_input:n}.vcf.bgz\"\n",
    "task: trunk_workers = 1, trunk_size = 1, walltime = '2h', mem = '55G', cores = 1, tags = f'{step_name}_{_output[0]:bn}'\n",
    "R: expand = '${ }', stdout = f\"{_output:nn}.stdout\", stderr = f\"{_output:nn}.stderr\", container = container, entrypoint = entrypoint\n",
    "   ## Define create_vcf function\n",
    "           create_vcf = function (chrom, pos, nea, ea, snp = NULL, ea_af = NULL, effect = NULL, \n",
    "        se = NULL, pval = NULL, name = NULL,cs = NULL, pip = NULL) \n",
    "    {\n",
    "        stopifnot(length(chrom) == length(pos))\n",
    "        if (is.null(snp)) {\n",
    "            snp <- paste0(chrom, \":\", pos)\n",
    "        }\n",
    "        snp <- paste0(chrom, \":\", pos)\n",
    "        nsnp <- length(chrom)\n",
    "        gen <- list()\n",
    "        ## Setupt data content for each sample column\n",
    "        if (!is.null(ea_af)) \n",
    "            gen[[\"AF\"]] <- matrix(ea_af, nsnp)\n",
    "        if (!is.null(effect)) \n",
    "            gen[[\"ES\"]] <- matrix(effect, nsnp)\n",
    "        if (!is.null(se)) \n",
    "            gen[[\"SE\"]] <- matrix(se, nsnp)\n",
    "        if (!is.null(pval)) \n",
    "            gen[[\"LP\"]] <- matrix(-log10(pval), nsnp)\n",
    "        if (!is.null(cs)) \n",
    "            gen[[\"CS\"]] <- matrix(cs, nsnp)\n",
    "        if (!is.null(pip)) \n",
    "            gen[[\"PIP\"]] <- matrix(pip, nsnp)\n",
    "        gen <- S4Vectors::SimpleList(gen)\n",
    "        \n",
    "      ## Setup snps info for the fix columns\n",
    "        gr <- GenomicRanges::GRanges(chrom, IRanges::IRanges(start = pos, \n",
    "            end = pos + pmax(nchar(nea), nchar(ea)) - 1, names = snp))\n",
    "         coldata <- S4Vectors::DataFrame(Studies = name, row.names = name)\n",
    "    ## Setup header informations\n",
    "        hdr <- VariantAnnotation::VCFHeader(header = IRanges::DataFrameList(fileformat = S4Vectors::DataFrame(Value = \"VCFv4.2\", \n",
    "            row.names = \"fileformat\")), sample = name)\n",
    "        VariantAnnotation::geno(hdr) <- S4Vectors::DataFrame(Number = c(\"A\", \n",
    "            \"A\", \"A\", \"A\", \"A\", \"A\"), Type = c(\"Float\", \"Float\", \n",
    "            \"Float\", \"Float\", \"Float\", \"Float\"), Description = c(\"Effect size estimate relative to the alternative allele\", \n",
    "            \"Standard error of effect size estimate\", \"-log10 p-value for effect estimate\",  \n",
    "            \"Alternate allele frequency in the association study\",\n",
    "            \"The CS this variate are captured, 0 indicates not in any cs\", \"The posterior inclusion probability to a CS\"), \n",
    "            row.names = c(\"ES\", \"SE\", \"LP\", \"AF\", \"CS\", \"PIP\"))\n",
    "      ## Save only the meta information in the sample columns \n",
    "        VariantAnnotation::geno(hdr) <- subset(VariantAnnotation::geno(hdr), \n",
    "            rownames(VariantAnnotation::geno(hdr)) %in% names(gen))\n",
    "      ## Save VCF \n",
    "        vcf <- VariantAnnotation::VCF(rowRanges = gr, colData = coldata, \n",
    "            exptData = list(header = hdr), geno = gen)\n",
    "        VariantAnnotation::alt(vcf) <- Biostrings::DNAStringSetList(as.list(ea))\n",
    "        VariantAnnotation::ref(vcf) <- Biostrings::DNAStringSet(nea)\n",
    "      ## Add fixed values\n",
    "        VariantAnnotation::fixed(vcf)$FILTER <- \"PASS\"\n",
    "          return(sort(vcf))\n",
    "        }\n",
    "    library(\"susieR\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tibble\")\n",
    "    library(\"purrr\")\n",
    "    library(\"readr\")\n",
    "    library(\"tidyr\")\n",
    "    \n",
    "    # Get list of cs snps\n",
    "    res = readRDS(${_input:r})\n",
    "    output_snps = tibble( snps = res$variable_name[which(res$pip >= 0)], snps_index = which((res$pip >= 0))  )\n",
    "    output_snps = output_snps%>%mutate( cs = map(snps_index,~which(res$sets$cs %in% .x))%>%as.numeric%>%replace_na(0),\n",
    "                             pip = map_dbl(snps_index,~(res$pip[.x])),\n",
    "                     chr = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = T)$V1),\n",
    "                     pos_alt_ref = map_chr(snps,~read.table(text = .x,sep = \":\",as.is = TRUE)$V2),\n",
    "                     pos = map_dbl(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE)$V1),\n",
    "                     alt = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V2),\n",
    "                     ref = map_chr(pos_alt_ref,~read.table(text = .x,sep = \"_\",as.is = TRUE, colClass = \"character\")$V3))\n",
    "    \n",
    "    effect_mtr = res$coef[output_snps$snps_index+1]%>%as.matrix\n",
    "    colnames(effect_mtr) = \"${name}\"\n",
    "    rownames(effect_mtr) = output_snps$snps\n",
    "    cs_mtr = effect_mtr\n",
    "    for(i in 1:nrow(cs_mtr)) cs_mtr[i,] =  output_snps$cs[[i]]  \n",
    "    pip_mtr = effect_mtr\n",
    "    for(i in 1:nrow(pip_mtr)) pip_mtr[i,] =  output_snps$pip[[i]]  \n",
    "    \n",
    "    output_vcf = create_vcf(\n",
    "           chrom = output_snps$chr,\n",
    "            pos = output_snps$pos,\n",
    "            ea = output_snps$alt,\n",
    "            nea = output_snps$ref,\n",
    "            effect = effect_mtr ,\n",
    "            pip = pip_mtr,\n",
    "            cs = cs_mtr,\n",
    "            name = colnames(effect_mtr)\n",
    "              )\n",
    "    VariantAnnotation::writeVcf(output_vcf,${_output:nr},index = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd0a65-ac69-4106-b48c-463b6525a671",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Cis-window analysis Result consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a0d94c-b049-4f71-948e-9520e66ecba9",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "# Exporting cis susie_twas results\n",
    "[cis_results_export_1, gwas_results_export_1]\n",
    "# per chunk we process at most 200 datasets\n",
    "parameter: per_chunk = 200\n",
    "# Region list should have last column being region name. \n",
    "# Meta-data will be written to this column\n",
    "parameter:region_file=path()\n",
    "parameter:file_path=''\n",
    "parameter:prefix=[]\n",
    "parameter:suffix=str\n",
    "parameter: condition_meta = path()\n",
    "# if the pip all variants in a cs < this threshold, then remove this cs\n",
    "parameter: pip_thres = 0.05\n",
    "# only keep top cs_size variants in one cs \n",
    "parameter: cs_size = 3\n",
    "# provide exported meta to filter the exported genes \n",
    "parameter: exported_file = path()\n",
    "# optional: qtl or gwas, there is slightly different in qtl and gwas rds file\n",
    "parameter: data_type = str\n",
    "# Optional: if a region list is provide the analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "# Otherwise, all regions with both genotype and phenotype files will be analyzed\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "parameter: min_corr = 0.5\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "region = pd.read_csv(region_file, sep='\\t', names=['chr', 'start', 'end', 'id'])\n",
    "region_ids = []\n",
    "# If region_list is provided, read the file and extract IDs\n",
    "if not region_list.is_dir():\n",
    "    if region_list.is_file():\n",
    "        region_list_df = pd.read_csv(region_list, sep='\\t', header=None, comment = \"#\")\n",
    "        region_ids = region_list_df.iloc[:, -1].unique()  # Extracting the last column for IDs\n",
    "    else:\n",
    "        raise ValueError(\"The region_list path provided is not a file.\")\n",
    "        \n",
    "if len(region_name) > 0:\n",
    "    region_ids = list(set(region_ids).union(set(region_name)))\n",
    "    \n",
    "if len(region_ids) > 0:\n",
    "    region = region[region['id'].isin(region_ids)]\n",
    "\n",
    "# Function to create list of formatted strings for each row\n",
    "def create_formatted_list(row):\n",
    "    if len(prefix) > 0:\n",
    "        formatted_list = [f\"{file_path}/{p}.{row['id']}.{suffix}\" for p in prefix]\n",
    "    else:\n",
    "        formatted_list = [f\"{file_path}/{row['id']}.{suffix}\"]  # GWAS data do not have prefix         \n",
    "    return formatted_list\n",
    "# Apply the function to each row\n",
    "region['original_data'] = region.apply(create_formatted_list, axis=1)\n",
    "\n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "def filter_existing_paths(row):\n",
    "    existing_paths = [path for path in row if os.path.exists(path)]\n",
    "    return existing_paths\n",
    "\n",
    "def is_file_exported(paths, results_set):\n",
    "    for path in paths:\n",
    "        basename = os.path.basename(path)\n",
    "        if basename not in results_set:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "region['original_data'] = region['original_data'].apply(filter_existing_paths)\n",
    "region = region[region['original_data'].map(bool)]\n",
    "\n",
    "# if provided exported meta, check if the original data are all exported already, isfo skip them \n",
    "if not exported_file.is_dir():\n",
    "    if exported_file.is_file():\n",
    "        results = pd.read_csv(exported_file, sep='\\t')\n",
    "        results_set = set(results['original_data'])\n",
    "        results_set = {item.strip() for sub in results_set for item in sub.split(',')}\n",
    "        mask = region['original_data'].apply(lambda paths: not is_file_exported(paths, results_set))\n",
    "        region = region[mask]\n",
    "    else:\n",
    "        raise ValueError(\"The exported_file path provided is not a file.\")\n",
    "\n",
    "regional_data = {\n",
    "    'meta': [(row['chr'], row['start'], row['end'], row['id']) for _, row in region.iterrows()],\n",
    "    'data': [(row['original_data']) for _, row in region.iterrows()]\n",
    "}\n",
    "\n",
    "meta_info = regional_data['meta']\n",
    "stop_if(len(regional_data['data']) == 0, f' All files have been exported already')\n",
    "\n",
    "input: regional_data[\"data\"], group_by = lambda x: group_by_region(x, regional_data[\"data\"]), group_with = \"meta_info\"\n",
    "output: f\"{cwd}/{name}_cache/{name}_batch{_index+1}.tsv\"\n",
    "task: trunk_workers = job_size, walltime = walltime, trunk_size = job_size, mem = mem, cores = numThreads, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\",stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container, entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "    # check top_loci existing or not\n",
    "    has_rows <- function(df) {\n",
    "      !is.null(df) && nrow(df) > 0\n",
    "    }\n",
    "    # update top loci table\n",
    "    update_top_loci_min_corr <- function(dat_susie, top_loci_df, coverage_value, threshold = ${min_corr}) {\n",
    "      if(coverage_value == 'cs_coverage_0.95') purity_res <- dat_susie$sets$purity\n",
    "       else purity_res <- dat_susie$sets_secondary[[gsub('cs_','',coverage_value)]]$sets$purity\n",
    "      not_pass_min_cs <- rownames(purity_res)[purity_res$min.abs.corr < threshold] %>%\n",
    "        gsub('L', '', .)\n",
    "      top_loci_df[[paste0( coverage_value, \"_min_corr\")]] <- top_loci_df[[coverage_value]]\n",
    "      if (length(not_pass_min_cs) > 0) {\n",
    "        top_loci_df[[paste0( coverage_value, \"_min_corr\")]][top_loci_df[[coverage_value]] %in% not_pass_min_cs] <- 0\n",
    "      }\n",
    "        return(top_loci_df)\n",
    "    }\n",
    "\n",
    "    # function to filter the cs with all variants pip < 0.05\n",
    "    update_and_filter_cs_ids <- function(dat_susie, df) {\n",
    "      cs_columns <- grep(\"^cs_coverage\", names(df), value = TRUE)\n",
    "      df$cs_all_non_zero_orig <- ifelse(rowSums(df[cs_columns] == 0) == length(cs_columns), FALSE, TRUE)\n",
    "      for (cs_column in cs_columns) {\n",
    "        unique_cs_ids <- unique(df[[cs_column]])\n",
    "        for (cs_id in unique_cs_ids) {\n",
    "          if (cs_id > 0) {\n",
    "            # pip check\n",
    "            pip_check <- df[df[[cs_column]] == cs_id, \"pip\"] < 0.05\n",
    "            if (all(pip_check, na.rm = TRUE)) {\n",
    "              df[[cs_column]] <- ifelse(df[[cs_column]] == cs_id, 0, df[[cs_column]])\n",
    "            }\n",
    "\n",
    "            # size check\n",
    "            # new_col_name <- paste0(cs_column, \"_refine\")\n",
    "            # df <- df %>%\n",
    "            #   group_by(.data[[cs_column]]) %>%\n",
    "            #   mutate(rank = row_number(-pip)) %>%\n",
    "            #   mutate(!!new_col_name := if_else(rank <= 3, .data[[cs_column]], as.integer(0))) %>% # if we want to replace original cs_column, just replace new_col_name with cs_column here, would filter it\n",
    "            #   ungroup() %>%\n",
    "            #   select(-rank)\n",
    "\n",
    "          }\n",
    "         df <- update_top_loci_min_corr(dat_susie, top_loci_df = df,coverage_value = cs_column)   \n",
    "        }\n",
    "      }\n",
    "      # Filter out rows where all cs_coverage columns are 0 and cs_all_non_zero_orig is TRUE\n",
    "      df <- df %>% filter(!(cs_all_non_zero_orig & rowSums(df[cs_columns] == 0) == length(cs_columns)))\n",
    "      # Remove the helper column\n",
    "      df <- select(df, -cs_all_non_zero_orig)\n",
    "      return(df)\n",
    "    }\n",
    "\n",
    "    refine_cs_size <- function(df){\n",
    "      return(x)\n",
    "    }\n",
    "\n",
    "    # function to decide run update_and_filter_cs_ids or not\n",
    "    process_top_loci <- function(dat_susie, data_frame) {\n",
    "      if (has_rows(data_frame) > 0) {\n",
    "        return(update_and_filter_cs_ids(dat_susie, data_frame))\n",
    "      } else {\n",
    "        return(data_frame)\n",
    "      }\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    # Process each path and collect results\n",
    "    orig_files = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input])})\n",
    "     # Extract info from each RDS file\n",
    "    results <- list()\n",
    "    gene = \"${_meta_info[3]}\"\n",
    "\n",
    "    # Post Processing: Extracting info\n",
    "    # use for loop instead of apply to save memory\n",
    "    res <- res_sum <- cons_top_loci <- list()\n",
    "    for(i in seq_along(orig_files)) {\n",
    "      rds_path <- orig_files[i]\n",
    "       dat <- tryCatch({\n",
    "        readRDS(rds_path)\n",
    "      }, error = function(e) {\n",
    "        writeLines(rds_path %>% basename, gsub(\".tsv\",\"_error\",\"${_output}\"))\n",
    "        return(NULL) # \n",
    "      })\n",
    "\n",
    "      if(is.null(dat)) next\n",
    "  \n",
    "      #extract qtl type from susie rds file name, if we have set decent condtiton name, this could be removed \n",
    "      # qtl_type <- sub(\"(\\\\.*?)\\\\..*\", \"\\\\1\", basename(rds_path)) %>% sub(\".*?_\", \"\\\\1\", .)\n",
    "\n",
    "      temp_list <- list() # Temporary list to store inner results\n",
    "        for(s in names(dat)){\n",
    "         dat_study <- dat[[s]]\n",
    "          temp_list <- list()\n",
    "          conditions <- names(dat_study) %>% .[!str_detect(pattern = 'sumstats',.)]\n",
    "  \n",
    "          if(length(conditions) > 0) {\n",
    "  \n",
    "            for(condition in conditions) {\n",
    "                dat_con <- dat_study[[condition]]\n",
    "                dat_susie <- dat_con$susie_result_trimmed\n",
    "  \n",
    "                if(${\"TRUE\" if condition_meta.is_file() else \"FALSE\"}){\n",
    "                      meta <- suppressMessages(read_delim(\"${condition_meta}\", col_names = F))\n",
    "                      qtl_con <- meta %>% filter(X1 == condition) %>% pull(X2)\n",
    "                      if (length(qtl_con) == 0) {\n",
    "                        qtl_con <- condition\n",
    "                        message(\"No matching entries found. qtl_con has been set to the condition value.\")\n",
    "                      }\n",
    "                    } else {qtl_con <- condition}\n",
    "  \n",
    "                res[[gene]][[s]][[qtl_con]] <- list(\n",
    "                  region_info = dat_con$region_info,\n",
    "                  top_loci = process_top_loci(dat_susie, dat_con$top_loci),\n",
    "                  preset_top_loci = process_top_loci(dat_susie, dat_con$preset_variants_result$top_loci),\n",
    "                  pip = dat_con$susie_result_trimmed$pip,\n",
    "                  variant_names = dat_con$variant_names,\n",
    "                  CV_table = dat_con$twas_cv_result$performance\n",
    "                )\n",
    "  \n",
    "                res_sum[[gene]][[s]][[qtl_con]] <- list(\n",
    "                  variant_names = dat_con$variant_names,\n",
    "                  sumstats = dat_con$sumstats\n",
    "                )\n",
    "                \n",
    "                if(has_rows(dat_con$top_loci) || has_rows(dat_con$preset_top_loci)) cons_top_loci[[gene]][[s]][[qtl_con]] <- qtl_con else  cons_top_loci[[gene]][[s]][[qtl_con]] <- NULL\n",
    "              }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    cons_top_loci <- cons_top_loci %>% compact()  # Use 'compact' to remove NULLs, assuming 'results' has the correct structure  \n",
    "    if ('${data_type}' == 'qtl'){\n",
    "      res = res[[1]]\n",
    "      res_sum = res_sum[[1]]\n",
    "      if(length(cons_top_loci) > 0) cons_top_loci = cons_top_loci[[1]] else cons_top_loci = NA\n",
    "    }\n",
    "  \n",
    "    combine_data = paste0(\"${_output:add}\",\"/\",\"${name}\", \".\", gene, \".cis_results_db.export.rds\")\n",
    "    combine_data_sumstats = paste0(\"${_output:add}\",\"/\",\"${name}\", \".\", gene, \".cis_results_db.export_sumstats.rds\")\n",
    "  \n",
    "    if (${\"TRUE\" if exported_file.is_file() else \"FALSE\"}){\n",
    "        res_exp <- readRDS(combine_data)\n",
    "        res[[gene]] <- c(res[[gene]], res_exp[[gene]])\n",
    "        res_sum_exp <- readRDS(combine_data_sumstats)\n",
    "        res_sum[[gene]] <- c(res_sum[[gene]], res_sum_exp[[gene]])\n",
    "    }\n",
    "    saveRDS(res, combine_data)\n",
    "    saveRDS(res_sum, combine_data_sumstats)\n",
    "\n",
    "    meta = data.frame(chr=\"${_meta_info[0]}\", start=\"${_meta_info[1]}\", end=\"${_meta_info[2]}\", gene_id=\"${_meta_info[3]}\", TSS = dat_con$region_info$region_coord$start, \n",
    "                      original_data = paste(basename(orig_files), collapse = \", \"), combined_data = basename(combine_data), combined_data_sumstats = basename(combine_data_sumstats), \n",
    "                      conditions = paste(names(res[[gene]]), collapse = \",\"), \n",
    "                      conditions_top_loci = if(length(cons_top_loci) > 0) cons_top_loci[[1]] %>% unlist %>% names %>% as.character %>% paste(., collapse = ',') else '')\n",
    "    write_delim(meta, \"${_output}\", delim = '\\t')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9544df-cce9-47c4-a44e-b81cd2ed8f72",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[cis_results_export_2, gwas_results_export_2]\n",
    "# provide exported meta to filter the exported genes \n",
    "parameter: exported_file = path()\n",
    "# optional: qtl or gwas, there is slightly different in qtl and gwas rds file\n",
    "parameter: data_type = str\n",
    "input: group_by = 'all'\n",
    "output: f\"{cwd}/{name}.{'cis_results_db' if data_type == 'qtl' else 'block_results_db'}.tsv\" \n",
    "# stop_if(_input[0] not in locals().keys(), 'All files have been exported already') #FIXME should we remove to a separate file. sothat we can stop globally as above\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    if [ -e \"${_output:ad}/${name}_cache/\" ]; then\n",
    "        sed 's/^chr/#chr/' `ls ${_output:ad}/${name}_cache/*tsv |head -n1` | head -n 1 > ${_output:an}.temp\n",
    "        tail -n +2 -q ${_output:ad}/${name}_cache/*.tsv >> ${_output:an}.temp\n",
    "        error_files=$(find \"${_output:ad}/${name}_cache/\" -type f -name \"*_error\")\n",
    "\n",
    "        if [[ -n $error_files ]]; then\n",
    "            cat $error_files >> ${_output:an}.error_genes\n",
    "        else\n",
    "            echo \"No truncated files detected\"\n",
    "        fi\n",
    "    else\n",
    "        echo \"All files have been exported already\"\n",
    "    fi\n",
    "    \n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    if (file.exists(paste0(${_output:anr},\".temp\"))) {\n",
    "        library(tidyverse)\n",
    "        meta <- read_delim(paste0(${_output:anr},\".temp\"), delim = '\\t')\n",
    "\n",
    "        if (${\"TRUE\" if exported_file.is_file() else \"FALSE\"}){\n",
    "          exp_meta <- read_delim(${_output:r}, delim = '\\t')\n",
    "          meta <- bind_rows(meta, exp_meta) %>%\n",
    "              group_by(gene_id) %>%\n",
    "              summarise(across(c(original_data, combined_data, combined_data_sumstats, conditions, conditions_top_loci), \n",
    "                               ~paste(unique(.), collapse = \",\")),\n",
    "                        .groups = 'drop')\n",
    "              }\n",
    "\n",
    "        write_delim(meta, ${_output:r}, delim = '\\t')\n",
    "    }\n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    rm -rf ${_output:ad}/${name}_cache/\n",
    "    rm -rf ${_output:an}.temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753185c2-0c82-4842-bd64-0c86152f2599",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## GWAS results consolidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a485f0-941b-4f17-95a7-f04da0e12d21",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "#get union of step1\n",
    "#1200 blocks costed ~2mins with in one for loop\n",
    "[gwas_results_export_3]\n",
    "output: f\"{cwd}/{name}.union_export.tsv.gz\" \n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = 1, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "    library(data.table)\n",
    "\n",
    "    mtx <- read_delim(${_input:r})\n",
    "    files <- mtx %>% filter(!is.na(conditions_top_loci)) %>% pull(combined_data) %>% paste0(${_input[0]:dr},'/','${name}','_cache/',.)\n",
    "    all_top_loci <- data.frame()\n",
    "\n",
    "    for (file in files) {\n",
    "      res <- readRDS(file)\n",
    "\n",
    "      file_top_loci <- lapply(names(res), function(block) {\n",
    "        lapply(names(res[[block]]), function(study) {\n",
    "          lapply(names(res[[block]][[study]]), function(method) {\n",
    "            if (!is.null(res[[block]][[study]][[method]]$top_loci)) {\n",
    "              temp_df <- res[[block]][[study]][[method]]$top_loci\n",
    "              mutate(temp_df, study = study, method = method, block = block)\n",
    "            } else {\n",
    "              NULL  \n",
    "            }\n",
    "          })\n",
    "        }) %>% bind_rows()  \n",
    "      }) %>% bind_rows() \n",
    "\n",
    "      all_top_loci <- bind_rows(all_top_loci, file_top_loci)\n",
    "    }\n",
    "\n",
    "    fwrite(all_top_loci, ${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a563cfe-7cff-4536-b440-c34126e7a192",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overlap QTL and GWAS results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4860a8e7-60ba-4c06-b1fa-80075b463392",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[overlap_qtl_gwas_1]\n",
    "parameter: per_chunk = 100\n",
    "parameter: gwas_meta_path = path()\n",
    "parameter: qtl_file_path = ''\n",
    "parameter: gwas_file_path = ''\n",
    "parameter: qtl_meta_path = path()\n",
    "# Optional: if a region list is provide the analysis will be focused on provided region. \n",
    "# The LAST column of this list will contain the ID of regions to focus on\n",
    "# Otherwise, all regions with both genotype and phenotype files will be analyzed\n",
    "parameter: region_list = path()\n",
    "# Optional: if a region name is provided \n",
    "# the analysis would be focused on the union of provides region list and region names\n",
    "parameter: region_name = []\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the data, suppressing messages is not typically done in pandas as it does not inherently output messages when loading files\n",
    "gwas_meta = pd.read_csv(gwas_meta_path, sep='\\t', low_memory=False)\n",
    "gwas_meta = gwas_meta[gwas_meta['conditions_top_loci'].notna()]\n",
    "\n",
    "qtl_meta = pd.read_csv(qtl_meta_path, sep='\\t', low_memory=False)\n",
    "qtl_meta = qtl_meta[qtl_meta['conditions_top_loci'].notna()]\n",
    "\n",
    "# Filter and mutate operations, translated to pandas\n",
    "gwas_meta['combined_data_toploci'] = gwas_meta.apply(lambda row: row['combined_data'] if pd.notnull(row['conditions_top_loci']) else pd.NA, axis=1)\n",
    "\n",
    "region_ids=[]\n",
    "# If region_list is provided, read the file and extract IDs\n",
    "if not region_list.is_dir():\n",
    "    if region_list.is_file():\n",
    "        region_list_df = pd.read_csv(region_list, sep='\\t', header=None, comment = \"#\")\n",
    "        region_ids = region_list_df.iloc[:, -1].unique()  # Extracting the last column for IDs\n",
    "    else:\n",
    "        raise ValueError(\"The region_list path provided is not a file.\")\n",
    "        \n",
    "if len(region_name) > 0:\n",
    "    region_ids = list(set(region_ids).union(set(region_name)))\n",
    "    \n",
    "if len(region_ids) > 0:\n",
    "    qtl_meta = qtl_meta[qtl_meta['gene_id'].isin(region_ids)]\n",
    "    \n",
    "def group_by_region(lst, partition):\n",
    "    # from itertools import accumulate\n",
    "    # partition = [len(x) for x in partition]\n",
    "    # Compute the cumulative sums once\n",
    "    # cumsum_vector = list(accumulate(partition))\n",
    "    # Use slicing based on the cumulative sums\n",
    "    # return [lst[(cumsum_vector[i-1] if i > 0 else 0):cumsum_vector[i]] for i in range(len(partition))]\n",
    "    return partition\n",
    "\n",
    "grouped_gwas_meta = {k: v for k, v in gwas_meta.groupby('#chr')}\n",
    "def check_overlap(gene_row, grouped_gwas_meta):\n",
    "    chr_group = gene_row['#chr']\n",
    "    if chr_group in grouped_gwas_meta:\n",
    "        block_region = grouped_gwas_meta[chr_group]\n",
    "        overlaps = block_region[\n",
    "            (block_region['start'] <= gene_row['end']) &\n",
    "            (block_region['end'] >= gene_row['start'])\n",
    "        ]\n",
    "        if not overlaps.empty:\n",
    "            return ','.join(overlaps['combined_data_toploci'].astype(str))\n",
    "    return pd.NA\n",
    "\n",
    "stop_if(len(qtl_meta) == 0, f'No file left for analysis ')\n",
    "\n",
    "qtl_meta_cand = qtl_meta.apply(lambda row: pd.Series({\n",
    "    'gwas_file': check_overlap(row, grouped_gwas_meta)\n",
    "}), axis=1)\n",
    "\n",
    "# Concatenate the new columns to the original qtl_meta DataFrame\n",
    "qtl_meta_cand = pd.concat([qtl_meta, qtl_meta_cand], axis=1)\n",
    "qtl_meta_filtered = qtl_meta_cand[qtl_meta_cand['gwas_file'].notna()]\n",
    "qtl_meta_filtered = qtl_meta_filtered.dropna(subset=['gwas_file'])\n",
    "\n",
    "\n",
    "regional_data = {\n",
    "    'meta': [(row['#chr'], row['start'], row['end'], row['gene_id'], row['gwas_file'].split(',')) for _, row in qtl_meta_filtered.iterrows()],\n",
    "    'qtl_data': [f\"{qtl_file_path}/{row['combined_data']}\" for _, row in qtl_meta_filtered.iterrows()]\n",
    "}\n",
    "\n",
    "meta_info = regional_data['meta']\n",
    "stop_if(len(regional_data['qtl_data']) == 0, f'No file left for analysis ')\n",
    "\n",
    "input: regional_data[\"qtl_data\"], group_by = lambda x: group_by_region(x, regional_data[\"qtl_data\"]), group_with = \"meta_info\"\n",
    "output: f\"{cwd}/gwas_qtl/cache/{name}_gwas_batch_meta{_index+1}.tsv\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = job_size, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    library(tidyverse)\n",
    "    library(plyr)\n",
    "    library(data.table)\n",
    "    # Function to check if the variant or its flipped version is in qtl_var\n",
    "    check_flip <- function(variant, qtl_var) {\n",
    "      # Strand flip function\n",
    "      strand_flip <- function(ref) {\n",
    "        flip <- ref\n",
    "        flip[ref == \"A\"] <- \"T\"\n",
    "        flip[ref == \"T\"] <- \"A\"\n",
    "        flip[ref == \"G\"] <- \"C\"\n",
    "        flip[ref == \"C\"] <- \"G\"\n",
    "        return(flip)\n",
    "      }\n",
    "       # Split the variant string into parts\n",
    "      variant_parts <- str_split(variant, ':', simplify = TRUE)[1, ]\n",
    "\n",
    "      # Extract the reference and alternate alleles\n",
    "      ref_allele <- variant_parts[3]\n",
    "      alt_allele <- variant_parts[4]\n",
    "\n",
    "      # Flip the alleles using the strand_flip function\n",
    "      flipped_ref <- strand_flip(ref_allele)\n",
    "      flipped_alt <- strand_flip(alt_allele)\n",
    "\n",
    "      # Create the flipped variant string\n",
    "      variant_flip <- paste(variant_parts[1], variant_parts[2], alt_allele, ref_allele, sep = \":\") # chr:pos:A:C & chr:pos:C:A\n",
    "      variant_strand <- paste(variant_parts[1], variant_parts[2], flipped_alt, flipped_ref, sep = \":\") #  chr:pos:A:C & chr:pos:T:G\n",
    "      variant_strand_flip <- paste(variant_parts[1], variant_parts[2], flipped_ref, flipped_alt, sep = \":\") # chr:pos:A:C & chr:pos:G:T\n",
    "\n",
    "      # Check if either the original variant or the flipped variant is in qtl_var\n",
    "      return(variant %in% qtl_var | variant_flip %in% qtl_var | variant_strand %in% qtl_var | variant_strand_flip %in% qtl_var)\n",
    "    }\n",
    "\n",
    "\n",
    "    # Function to add 'chr' in variants\n",
    "    add_chr_prefix <- function(var) {\n",
    "      if (any(grepl(\"chr\", var))) {\n",
    "        var <- var\n",
    "      } else {\n",
    "        var <- paste0(\"chr\", var)\n",
    "      }\n",
    "      return(var)\n",
    "    }\n",
    "\n",
    "    # Function to check if a dataframe has rows\n",
    "    has_rows <- function(df) {\n",
    "      !is.null(df) && nrow(df) > 0\n",
    "    }\n",
    "\n",
    "\n",
    "    extract_top_loci <- function(res, include_method = FALSE) {\n",
    "      all_top_loci <- lapply(names(res), function(region) {\n",
    "        lapply(names(res[[region]]), function(study) {\n",
    "          if (include_method) {\n",
    "            method_results <- lapply(names(res[[region]][[study]]), function(method) {\n",
    "              top_loci <- NULL\n",
    "              if (!is.null(res[[region]][[study]][[method]]$top_loci) && nrow(res[[region]][[study]][[method]]$top_loci) > 0) {\n",
    "                top_loci <- mutate(res[[region]][[study]][[method]]$top_loci, study = study, method = method, region = region)\n",
    "              }\n",
    "              return(top_loci)\n",
    "            })\n",
    "            return(bind_rows(method_results))\n",
    "          } else {\n",
    "            top_loci <- list()\n",
    "            if (!is.null(res[[region]][[study]]$top_loci) && nrow(res[[region]][[study]]$top_loci) > 0) {\n",
    "              top_loci[[length(top_loci) + 1]] <- mutate(res[[region]][[study]]$top_loci, study = study, region = region, method = 'top_loci')\n",
    "            }\n",
    "            if (!is.null(res[[region]][[study]]$preset_top_loci) && nrow(res[[region]][[study]]$preset_top_loci) > 0) {\n",
    "              top_loci[[length(top_loci) + 1]] <- mutate(res[[region]][[study]]$preset_top_loci, study = study, region = region, method = 'preset_top_loci')\n",
    "            }\n",
    "            return(bind_rows(top_loci))\n",
    "          }\n",
    "        })\n",
    "      }) %>% bind_rows() %>% na.omit()\n",
    "\n",
    "      return(all_top_loci)\n",
    "    }\n",
    "\n",
    "  \n",
    "    # load data \n",
    "    qtl_file = c(${\",\".join(['\"%s\"' % x.absolute() for x in _input])})\n",
    "     # Extract info from each RDS file\n",
    "    gwas_files = c(${\",\".join('\"%s\"' % x for x in _meta_info[4])}) %>% paste0('${gwas_file_path}','/',.)\n",
    "    # Process GWAS files\n",
    "    gwas_all_top_loci <- do.call(rbind, lapply(gwas_files, function(file) {\n",
    "      res <- readRDS(file)\n",
    "      gwas_all_top_loci <- extract_top_loci(res, include_method = TRUE) \n",
    "    }))\n",
    "    if(!is.null(gwas_all_top_loci) && nrow(gwas_all_top_loci) > 0){# fixme: could remove this judge if we get a solid enough meta\n",
    "      gwas_all_top_loci <- gwas_all_top_loci%>% select(-c('z'))\n",
    "\n",
    "      # Process QTL file\n",
    "      qtl <- readRDS(qtl_file)\n",
    "      qtl_all_top_loci <- extract_top_loci(qtl) \n",
    "      if(!is.null(qtl_all_top_loci) && nrow(qtl_all_top_loci) > 0){# fixme: could remove this judge if we get a solid enough meta\n",
    "        # qtl_all_top_loci <- qtl_all_top_loci%>% select(-c('betahat','sebetahat','maf'))\n",
    "\n",
    "        cs_cal <- c('cs_coverage_0.95','cs_coverage_0.7','cs_coverage_0.5')\n",
    "\n",
    "        qtl_all_var <- qtl_all_top_loci %>%\n",
    "            #filter(rowSums(.[,cs_cal]) > 0 ) %>% #fixme\n",
    "            pull(variant_id)\n",
    "\n",
    "        gwas_all_var <- gwas_all_top_loci %>%\n",
    "            #filter(rowSums(.[,cs_cal]) > 0 ) %>% #fixme\n",
    "            pull(variant_id)\n",
    "\n",
    "        gwas_all_var <- if(any(grepl(\"chr\", qtl_all_var))) add_chr_prefix(gwas_all_var) else gsub(\"chr\", \"\", gwas_all_var)\n",
    "        gwas_all_top_loci$variant_id <- gwas_all_var\n",
    "        int_var <- gwas_all_top_loci$variant_id[check_flip(gwas_all_var, qtl_all_var)]\n",
    "        if(length(int_var) > 0){\n",
    "            gwas_all_top_loci <- gwas_all_top_loci %>% filter(check_flip(gwas_all_var, qtl_all_var))\n",
    "\n",
    "            all_top_loci <- rbind.fill(gwas_all_top_loci, qtl_all_top_loci)\n",
    "            fwrite(all_top_loci,  gsub('_gwas_batch_meta','_gwas_batch_export',${_output:r}))\n",
    "\n",
    "            new_gwas <- split(gwas_all_top_loci, gwas_all_top_loci$study)\n",
    "            new_gwas <- lapply(new_gwas, function(df) {\n",
    "              split(df, df$method)\n",
    "            })\n",
    "\n",
    "            qtl[[1]] <- c(qtl[[1]], new_gwas)\n",
    "            new_qtl_path <-  paste0(${_output:ddr},\"/\",gsub(\".rds\",\".overlapped.gwas.rds\",basename(qtl_file)))\n",
    "            saveRDS(qtl, new_qtl_path)\n",
    "  \n",
    "            block_top_loci = gwas_all_top_loci$region %>% unique %>% paste(., collapse = ',')\n",
    "            final_combined_data = new_qtl_path %>% basename\n",
    "        } else {block_top_loci = final_combined_data = NA} # fixme: could remove this judge if we get a solid enough meta\n",
    "      } else {block_top_loci = final_combined_data = NA} # fixme: could remove this judge if we get a solid enough meta\n",
    "    } else {\n",
    "        block_top_loci = final_combined_data = NA\n",
    "    }\n",
    " \n",
    "    qtl_meta <- suppressMessages(read_delim('${qtl_meta_path}'))\n",
    "    qtl_meta <- qtl_meta %>% filter(gene_id == '${_meta_info[3]}')  %>% mutate(block_top_loci = block_top_loci,\n",
    "                                                     final_combined_data = final_combined_data)\n",
    "    fwrite(qtl_meta, ${_output:r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886d6a0f-b09f-4b09-92ee-8fe8b9f1905e",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[overlap_qtl_gwas_2]\n",
    "input: group_by = 'all'\n",
    "output: f\"{cwd}/{name}.overlapped.gwas.tsv\"\n",
    "task: trunk_workers = 1, walltime = '1h', trunk_size = job_size, mem = '16G', cores = 1, tags = f'{_output:bn}'\n",
    "R: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    library(data.table)\n",
    "    exp_path <- ${_input[0]:adr}\n",
    "    meta_files <- c(${\",\".join(['\"%s\"' % x.absolute() for x in _input])})\n",
    "    exp_files <- list.files(exp_path, \"_gwas_batch_export\", full.names = T)\n",
    "    meta_list <- exp_list <- list()\n",
    "    meta_combined <- rbindlist(lapply(meta_files, fread), fill = TRUE)\n",
    "    exp_combined <- rbindlist(lapply(exp_files, fread), fill = TRUE)\n",
    "    fwrite(exp_combined, gsub(\"tsv\",\"export.csv.gz\",\"${_output}\"))\n",
    "    fwrite(meta_combined, \"${_output}\", sep = '\\t')\n",
    "  \n",
    "bash: expand = \"${ }\", container = container, stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', entrypoint=entrypoint\n",
    "    # rm -rf ${_input[0]:adr}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
