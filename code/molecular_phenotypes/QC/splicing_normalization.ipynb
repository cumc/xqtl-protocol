{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99798700-31d0-4ca9-851b-36d13e7b2111",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Normalization and phenotype table generation for splicingQTL analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed490dfc-e60f-4ba6-bffa-397fce023112",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Methods\n",
    "\n",
    "Leafcutter and psichomics are continued being used here, check /molecular_phenotyles/calling/splicing_calling.ipynb for details.\n",
    "\n",
    "In this part of pipeline, raw data from leafcutter and psichomics will be first convert to bed format. Then quality control is done by remove features with over some rate of NAs across samples (default 40%), replace NAs in the remaining samples by mean existed values, and then remove introns with less than a minimal variation (default 0.005). Quantile-Quantile Normalization is performed on the QC'd phenotype data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67a4151-077d-4180-8787-3f9a8ed5d0d4",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "The sample_list_intron_usage_perind.counts.gz file generated by previous splicing_calling.ipynb.\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "The psichomics_raw_data.tsv file generated by previous splicing_calling.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623d756-84c9-4eee-a9ec-29a8d28c1db4",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "`{sample_list}` below refers to the name of the meta-data file input for previous step.\n",
    "\n",
    "Main output include: \n",
    "\n",
    "`{sample_list}_intron_usage_perind.counts.gz_raw_data.qqnorm.txt` a merged table with normalized intron usage ratio for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "` a merged table with normalized intron usage ratio for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "\n",
    "```\n",
    "#Chr       start        end        ID                                                      samp1 samp2 samp3 ...\n",
    "chromosome intron_start intron_end {chr}:{intron_start}:{intron_end}:{cluster_id}_{strand} data  data  data  ...  \n",
    "```\n",
    "\n",
    "(the strand info in \"ID\" column is calculated strandness of junctions via retools in previous workflow)\n",
    "\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "Main output include: \n",
    "\n",
    "`psichomics_raw_data_bedded.qqnorm.txt` a merged table with normalized intron usage ratio for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "` a merged table with normalized percent spliced in (psi) value for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "\n",
    "```\n",
    "#Chr       start        end        ID                                               samp1 samp2 samp3 ...\n",
    "chromosome intron_start intron_end {event_type}_{chr}_{strand}_{coordinates}_{gene} data  data  data  ...  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c676f4d0-4fff-4219-98b0-61eb380f2c7a",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8335ded-bb76-41ea-9bb6-c9537c308539",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### For `leafcutter`\n",
    "Run files here [google drive](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R) with the workflow in /molecular_phenotyles/calling/splicing_calling.ipynb, the output file `sample_fastq_bam_list_intron_usage_perind.counts.gz` is the minimal working example input here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495049cb-7dd2-452f-ae51-1e03891e9ab3",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/splicing_normalization.ipynb leafcutter_norm \\\n",
    "    --cwd output/ \\\n",
    "    --ratios output/sample_list_intron_usage_perind.counts.gz \\\n",
    "    --container containers/leafcutter.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b57fa-ce67-46b8-983c-5fc2dc12799d",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### For `psichomics`\n",
    "Run files here [google drive](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R) with the workflow in /molecular_phenotyles/calling/splicing_calling.ipynb, the output file `psi_raw_data.tsv` is the minimal working example input here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed534ca8-47bb-40a1-82a0-48267fe436ef",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/splicing_normalization.ipynb psichomics_norm\\\n",
    "    --cwd output \\\n",
    "    --ratios input/psi_raw_data.tsv \\\n",
    "    --container containers/psichomics.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c15210-952d-486c-ac5b-0e949e7f8550",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abe3972-d1ba-4b45-bddf-aeff7c521ea0",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run splicing_normalization.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  leafcutter_norm\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --ratios VAL (as path, required)\n",
      "                        intron usage ratio file wiht samples after QC\n",
      "  --job-size 1 (as int)\n",
      "                        Raw data directory, default to the same directory as\n",
      "                        sample list parameter: data_dir = path(f\"{ratios:d}\")\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  leafcutter_norm_1:\n",
      "    Workflow Options:\n",
      "      --chr-blacklist  f'{ratios:dd}/black_list.txt'\n",
      "\n",
      "  leafcutter_norm_2:\n"
     ]
    }
   ],
   "source": [
    "sos run splicing_normalization.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb2e11-c05c-494d-8236-6d06bcae1002",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b4ac2d9-5d4c-4d21-8c24-0cff215a9e3b",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files. \n",
    "parameter: cwd = path(\"output\")\n",
    "# intron usage ratio file wiht samples after QC\n",
    "\n",
    "# optional parameter black list if user want to blacklist some chromosomes and not to analyze\n",
    "parameter: chr_blacklist = path(\".\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5923b09e-2dae-4aa6-9b85-ad2324674052",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## 0. Generate sample list \n",
    "The rna seq sample IDs are different from the wgs samples. And Hao provided a table for that, but I need to cut that list in a same length with my samples in sQTL analysis.\n",
    "\n",
    "### Step Inputs:\n",
    "\n",
    "* `ROSMAP_JointCall_sample_participant_lookup_fixed`: A sample comparison table for ROSMAP datasets. \n",
    "\n",
    "### Step Outputs:\n",
    "\n",
    "* `ROSMAP_JointCall_sample_participant_lookup_fixed.rnaseq`: A sample comparison table for ROSMAP datasets with same length as length with my samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a9066-62da-4654-ac4d-7a93f0e5c33f",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[Junc_list]\n",
    "parameter: junc_path = path\n",
    "parameter: leafcutter_version = \"leafcutter2\"\n",
    "parameter: dataset = \"ROSMAP_DLPFC\"\n",
    "output: f'{cwd}/{leafcutter_version}/{dataset}_intron_usage_perind.junc'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stderr'\n",
    "    cd ${cwd}\n",
    "    realpath  ${junc_path}/*junc > ${_output}.tmp\n",
    "    grep -v \"all\" ${_output}.tmp > ${_output}\n",
    "    rm ${_output}.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f5ac1d-ec53-4f8d-bdb2-441874b3bf3a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[Jointcall_samples]\n",
    "parameter: sample_table = path\n",
    "parameter: junc_list = path\n",
    "parameter: leafcutter_version = \"leafcutter2\"\n",
    "input: sample_table, junc_list\n",
    "output: f'{cwd}/{leafcutter_version}/{sample_table:b}.rnaseq'\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stderr'\n",
    "    library(data.table)\n",
    "    library(tidyverse)\n",
    "    sample_lookup_hao = read_delim(${_input[0]:r}, \"\\t\" ,col_names = T)\n",
    "    sample_lookup_hao$sample_id<-sample_lookup_hao$sample_id%>%gsub(\".final\",\"\",.)\n",
    "    sample_ids<-list.files(paste0(\"${cwd}\",\"/\",\"${leafcutter_version}\",\"/\"),\"junc$\")%>%stringr::str_split(.,\"[.]\",simplify=T)%>%.[,1]\n",
    "    junc_list<-read.table(${_input[1]:r})\n",
    "    sample_ids<-junc_list$V1%>%basename%>%stringr::str_split(.,\"[.]\",simplify=T)%>%.[,1]\n",
    "    sample_lookup_hao<-sample_lookup_hao[sample_lookup_hao$sample_id%in%sample_ids,]\n",
    "    write.table(sample_lookup_hao,${_output:r},quote = F,row.names = F,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844485e2-525a-42d1-8cfe-8d91a9b3b446",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "`sample_id` is the ID for RNAseq/bam file preffix, and `participant_id` is the ID for WGS/geno file preffix. Here I use the sample_lookup file from Hao, but I need to cut it to the same length as my files first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b2c2ff-60e8-4076-89a8-650e3664300d",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "##prepare sample list\n",
    "#echo \"sample_id\" > output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq1\n",
    "#cut -d / -f11 output/leafcutter2/ROSMAP_DLPFC_intron_usage_perind.junc |cut -d. -f1    >> output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq1\n",
    "#echo \"participant_id\" > output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq2\n",
    "#cut -d / -f11 output/leafcutter2/ROSMAP_DLPFC_intron_usage_perind.junc |cut -d. -f1    >> output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq2\n",
    "#\n",
    "#paste -d \"\\t\" output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq1 output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq2 > output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq\n",
    "#rm output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq1 output/leafcutter2/ROSMAP_DLPFC_sample.rnaseq2\n",
    "#\n",
    "##this is to make a sample list, but I'd like to use the one same as this [notebook](https://github.com/cumc/fungen-xqtl-analysis/blob/main/analysis/Zhang_BU/ROSMAP_DLPFC/sQTL/phenotype_preprocessing_sQTL.ipynb), which is generated by Hao\n",
    "##map the rnaseq id to wgs id\n",
    "#sample_lookup = read_delim(\"/mnt/vast/hpc/csg/rf2872/Work/leaf_cutter2/ROSMAP_DLPFC/ROSMAP_DLPFC_sample.rnaseq\", \"\\t\" ,col_names = T)\n",
    "#rosmap_index=read.csv(\"/mnt/mfs/ctcn/datasets/rosmap/rnaseq/dlpfcTissue/synapseReprocessing/experimentalDesign/ROSMAP_IDkey.csv\")\n",
    "#sample_lookup$participant_id<-rosmap_index$wgs_id[match(sample_lookup$participant_id,rosmap_index$rnaseq_id)]\n",
    "#sample_lookup$participant_id[is.na(sample_lookup$participant_id) | sample_lookup$participant_id == \"\"] <- sample_lookup$sample_id[is.na(sample_lookup$participant_id) | sample_lookup$participant_id == \"\"]\n",
    "#write.table(sample_lookup,\"/home/rf2872/Work/leaf_cutter2/ROSMAP_DLPFC/ROSMAP_DLPFC_sample.test\",quote = F,row.names = F,sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49226cb9-85e4-4c42-9a07-780b5287e524",
   "metadata": {
    "kernel": "R",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_lookup_hao = read_delim(\"/mnt/mfs/hgrcgrid/homes/hs3163/ROSMAP_JointCall_sample_participant_lookup_fixed\", \"\\t\" ,col_names = T)\n",
    "sample_lookup_hao$sample_id<-sample_lookup_hao$sample_id%>%gsub(\".final\",\"\",.)\n",
    "sample_ids<-list.files(\"~/data//data_ROSMAP//junc_files//leafcutter/\",\"junc$\")%>%stringr::str_split(.,\"[.]\",simplify=T)%>%.[,1]\n",
    "sample_lookup_hao<-sample_lookup_hao[sample_lookup_hao$sample_id%in%sample_ids,]\n",
    "write.table(sample_lookup_hao,\"/home/rf2872/Work/leaf_cutter2/ROSMAP_DLPFC/ROSMAP_JointCall_sample_participant_lookup_fixed.rnaseq\",quote = F,row.names = F,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d21bfe6-abcc-499e-aa96-01366df97b0f",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `leafcutter_norm`\n",
    "\n",
    "Documentation: [`leafcutter`](https://davidaknowles.github.io/leafcutter/index.html). The choices of regtool parameters are [discussed here](https://github.com/davidaknowles/leafcutter/issues/127).\n",
    "\n",
    "\n",
    "### Parameter Annotations\n",
    "\n",
    "* chr_blacklist: file of blacklisted chromosomes to exclude from analysis, one per line. If none is provided, will default blacklist nothing.\n",
    "\n",
    "### Things to keep in mind\n",
    "\n",
    "* Seems leafcutter_norm_1 requires ~ 10G memory (or larger if having large input) or there will be segmentation fault.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82ab7297-a49b-49c2-8cfa-05444082a851",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_norm_1]\n",
    "parameter: ratios = path\n",
    "import os\n",
    "if os.path.isfile(f'{ratios:dd}/black_list.txt'):\n",
    "    chr_blacklist = f'{ratios:dd}/black_list.txt'\n",
    "input: ratios, group_by = 'all'\n",
    "output: f'{ratios}_phenotype_file_list.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    # code in [leafcutter_norm_1] and [leafcutter_norm_3] is modified from \n",
    "    # https://github.com/davidaknowles/leafcutter/blob/master/scripts/prepare_phenotype_table.py\n",
    "    import sys\n",
    "    import gzip\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy as sc\n",
    "    import pickle\n",
    "\n",
    "    from sklearn import linear_model\n",
    "\n",
    "    def stream_table(f, ss = ''):\n",
    "        fc = '#'\n",
    "        while fc[0] == \"#\":\n",
    "            fc = f.readline().strip()\n",
    "            head = fc.split(ss)\n",
    "\n",
    "        for ln in f:\n",
    "            ln = ln.strip().split(ss)\n",
    "            attr = {}\n",
    "\n",
    "            for i in range(len(head)):\n",
    "                try: attr[head[i]] = ln[i]\n",
    "                except: break\n",
    "            yield attr\n",
    "\n",
    "    def get_chromosomes(ratio_file):\n",
    "        \"\"\"Get chromosomes from table. Returns set of chromosome names\"\"\"\n",
    "        try: open(ratio_file)\n",
    "        except:\n",
    "            sys.stderr.write(\"Can't find %s..exiting\\n\"%(ratio_file))\n",
    "            return\n",
    "        sys.stderr.write(\"Parsing chromosome names...\\n\")\n",
    "        chromosomes = set()\n",
    "        with gzip.open(ratio_file, 'rt') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    chromosomes.add(line.split(\":\")[0])\n",
    "        return(chromosomes)\n",
    "\n",
    "    def get_blacklist_chromosomes(chromosome_blacklist_file):\n",
    "        \"\"\"\n",
    "        Get list of chromosomes to ignore from a file with one blacklisted\n",
    "        chromosome per line. Returns list. eg. ['X', 'Y', 'MT']\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isfile(chromosome_blacklist_file):\n",
    "            with open(chromosome_blacklist_file, 'r') as f:\n",
    "                return(f.read().splitlines())\n",
    "        else:\n",
    "            return([])\n",
    "\n",
    "    def create_phenotype_table(ratio_file, chroms, blacklist_chroms):\n",
    "        dic_pop, fout = {}, {}\n",
    "        try: open(ratio_file)\n",
    "        except:\n",
    "            sys.stderr.write(\"Can't find %s..exiting\\n\"%(ratio_file))\n",
    "            return\n",
    "\n",
    "        sys.stderr.write(\"Starting...\\n\")\n",
    "        for i in chroms:\n",
    "            fout[i] = open(ratio_file+\".phen_\"+i, 'w')\n",
    "            fout_ave = open(ratio_file+\".ave\", 'w')\n",
    "        valRows, valRowsnn, geneRows = [], [], []\n",
    "        finished = False\n",
    "        header = gzip.open(ratio_file, 'rt').readline().split()[1:]\n",
    "\n",
    "        for i in fout:\n",
    "            fout[i].write(\"\\t\".join([\"#chr\",\"start\", \"end\", \"ID\"]+header)+'\\n')\n",
    "\n",
    "        for dic in stream_table(gzip.open(ratio_file, 'rt'),' '):\n",
    "\n",
    "            chrom = dic['chrom']\n",
    "            chr_ = chrom.split(\":\")[0]\n",
    "            if chr_ in blacklist_chroms: continue\n",
    "            NA_indices, aveReads = [], []\n",
    "            tmpvalRow = []\n",
    "\n",
    "            i = 0\n",
    "            for sample in header:\n",
    "\n",
    "                try: count = dic[sample]\n",
    "                except: print([chrom, len(dic)])\n",
    "                num, denom = count.split('/')\n",
    "                if float(denom) < 1:\n",
    "                    count = \"NA\"\n",
    "                    tmpvalRow.append(\"NA\")\n",
    "                    NA_indices.append(i)\n",
    "                else:\n",
    "                    # add a 0.5 pseudocount\n",
    "                    count = (float(num)+0.5)/((float(denom))+0.5)\n",
    "                    tmpvalRow.append(count)\n",
    "                    aveReads.append(count)\n",
    "\n",
    "            chr_, s, e, clu = chrom.split(\":\")\n",
    "            if len(tmpvalRow) > 0:\n",
    "                fout[chr_].write(\"\\t\".join([chr_,s,e,chrom]+[str(x) for x in tmpvalRow])+'\\n')\n",
    "                fout_ave.write(\" \".join([\"%s\"%chrom]+[str(min(aveReads)), str(max(aveReads)), str(np.mean(aveReads))])+'\\n')\n",
    "\n",
    "                valRows.append(tmpvalRow)\n",
    "                geneRows.append(\"\\t\".join([chr_,s,e,chrom]))\n",
    "                if len(geneRows) % 1000 == 0:\n",
    "                    sys.stderr.write(\"Parsed %s introns...\\n\"%len(geneRows))\n",
    "\n",
    "        for i in fout:\n",
    "            fout[i].close()\n",
    "\n",
    "        matrix = np.array(valRows)\n",
    "\n",
    "        # write the corrected tables\n",
    "\n",
    "        sample_names = []\n",
    "\n",
    "        for name in header:\n",
    "            sample_names.append(name.replace('.Aligned.sortedByCoord.out.md', ''))\n",
    "\n",
    "        fout = {}\n",
    "        for i in chroms:\n",
    "            fn=\"%s.qqnorm_%s\"%(ratio_file,i)\n",
    "            print(\"Outputting: \" + fn)\n",
    "            fout[i] = open(fn, 'w')\n",
    "            fout[i].write(\"\\t\".join(['#Chr','start','end','ID'] + sample_names)+'\\n')\n",
    "        lst = []\n",
    "        for i in range(len(matrix)):\n",
    "            chrom, s = geneRows[i].split()[:2]\n",
    "\n",
    "            lst.append((chrom, int(s), \"\\t\".join([geneRows[i]] + [str(x) for x in  matrix[i]])+'\\n'))\n",
    "\n",
    "        lst.sort()\n",
    "        for ln in lst:\n",
    "            fout[ln[0]].write(ln[2])\n",
    "\n",
    "        fout_run = open(\"%s_phenotype_file_list.txt\"%ratio_file, 'w')\n",
    "\n",
    "        fout_run.write(\"#chr\\t#dir\\n\")\n",
    "\n",
    "        for i in fout:\n",
    "            fout[i].close()\n",
    "            fout_run.write(\"%s\\t\"%(i))\n",
    "            fout_run.write(\"%s.qqnorm_%s\\n\"%(ratio_file, i))\n",
    "        fout_run.close()\n",
    "\n",
    "    ratio_file = f'${_input}'\n",
    "    chroms = get_chromosomes(f'${_input}')\n",
    "    blacklist_chroms = get_blacklist_chromosomes(f'${chr_blacklist}')\n",
    "\n",
    "    create_phenotype_table(ratio_file, chroms, blacklist_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83485f93-37ff-42d5-a461-2e6a183cb52e",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[leafcutter_norm_2]\n",
    "import pandas as pd\n",
    "molecular_pheno_chr_inv = pd.read_csv(f'{_input[0]}',sep = \"\\t\")\n",
    "molecular_pheno_chr_inv = molecular_pheno_chr_inv.values.tolist()\n",
    "file_inv = [x[1] for x in molecular_pheno_chr_inv]\n",
    "input: file_inv # This design is necessary to avoid using for_each, as sos can not take chr number as an input.\n",
    "output: f'{_input[0]:n}_raw_data.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    head -1 ${_input[0]:r}  > ${_output}\n",
    "    cat ${_input:r} | grep -v \"#Chr\" >> ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa8dcd0-a5cd-42bc-845e-7384587ebc8b",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[leafcutter_norm_3]\n",
    "# minimal NA rate with in sample values for a possible alternative splicing event to be kept (default 0.4, chosen according to leafcutter default na rate)\n",
    "parameter: na_rate = 0.4\n",
    "# minimal variance across samples for a possible alternative splicing event to be kept (default 0.001, chosen according to psichomics suggested minimal variance)\n",
    "parameter: min_variance = 0.001\n",
    "output: f'{_input:n}.qqnorm.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    from scipy.stats import rankdata\n",
    "\n",
    "    def qqnorm(x):\n",
    "        n=len(x)\n",
    "        a=3.0/8.0 if n<=10 else 0.5\n",
    "        return(norm.ppf( (rankdata(x)-a)/(n+1.0-2.0*a) ))\n",
    "\n",
    "    raw_df = pd.read_csv(f'${_input}',sep = \"\\t\")\n",
    "    valRows = raw_df.iloc[:,4:]\n",
    "    headers = list(raw_df)\n",
    "\n",
    "    drop_list = []\n",
    "    na_limit = len(valRows.columns)*${na_rate}\n",
    "\n",
    "    for index, row in valRows.iterrows():\n",
    "\n",
    "        # If ratio is missing for over 40% of the samples, drop\n",
    "        if (row.isna().sum()) > na_limit:\n",
    "            drop_list.append(index)\n",
    "        # Set missing values as the mean of existed values in a row\n",
    "        else:\n",
    "            row.fillna(row.mean())\n",
    "        # drop introns with variance smaller than some minimal value\n",
    "        if np.std(row) < ${min_variance}:\n",
    "            drop_list.append(index)\n",
    "\n",
    "    # save the intron information and sample values for remaining introns/rows\n",
    "    newtable = raw_df.drop(drop_list).iloc[:,0:4]\n",
    "    valRows = valRows.drop(drop_list)\n",
    "\n",
    "    # scale normalize on each row\n",
    "    valRows_matrix = []\n",
    "    for c in (valRows.values.tolist()):\n",
    "        c = preprocessing.scale(c)\n",
    "        valRows_matrix.append(c)\n",
    "    \n",
    "    # qqnorms on the columns\n",
    "    matrix = np.array(valRows_matrix)\n",
    "    for i in range(len(matrix[0,:])):\n",
    "        matrix[:,i] = qqnorm(matrix[:,i])\n",
    "    normalized_table = pd.DataFrame(matrix)\n",
    "\n",
    "    # reset row index for the saved intron infomation so the index will match sample values\n",
    "    newtable = newtable.reset_index(drop=True)\n",
    "    # merge the two parts of table\n",
    "    output = pd.concat([newtable, normalized_table], axis=1)\n",
    "    output.columns = headers\n",
    "\n",
    "    # write normalized table\n",
    "    output.to_csv(f'${_input:n}.qqnorm.txt', sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c853c4a9-5a2c-477b-a033-82bf077b04f6",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `psichomics_norm`\n",
    "\n",
    "Documentation: [`psichomics`](http://bioconductor.org/packages/release/bioc/html/psichomics.html)\n",
    "Consider retaining more information, the only QC on PSI values here are NA removal and a minimal variance filter, however, psichomics team suggested some further QC which can be checked [here](https://github.com/nuno-agostinho/psichomics/issues/450).\n",
    "For reference, default minimal variance in leafcutter QC is 0.005.\n",
    "\n",
    "Due to the difference of alternative splicing event type in psichomics outputs, we are not performing normalization on each sample's data (columns) but only on each event (rows).\n",
    "\n",
    "***TO DO:*** rename the .qqnorm. and test downstreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84fda8a-268c-4372-b673-bf99b2472e14",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[psichomics_norm_1]\n",
    "parameter: ratios = path\n",
    "input: ratios, group_by = 'all'\n",
    "output: f'{cwd}/psichomics_raw_data_bedded.txt' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    library(psichomics)\n",
    "    library(data.table)\n",
    "\n",
    "    psi_data <- as.matrix(fread(\"${_input}\"),rownames=1)\n",
    "    psi_data = as.data.frame(psi_data)\n",
    "  \n",
    "    # Process PSI df into bed file for tensorQTL. (This part of code is modified from Ryan Yordanoff's work)\n",
    "    parsed_events <- parseSplicingEvent(row.names(psi_data))\n",
    "    \n",
    "    # Create bedfile df and fill values with parsed values\n",
    "    bed_file <- data.frame(\"chr\"=parsed_events$chrom,\"start\"=parsed_events$start,\"end\"=parsed_events$end,\"ID\"=row.names(parsed_events),psi_data,check.names = FALSE)\n",
    "    names(bed_file)[1] <- \"#Chr\"\n",
    "    bed_file$'#Chr' <- sub(\"^\", \"chr\", bed_file$'#Chr')\n",
    "    row.names(bed_file) <- NULL   \n",
    "  \n",
    "    # Create BED file output\n",
    "    write.table(x=bed_file, file = \"${cwd}/psichomics_raw_data_bedded.txt\", quote = FALSE, row.names = FALSE, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5051a-f3df-431e-86c2-92edf60f3165",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[psichomics_norm_2]\n",
    "# minimal NA rate with in sample values for a possible alternative splicing event to be kept (default 0.4, chosen according to leafcutter default na rate)\n",
    "parameter: na_rate = 0.4\n",
    "# minimal variance across samples for a possible alternative splicing event to be kept (default 0.001, chosen according to psichomics suggested minimal variance)\n",
    "parameter: min_variance = 0.001\n",
    "output: f'{_input:n}.qqnorm.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    from scipy.stats import rankdata\n",
    "\n",
    "    raw_df = pd.read_csv(f'${_input}',sep = \"\\t\")\n",
    "    valRows = raw_df.iloc[:,4:]\n",
    "    headers = list(raw_df)\n",
    "\n",
    "    drop_list = []\n",
    "    na_limit = len(valRows.columns)*${na_rate}\n",
    "\n",
    "    for index, row in valRows.iterrows():\n",
    "\n",
    "        # If ratio is missing for over 40% of the samples, drop\n",
    "        if (row.isna().sum()) > na_limit:\n",
    "            drop_list.append(index)\n",
    "            # drop introns with variance smaller than some minimal value\n",
    "        elif np.std(row) < ${min_variance}:\n",
    "            drop_list.append(index)\n",
    "\n",
    "    \n",
    "    # OLD codeï¼ŒSaved temporarily for inspection\n",
    "    #for index, row in valRows.iterrows():\n",
    "\n",
    "        ## If ratio is missing for over 40% of the samples, drop\n",
    "        #if (row.isna().sum()) > na_limit:\n",
    "            #drop_list.append(index)\n",
    "        ## Set missing values as the mean of existed values in a row\n",
    "        #else:\n",
    "            #row.fillna(row.mean())\n",
    "        ## drop introns with variance smaller than some minimal value\n",
    "        #if np.std(row) < ${min_variance}:\n",
    "            #drop_list.append(index)\n",
    "\n",
    "    # save the intron information and sample values for remaining introns/rows\n",
    "    newtable = raw_df.drop(drop_list).iloc[:,0:4]\n",
    "    valRows = valRows.drop(drop_list)\n",
    "\n",
    "    valMeans = valRows.mean(axis=1)\n",
    "\n",
    "    # Mean imputation\n",
    "    for i, col in enumerate(valRows):\n",
    "        valRows.iloc[:, i] = valRows.iloc[:, i].fillna(valMeans)\n",
    "\n",
    "    # scale normalize on each row\n",
    "    valRows_matrix = []\n",
    "    for c in (valRows.values.tolist()):\n",
    "        c = preprocessing.scale(c)\n",
    "        valRows_matrix.append(c)\n",
    "    \n",
    "    # qqnorms on the columns\n",
    "    matrix = np.array(valRows_matrix)\n",
    "    normalized_table = pd.DataFrame(matrix)\n",
    "\n",
    "    # reset row index for the saved intron infomation so the index will match sample values\n",
    "    newtable = newtable.reset_index(drop=True)\n",
    "    # merge the two parts of table\n",
    "    output = pd.concat([newtable, normalized_table], axis=1)\n",
    "    output.columns = headers\n",
    "\n",
    "    # write normalized table, avoid scientific notation\n",
    "    output.to_csv(f'${_input:n}.qqnorm.txt', sep=\"\\t\", index=None, float_format='%.16f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     {
      "name": "ipython",
      "version": 3
     }
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
