{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binding-ottawa",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Quantifying alternative splicing from RNA-seq data\n",
    "\n",
    "This pipeline implements our pipeline to call alternative splicing events from RNA-seq data, using [`leafcutter`](https://www.nature.com/articles/s41588-017-0004-9) and [`psichomics`](https://academic.oup.com/nar/article/47/2/e7/5114259) to call the RNA-seq data from original `fastq.gz` data. It implements the GTEx pipeline for GTEx/TOPMed project. Please refer to [this page](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md) for detail. The choice of pipeline modules in this project is supported by internal (unpublished) benchmarks from GTEx group.\n",
    "\n",
    "**Various reference data needs to be prepared before using this workflow**. [Here we provide a module](https://cumc.github.io/xqtl-pipeline/code/data_preprocessing/reference_data.html) to download and prepare the reference data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-indie",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Methods overview\n",
    "\n",
    "There are many types of alternative splicing events. See [Wang et al (2008)](https://pubmed.ncbi.nlm.nih.gov/18978772/) For an illustration on different events. We will apply two methods to quantify alternative splicing:\n",
    "\n",
    "1. [`psichomics`](https://academic.oup.com/nar/article/47/2/e7/5114259) that quantifies each specific event. In particular the exon skipping event which is used also in GTEx sQTL analysis.\n",
    "2. [`leafcutter`](https://www.nature.com/articles/s41588-017-0004-9) to quantify the usage of alternatively excised introns. This collectively captures skipped exons, 5’ and 3’ alternative splice site usage and other complex events. The method was previously applied to ROSMAP data as part of the Brain xQTL version 2.0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-bathroom",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "\n",
    "The bam file can be generated by `the STAR_align` workflow from our RNA_calling.ipynb module. \n",
    "\n",
    "A meta-data file, white space delimited without header, containing 2 columns: sample ID, path to the BAM file:\n",
    "\n",
    "```\n",
    "sample_1 samp1.bam\n",
    "sample_2 samp2.bam\n",
    "sample_3 samp3.bam\n",
    "```\n",
    "\n",
    "All the BAM files should be available under specified folder (default assumes the same folder as where the meta-data file is).\n",
    "\n",
    "If intend to blacklist some chromosomes and not analyze it, add one text file named black_list.txt with one chromosome name per line in the same directory of the meta-data file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-costs",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "- `leafcutter`: {sample_list} below refers to the name of the meta-data file input, {sample_name} refers to the name of each input BAM file, and {chromosome_id} refers to chromosome names, ie 1,2,3,X,Y.\n",
    "\n",
    "Major output include: \n",
    "\n",
    "`{sample_list}_intron_usage_perind.counts.gz` file with row id in format: \"chromosome:intron_start:intron_end:cluster_id\", column labeled as input sample names and each type of intron usage ratio under each sample (i.e. #particular intron in a sample / #total introns classified in the same cluster in a sample) in each cells. \n",
    "\n",
    "`{sample_list}_intron_usage_perind_numers.counts.gz` file with the same row and column label but the count of each intron in each cells.\n",
    "\n",
    "`{sample_list}_intron_usage_perind.counts.gz.qqnorm_{chromosome_id}.gz` files, which is phenotype tables for QTL analysis.\n",
    "\n",
    "\n",
    "- `psichomics`: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-lighting",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-recruitment",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### For `leafcutter`\n",
    "A minimal working example is uploaded in the [google drive](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R?usp=sharing) , pleased be noted that these MWE will not produce actual analysis result. The data required to produce the intron percentage would have the size of ~25G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-craft",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/splicing_calling.ipynb leafcutter \\\n",
    "    --cwd output/ \\\n",
    "    --samples sample_bam.list \\\n",
    "    --container containers/leafcutter.sif "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-handbook",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "### For `psichomics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-stocks",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run splicing_calling.ipynb psichomics \\\n",
    "    --cwd output/rnaseq/splicing \\\n",
    "    --samples data/sample_bam.list \\\n",
    "    --data-dir data \\\n",
    "    --container container/splicing.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-member",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mineral-motorcycle",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run splicing_calling.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  leafcutter\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --samples VAL (as path, required)\n",
      "                        Sample meta data list\n",
      "  --data-dir  path(f\"{samples:d}\")\n",
      "\n",
      "                        Raw data directory, default to the same directory as\n",
      "                        sample list\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  leafcutter_1:\n",
      "    Workflow Options:\n",
      "      --min-anchor-len 8 (as int)\n",
      "      --min-split-reads 50 (as int)\n",
      "      --max-intron-len 500000 (as int)\n",
      "  leafcutter_2:\n",
      "    Workflow Options:\n",
      "      --min-split-reads 50 (as int)\n",
      "      --max-intron-len 500000 (as int)\n"
     ]
    }
   ],
   "source": [
    "sos run splicing_calling.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-ontario",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "australian-declaration",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files. \n",
    "parameter: cwd = path(\"output\")\n",
    "# Sample meta data list\n",
    "parameter: samples = path\n",
    "# Raw data directory, default to the same directory as sample list\n",
    "parameter: data_dir = path(f\"{samples:d}\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_samples(fn, dr):\n",
    "    import os\n",
    "    samples = [x.strip().split() for x in open(fn).readlines()]\n",
    "    names = []\n",
    "    files = []\n",
    "    for i, x in enumerate(samples):\n",
    "        if len(x)<2:\n",
    "            raise ValueError(f\"Line {i+1} of file {fn} must have 2 columns\")\n",
    "        names.append(x[0])\n",
    "        for y in x[1:]:\n",
    "            y = os.path.join(dr, y)\n",
    "            if not os.path.isfile(y):\n",
    "                raise ValueError(f\"File {y} does not exist\")\n",
    "            files.append(y)\n",
    "    if len(files) != len(set(files)):\n",
    "        raise ValueError(\"Duplicated files are found (but should not be allowed) in BAM file list\")\n",
    "    return names, files\n",
    "\n",
    "sample_id, bam = get_samples(samples, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-leisure",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `leafcutter`\n",
    "\n",
    "Documentation: [`leafcutter`](https://davidaknowles.github.io/leafcutter/index.html). The choices of regtool parameters are [discussed here](https://github.com/davidaknowles/leafcutter/issues/127).\n",
    "\n",
    "### Parameter Annotations\n",
    "\n",
    "* anchor_len: anchor length\n",
    "* min_intron_len: minimum intron length to be analyzed\n",
    "* max_intron_len: maximum intron length to be analyzed\n",
    "* strandness: strandness status, 0 for unstranded, 1 for first-strand/RF, 2 for second-strand/FR\n",
    "* min_split_reads: minimal split reads allows to form a cluster\n",
    "* npc: the number of PCs wanted to be calculated as covariates in QTL analysis\n",
    "* chr_blacklist: file of blacklisted chromosomes to exclude from analysis, one per line. If none is provided, will default to blacklisting X and Y. Use by adding --ChromosomeBlackList option for the prepare_phenotype_table.py command in step leafcutter_3, \n",
    "\n",
    "### Things to keep in mind\n",
    "\n",
    "* If .bam.bai index files of the .bam input are ready before using leafCutter, it can be placed in the same directory with input .bam files and the \"samtools index ${_input}\" line can be skipped.\n",
    "* need to be updated with function to flag rna strandness for each input sample seperately\n",
    "* Seems leafcutter_3 requires ~ 10G memory or there will be segmentation fault.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aerial-temperature",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_1]\n",
    "parameter: anchor_len = 8\n",
    "parameter: min_intron_len = 50\n",
    "parameter: max_intron_len = 500000\n",
    "parameter: strandness = 1\n",
    "input: bam, group_by = 1\n",
    "output: f'{cwd}/{_input:bn}.junc' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container\n",
    "    samtools index ${_input}\n",
    "    regtools junctions extract -a ${anchor_len} -m ${min_intron_len} -M ${max_intron_len} -s ${strandness} ${_input} -o ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "going-history",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_2]\n",
    "parameter: min_split_reads = 50\n",
    "parameter: max_intron_len = 500000\n",
    "input: group_by = 'all'\n",
    "output: f'{cwd}/{samples:bn}_intron_usage_perind.counts.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    for i in ${_input:r}; do\n",
    "    echo $i >> ${_output:nn}.junc ; done\n",
    "    python /opt/leafcutter/clustering/leafcutter_cluster_regtools.py -j ${_output:nn}.junc -o ${f'{_output:bnn}'.replace(\"_perind\",\"\")} -m ${min_split_reads} -l ${max_intron_len} -r ${cwd}\n",
    "    mv ${_output:nn}.junc ${_output:nn}.junc_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d0b8835-0cf7-4187-ab57-c3806eb3f847",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_3]\n",
    "parameter: npc = 10\n",
    "parameter: chr_blacklist = f'{samples:d}/black_list.txt'\n",
    "output: f'{cwd}/{samples:bn}_intron_usage_perind.counts.gz.PCs',\n",
    "        f'{cwd}/{samples:bn}_intron_usage_perind.counts.gz_prepare.sh',\n",
    "        f'{cwd}/sample_list_intron_usage_phenotype_list_by_chromosome.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    /opt/leafcutter/scripts/prepare_phenotype_table.py ${_input} -p ${npc} --ChromosomeBlackList ${chr_blacklist}\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    import pandas as pd\n",
    "    blk_lst = pd.read_csv(f'${chr_blacklist}', sep = \" \", header=None)\n",
    "    blk_lst = blk_lst.iloc[:,0].values.tolist()\n",
    "    df = pd.read_csv(f'${_output[1]}', sep=\" \")\n",
    "    df = df[df['bgzip'] == \"-p\"]\n",
    "    names = df.iloc[:,2].values.tolist()\n",
    "    chrs = []\n",
    "    for i in names:\n",
    "        i = i.split('_')[-1]\n",
    "        i = i.split('.')[0]\n",
    "        chrs.append(i)\n",
    "    zipped = list(zip(chrs, names))\n",
    "    dfout = pd.DataFrame(zipped, columns=['#chr','#dir'])\n",
    "    for i in blk_lst:\n",
    "        dfout = dfout[dfout['#chr'] != str(i)]\n",
    "    dfout.to_csv(f'${cwd}/sample_list_intron_usage_phenotype_list_by_chromosome.txt', sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55a48ae8-e09f-4fc7-8979-e2a1d427aada",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[leafcutter_4]\n",
    "molecular_pheno_chr_inv = pd.read_csv(f'{_input[2]}',sep = \"\\t\")\n",
    "molecular_pheno_chr_inv = molecular_pheno_chr_inv.values.tolist()\n",
    "file_inv = [x[1][:-3] for x in molecular_pheno_chr_inv]\n",
    "input: file_inv, group_by = 1 # This design is necessary to avoid using for_each, as sos can not take chr number as an input.\n",
    "output: f'{_input}.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container\n",
    "    bgzip -f ${_input}\n",
    "    tabix -p bed ${f'{_input}.gz'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-reception",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `psichomics`\n",
    "\n",
    "Documentation: [`psichomics`](http://bioconductor.org/packages/release/bioc/html/psichomics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd786285-cf65-4fe5-8536-98237de335f5",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     {
      "name": "ipython",
      "version": 3
     }
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.23.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
