{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "thousand-spice",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Reference data standardization\n",
    "\n",
    "This module provides reference data download, indexing and preprocessing (if necessary), in preparation for use throughout the pipeline.\n",
    "\n",
    "We have included the PDF document compiled by Data Standardization Working Group in the [on Synapse](https://www.synapse.org/#!Synapse:syn36416587) as well as on [ADSP Dashboard](https://www.niagads.org/adsp/content/adspgcadgenomeresources-v2pdf). It contains the reference data to use for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-sucking",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This module is based on the [TOPMed workflow from Broad](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md). The reference data after we process it (details see Methods section and the rest of the analysis) can be found [in this folder on Synapse](https://www.synapse.org/#!Synapse:syn36416587). \n",
    "\n",
    "### Reference files for RNA-seq expression quantification\n",
    "\n",
    "**We have decided to use these preprocessed reference files for RNA-seq expression and splicing quantification.** Specifically:\n",
    "\n",
    "1. `GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.{dict,fasta,fasta.fai}`\n",
    "2. `Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf` for stranded protocol, and `Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf` for unstranded protocol.\n",
    "3. Everything under `STAR_Index` folder\n",
    "4. Everything under `RSEM_Index` folder\n",
    "5. Optionally, for quality control, `gtf_ref.flat`\n",
    "\n",
    "### Reference files for Methylation\n",
    "\n",
    "FIXME @hao\n",
    "\n",
    "### Reference files for alternative splicing\n",
    "\n",
    "FIXME @xuanhe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-hearing",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Details\n",
    "\n",
    "Workflows implemented include:\n",
    "\n",
    "\n",
    "### Process reference fasta file\n",
    "\n",
    "\n",
    "- Input:  a reference fasta file.\n",
    "\n",
    "- Output: a reference fasta file without HLA, ALT, Decoy but with ERCC.\n",
    "\n",
    "### Convert transcript feature file gff3 to gtf\n",
    "\n",
    "- Input: an uncompressed gff3 file.(i.e. can be view via cat)\n",
    "- Output: a gtf file.\n",
    "\n",
    "### Convert transcript feature file gff3 to gtf\n",
    "\n",
    "- Input: an uncompressed gff3 file.(i.e. can be view via cat)\n",
    "- Output: a gtf file.\n",
    "\n",
    "### Collapse transcript features into genes\n",
    "\n",
    "- Input: a gtf file.\n",
    "- Output: a gtf file with collapesed gene model.\n",
    "\n",
    "### Generate STAR index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of STAR index.\n",
    "\n",
    "### Generate RSEM index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of RSEM index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-juvenile",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## List of commands to prepare reference data\n",
    "\n",
    "To download reference data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-prevention",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb download_hg_reference --cwd reference_data\n",
    "sos run pipeline/reference_data.ipynb download_gene_annotation --cwd reference_data\n",
    "sos run pipeline/reference_data.ipynb download_ercc_reference --cwd reference_data\n",
    "sos run pipeline/reference_data.ipynb download_dbsnp --cwd reference_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-boston",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "- Approximate time: an hour, depending on the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooperative-range",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format reference data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-perry",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb hg_reference \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-reference reference_data/ERCC92.fa \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.fa \\\n",
    "    --container container/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-rover",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb hg_gtf \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.gtf \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy.fasta \\\n",
    "    --container containers/rna_quantification.sif --stranded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-murray",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format gene feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-special",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb gene_annotation \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-gtf reference_data/ERCC92.gtf \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.gtf \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy.fasta \\\n",
    "    --container containers/rna_quantification.sif --stranded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qualified-durham",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "- Approximate time: 10min \n",
    "- Memory: 16GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-civilization",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Notice that for un-stranded RNA-seq protocol please use switch `--no-stranded` to the command above instead of `--stranded`. More details can be found later in the document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-equity",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Generating STAR index without the GTF annotation file allow customize read lenght lateron in STAR alignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-shopping",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb STAR_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --container containers/rna_quantification.sif \\\n",
    "    --mem 40G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-management",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "- Approximate time: 30 min\n",
    "- Memory: 40 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-regression",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate RSEM index with the gtf file **prior** to the gene collapsing step ( **without** the gene tag in its file name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-insider",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb RSEM_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container containers/rna_quantification.sif  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-productivity",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "- Approximate time: 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-carry",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate RefFlat annotation for Picard QC,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-excerpt",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb RefFlat_generation \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container containers/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behind-motorcycle",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate the SUPPA annotation for psichomics to detect RNA alternative splicing events,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-flexibility",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb SUPPA_annotation \\\n",
    "    --hg_gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container containers/psichomics.sif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-framework",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Approximate time: ? min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc48676e-2c08-462e-8499-447457e3d153",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step extract the rsID for the known varriants, so that we can distinguish them from the novel variants we had in our study. The procedure/rationale is [explained in this post](https://hbctraining.github.io/In-depth-NGS-Data-Analysis-Course/sessionVI/lessons/03_annotation-snpeff.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d0aec-f983-4915-ac69-412dd666cf86",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run pipeline/VCF_QC.ipynb dbsnp_annotate \\\n",
    "    --genoFile reference_data/00-All.vcf.gz \\\n",
    "    --cwd output/reference_data \\\n",
    "    --container containers/bioinfo.sif \\\n",
    "    -J 50 -c csg.yml -q csg --add_chr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-empire",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fancy-tamil",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run reference_data.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  download_hg_reference\n",
      "  download_gene_annotation\n",
      "  download_ercc_reference\n",
      "  download_dbsnp\n",
      "  gff3_to_gtf\n",
      "  hg_reference\n",
      "  hg_gtf\n",
      "  ercc_gtf\n",
      "  gene_annotation\n",
      "  STAR_index\n",
      "  RSEM_index\n",
      "  RefFlat_generation\n",
      "  SUPPA_annotation\n",
      "  psi_hg38_annotation_modification\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  download_hg_reference:\n",
      "  download_gene_annotation:\n",
      "  download_ercc_reference:\n",
      "  download_dbsnp:\n",
      "  gff3_to_gtf:\n",
      "    Workflow Options:\n",
      "      --gff3-file VAL (as path, required)\n",
      "  hg_reference_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "                        Path to HG reference file\n",
      "  hg_reference_2:\n",
      "    Workflow Options:\n",
      "      --ercc-reference VAL (as path, required)\n",
      "  hg_reference_3:\n",
      "  hg_gtf_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "      --hg-gtf VAL (as path, required)\n",
      "  hg_gtf_2:\n",
      "    Workflow Options:\n",
      "      --[no-]stranded (required)\n",
      "  ercc_gtf:\n",
      "    Workflow Options:\n",
      "      --ercc-gtf VAL (as path, required)\n",
      "  gene_annotation:\n",
      "  STAR_index:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "  RSEM_index:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hg-reference VAL (as path, required)\n",
      "  RefFlat_generation:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "  SUPPA_annotation_1:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "  SUPPA_annotation_2:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "  psi_hg38_annotation_modification_1:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hgrc-db VAL (as path, required)\n"
     ]
    }
   ],
   "source": [
    "sos run reference_data.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-vietnamese",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files.\n",
    "parameter: cwd = path(\"output\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "cwd = path(f'{cwd:a}')\n",
    "from sos.utils import expand_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-general",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-machinery",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_hg_reference]\n",
    "output: f\"{cwd:a}/GRCh38_full_analysis_set_plus_decoy_hla.fa\"\n",
    "download: dest_dir = cwd\n",
    "    ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-worth",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_gene_annotation]\n",
    "output: f\"{cwd:a}/Homo_sapiens.GRCh38.103.chr.gtf\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    http://ftp.ensembl.org/pub/release-103/gtf/homo_sapiens/Homo_sapiens.GRCh38.103.chr.gtf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-refund",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_ercc_reference]\n",
    "output: f\"{cwd:a}/ERCC92.gtf\", f\"{cwd:a}/ERCC92.fa\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    https://tools.thermofisher.com/content/sfs/manuals/ERCC92.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-bathroom",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_dbsnp]\n",
    "output: f\"{cwd:a}/00-All.vcf.gz\", f\"{cwd:a}/00-All.vcf.gz.tbi\"\n",
    "download: dest_dir = cwd\n",
    "    ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz\n",
    "    ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz.tbi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-grave",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## GFF3 to GTF formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-soviet",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gff3_to_gtf]\n",
    "parameter: gff3_file = path\n",
    "input: gff3_file\n",
    "output: f'{cwd}/{_input:bn}.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "        gffread ${_input} -T -o ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-junction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## HG reference file preprocessing\n",
    "1. Remove the HLA/ALT/Decoy record from the fasta -- because none of the downstreams RNA-seq calling pipeline component can handle them properly.\n",
    "2. Adding in ERCC information to the fasta file -- even if ERCC is not included in the RNA-seq library it does not harm to add them.\n",
    "3. Generating index for the fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-injury",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_1 (HLA ALT Decoy removal)]\n",
    "# Path to HG reference file\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference\n",
    "output:  f'{cwd}/{_input:bn}.noALT_noHLA_noDecoy.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}', 'r') as fasta:\n",
    "        contigs = fasta.read()\n",
    "        contigs = contigs.split('>')\n",
    "        contig_ids = [i.split(' ', 1)[0] for i in contigs]\n",
    "\n",
    "        # exclude ALT, HLA and decoy contigs\n",
    "        filtered_fasta = '>'.join([c for i,c in zip(contig_ids, contigs)\n",
    "        if not (i[-4:]=='_alt' or i[:3]=='HLA' or i[-6:]=='_decoy')])\n",
    "    \n",
    "    with open('${_output}', 'w') as fasta:\n",
    "        fasta.write(filtered_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-breed",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_2 (merge with ERCC reference)]\n",
    "parameter: ercc_reference = path\n",
    "output: f'{cwd}/{_input:bn}_ERCC.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    sed 's/ERCC-/ERCC_/g' ${ercc_reference} > ${ercc_reference:n}.patched.fa\n",
    "    cat ${_input} ${ercc_reference:n}.patched.fa > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-clone",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[hg_reference_3 (index the fasta file)]\n",
    "output: f'{cwd}/{_input:bn}.dict'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    samtools faidx ${_input}\n",
    "    java -jar /opt/picard-tools/picard.jar \\\n",
    "        CreateSequenceDictionary \\\n",
    "        R=${_input} \\\n",
    "        O=${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-anaheim",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Transcript and gene model reference processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-dinner",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step modify the `gtf` file for following reasons:\n",
    "\n",
    "1. RSEM require GTF input to have the same chromosome name format (with `chr` prefix) as the fasta file. **Although for STAR, this problem can be solved by the `--sjdbGTFchrPrefix \"chr\"` option, we have to add `chr` to it for use with RSEM anyways.** That is why we would like to do it here. \n",
    "2. Gene model collapsing script `collapse_annotation.py` from GTEx require the gtf have `transcript_type` instead `transcript_biotype` in its annotation. We rename it here, although **this problem can also be solved by modifying the collapse_annotation.py while building the docker, since we are already customizing the reference file for 1 above, we add this in as well, as another customization made.**\n",
    "3. Adding in ERCC information to the `gtf` reference.\n",
    "\n",
    "We may reimplement 1 and 2 with the alternative approaches discussed above if the problem with RSEM is solved, or when RSEM is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-candidate",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_1 (add chr prefix to gtf file)]\n",
    "parameter: hg_reference = path\n",
    "parameter: hg_gtf = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f'{cwd}/{_input[1]:bn}.reformatted.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    options(scipen = 999)\n",
    "    con <- file(\"${_input[0]}\",\"r\")\n",
    "    fasta <- readLines(con,n=1)\n",
    "    close(con)\n",
    "    gtf = read_delim(\"${_input[1]}\", \"\\t\",  col_names  = F, comment = \"#\", col_types=\"ccccccccc\")\n",
    "    if(!str_detect(fasta,\">chr\")) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = str_remove_all(X1,\"chr\"))\n",
    "    } else if (!any(str_detect(gtf$X1[1],\"chr\"))) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = paste0(\"chr\",X1))\n",
    "    } else (gtf_mod = gtf)\n",
    "    if(any(str_detect(gtf_mod$X9, \"transcript_biotype\"))) {\n",
    "      gtf_mod = gtf_mod%>%mutate(X9 = str_replace_all(X9,\"transcript_biotype\",\"transcript_type\"))\n",
    "    }\n",
    "    gtf_mod%>%write.table(\"${_output}\",sep = \"\\t\",quote = FALSE,col.names = F,row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elementary-impact",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**Text below is taken from https://github.com/broadinstitute/gtex-pipeline/tree/master/gene_model**\n",
    "\n",
    "\n",
    "Gene-level expression and eQTLs from the GTEx project are calculated based on a collapsed gene model (i.e., combining all isoforms of a gene into a single transcript), according to the following rules:\n",
    "\n",
    "1. Transcripts annotated as “retained_intron” or “read_through” are excluded. Additionally, transcripts that overlap with annotated read-through transcripts may be blacklisted (blacklists for GENCODE v19, 24 & 25 are provided in this repository; no transcripts were blacklisted for v26).\n",
    "2. The union of all exon intervals of each gene is calculated.\n",
    "3. Overlapping intervals between genes are excluded from all genes.\n",
    "\n",
    "\n",
    "The purpose of step 3 is primarily to exclude overlapping regions from genes annotated on both strands, which can't be unambiguously quantified from unstranded RNA-seq (GTEx samples were sequenced using an unstranded protocol). For stranded protocols, this step can be skipped by adding the `--collapse_only` flag.\n",
    "\n",
    "Further documentation is available on the [GTEx Portal](https://gtexportal.org/home/documentationPage#staticTextAnalysisMethods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-prairie",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_2 (collapsed gene model)]\n",
    "parameter: stranded = bool\n",
    "output: f'{_input:n}{\".collapse_only\" if stranded else \"\"}.gene.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    collapse_annotation.py ${\"--collapse_only\" if stranded else \"\"} ${_input} ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-friendship",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ercc_gtf (Preprocess ERCC gtf file)]\n",
    "parameter: ercc_gtf = path\n",
    "input: ercc_gtf\n",
    "output: f'{cwd}/{_input:bn}.genes.patched.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}') as exon_gtf, open('${_output}', 'w') as gene_gtf:\n",
    "        for line in exon_gtf:\n",
    "            f = line.strip().split('\\t')\n",
    "            f[0] = f[0].replace('-','_')  # required for RNA-SeQC/GATK (no '-' in contig name)\n",
    "        \n",
    "            attr = f[8]\n",
    "            if attr[-1]==';':\n",
    "                attr = attr[:-1]\n",
    "            attr = dict([i.split(' ') for i in attr.replace('\"','').split('; ')])\n",
    "            # add gene_name, gene_type\n",
    "            attr['gene_name'] = attr['gene_id']\n",
    "            attr['gene_type'] = 'ercc_control'\n",
    "            attr['gene_status'] = 'KNOWN'\n",
    "            attr['level'] = 2\n",
    "            for k in ['id', 'type', 'name', 'status']:\n",
    "                attr['transcript_'+k] = attr['gene_'+k]\n",
    "        \n",
    "            attr_str = []\n",
    "            for k in ['gene_id', 'transcript_id', 'gene_type', 'gene_status', 'gene_name',\n",
    "                'transcript_type', 'transcript_status', 'transcript_name']:\n",
    "                attr_str.append('{0:s} \"{1:s}\";'.format(k, attr[k]))\n",
    "            attr_str.append('{0:s} {1:d};'.format('level', attr['level']))\n",
    "            f[8] = ' '.join(attr_str)\n",
    "        \n",
    "            # write gene, transcript, exon\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['gene']+f[3:])+'\\n')\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['transcript']+f[3:])+'\\n')\n",
    "            f[8] = ' '.join(attr_str[:2])\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['exon']+f[3:])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-picnic",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gene_annotation]\n",
    "input: output_from(\"hg_gtf_1\"), output_from(\"hg_gtf_2\"), output_from(\"ercc_gtf\")\n",
    "output: f'{cwd}/{_input[0]:bn}.ERCC.gtf', f'{cwd}/{_input[1]:bn}.ERCC.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container\n",
    "    cat ${_input[0]} ${_input[2]} > ${_output[0]}\n",
    "    cat ${_input[1]} ${_input[2]} > ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-nigeria",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Generating index file for `STAR` \n",
    "\n",
    "This step generate the index file for STAR alignment. This file just need to generate once and can be re-used. \n",
    "\n",
    "**At least 40GB of memory is needed**.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. Both of them needs to be unzipped. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads. We use 100 here as recommended by the TOPMed pipeline. See here for [some additional discussions](https://groups.google.com/g/rna-star/c/h9oh10UlvhI/m/BfSPGivUHmsJ). \n",
    "\n",
    "### Step Output\n",
    "\n",
    "* Indexing file stored in `{cwd}/STAR_index`, which will be used by `STAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-share",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_index]\n",
    "parameter: hg_reference = path\n",
    "# Specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "# Default choice follows from TOPMed pipeline recommendation.\n",
    "if expand_size(mem) < expand_size('40G'):\n",
    "    print(\"Insufficent memory for STAR, changing to 40G\")\n",
    "    star_mem = '40G'\n",
    "else:\n",
    "    star_mem = mem\n",
    "input: hg_reference\n",
    "output: f\"{cwd}/STAR_Index/chrName.txt\", \n",
    "        f\"{cwd}/STAR_Index/SAindex\", f\"{cwd}/STAR_Index/SA\", f\"{cwd}/STAR_Index/genomeParameters.txt\", \n",
    "        f\"{cwd}/STAR_Index/chrStart.txt\",\n",
    "        f\"{cwd}/STAR_Index/chrLength.txt\", \n",
    "        f\"{cwd}/STAR_Index/Genome\", f\"{cwd}/STAR_Index/chrNameLength.txt\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[1]:n}.stderr', stdout = f'{_output[1]:n}.stdout'\n",
    "    STAR --runMode genomeGenerate \\\n",
    "         --genomeDir ${_output[0]:d} \\\n",
    "         --genomeFastaFiles ${_input[0]} \\\n",
    "         --runThreadN ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-password",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Generating index file for `RSEM`\n",
    "\n",
    "This step generate the indexing file for `RSEM`. This file just need to generate once.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "\n",
    "### Step Outputs\n",
    "* Indexing file stored in `RSEM_index_dir`, which will be used by `RSEM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-cycling",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[RSEM_index]\n",
    "parameter: hg_gtf = path\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f\"{cwd}/RSEM_Index/rsem_reference.n2g.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.grp\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.ti\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.chrlist\", f\"{cwd}/RSEM_Index/rsem_reference.seq\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.transcripts.fa\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[1]:n}.stderr', stdout = f'{_output[1]:n}.stdout'\n",
    "    rsem-prepare-reference \\\n",
    "            ${_input[0]} \\\n",
    "            ${_output[1]:n} \\\n",
    "            --gtf ${_input[1]} \\\n",
    "            --num-threads ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-registration",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Generation of RefFlat file \n",
    "This file is needed for picard CollectRnaSeqMetrics module, which in turn \n",
    ">produces metrics describing the distribution of the bases within the transcripts. It calculates the total numbers and the fractions of nucleotides within specific genomic regions including untranslated regions (UTRs), introns, intergenic sequences (between discrete genes), and peptide-coding sequences (exons). This tool also determines the numbers of bases that pass quality filters that are specific to Illumina data (PF_BASES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-routine",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[RefFlat_generation]\n",
    "parameter: hg_gtf = path\n",
    "input: hg_gtf\n",
    "output: f'{_input:n}.ref.flat'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    gtfToGenePred ${_input}  ${_output}.tmp -genePredExt -geneNameAsName2\n",
    "    awk -F'\\t' -v OFS=\"\\t\" '{$1=$12 OFS $1;}7' ${_output}.tmp | cut -f 1-11 > ${_output}\n",
    "    rm ${_output}.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stable-punch",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Generation of SUPPA annotation for psichomics.\n",
    "The generation of custom alternative splicing annotation is based on [this tutorial](https://rpubs.com/nuno-agostinho/preparing-AS-annotation). The way to generate the local alternative splicing suppa output is documented on [SUPPA github page](https://github.com/comprna/SUPPA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-luther",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb SUPPA_annotation \\\n",
    "    --hg_gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container containers/psochimics.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-ottawa",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SUPPA_annotation_1]\n",
    "parameter: hg_gtf = path\n",
    "input: hg_gtf\n",
    "output: f'{cwd}/hg38.{_input:bn}_SE_strict.ioe' # The stderr file must not shared the same start with the output file\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{cwd}/{_input:bn}.stderr', stdout = f'{cwd}/{_input:bn}.stdout'\n",
    "    python /opt/SUPPA/suppa.py generateEvents -i ${_input} -o ${cwd}/hg38.${_input:bn} -f ioe -e SE SS MX RI FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-costs",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SUPPA_annotation_2]\n",
    "parameter: hg_gtf = path\n",
    "output: f'{cwd}/{hg_gtf:bn}.SUPPA_annotation.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    library(\"psichomics\")\n",
    "    suppa <- parseSuppaAnnotation(\"${_input:d}\", genome=\"hg38\") \n",
    "    annot <- prepareAnnotationFromEvents(suppa)\n",
    "    saveRDS(annot, file=${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "korean-bread",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Generate psichomics Hg38 splicing annotation\n",
    "\n",
    "Since the original annotation provided by psichomics package is using gene symbols, we modified it to use Ensembl IDs. The modified annotation will be used for [detecting RNA alternative splicing using psichomics](https://cumc.github.io/xqtl-pipeline/code/molecular_phenotypes/calling/splicing_calling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accredited-disposal",
   "metadata": {
    "kernel": "Bash",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb psi_hg38_annotation \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformated.gtf \\\n",
    "    --hgrc-db reference_data/hgnc_database.txt \\\n",
    "    --container container/psichomics.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-maldives",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[psi_hg38_annotation]\n",
    "parameter: hg_gtf = path\n",
    "# FIXME: Please document what this file is and where do we get it @xuanhe.\n",
    "parameter: hgrc_db = path\n",
    "input: hg_gtf, hgrc_db\n",
    "output: f'{cwd}/psichomics_hg38_annotation.rds'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    library(\"psichomics\")\n",
    "    library(\"purrr\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"data.table\")\n",
    "  \n",
    "    # load psicomics default annotation, option of hg38 from listSplicingAnnotations()\n",
    "    annotation <- loadAnnotation(\"AH63657\")\n",
    "\n",
    "    \n",
    "    # reduce the demension of annotation file\n",
    "    annotation <- \n",
    "      map(annotation, ~.x%>%\n",
    "                       tidyr::unnest(cols = `Gene`))\n",
    "  \n",
    "    # Create empty colomns for each event for easier mapping\n",
    "    annotation[[\"Tandem UTR\"]][[\"SUPPA.Event.ID\"]] <- NA\n",
    "    annotation[[\"Tandem UTR\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "    annotation[[\"Alternative first exon\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "    annotation[[\"Alternative last exon\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "    annotation[[\"Mutually exclusive exon\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "  \n",
    "    # extract Ensembl ID substring from original SUPPA.ID and VASTTOOL.ID\n",
    "    annotation <- \n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(ENSG.SUPPA = substr(`SUPPA.Event.ID`, 1, 15))%>%\n",
    "                       mutate(ENSG.VAST = substr(`VAST-TOOLS.Event.ID`, 1, 15)))\n",
    "  \n",
    "    # Load gtf file\n",
    "    gtf_sample <- read.table('${_input[0]}',header = FALSE, sep = '\\t')\n",
    "  \n",
    "    # from the gtf file, seperate gene names and corresponding Ensembl ID\n",
    "    gtf_sample <- separate(gtf_sample, V9, sep = \";\",into = c(\"gene_id\", \"transcript_id\", \"exon_number\", \"gene_name\"))\n",
    "    gtf_sample <- separate(gtf_sample, gene_id, sep = \" \",into = c(\"gene_id\", \"gene_id_val\"))\n",
    "    gtf_sample <- separate(gtf_sample, gene_name, sep = \"e \",into = c(\"gene_name\", \"gene_name_val\"))\n",
    "  \n",
    "    gtf_name_id_match <- gtf_sample[,c(\"gene_id_val\",\"gene_name_val\")]\n",
    "    gtf_name_id_match <- gtf_name_id_match[!duplicated(gtf_name_id_match), ]\n",
    "  \n",
    "    # For any matched approved id in the psi hg38 annotation and gtf file, record the corresponding Ensembl ID\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(`ENSG.GTF` = gtf_name_id_match$gene_id_val[match(`Gene`, gtf_name_id_match$gene_name_val)]))\n",
    "  \n",
    "    # load hgnc database\n",
    "    hgnc_db <- fread('${_input[1]}', fill = TRUE, header = TRUE, sep = '\\t', quote=\"\")\n",
    "  \n",
    "    # Combine the `Ensembl.ID.supplied.by.Ensembl.` and `Ensembl.gene.ID` column, if there are any conflict use the former\n",
    "    # For conflict ones (15 total) both the former and latter records are poiting to the same gene name in Ensembl website so the order should not matter\n",
    "    hgnc_db <- hgnc_db %>%\n",
    "    mutate(ENSG.ID = ifelse(`Ensembl ID(supplied by Ensembl)` == \"\", `Ensembl gene ID`, `Ensembl ID(supplied by Ensembl)`))\n",
    "  \n",
    "    # Create a one to one reference list for approved names, previous names and aliases\n",
    "    # There is no duplicate symbol and Ensembl id info for approved symbol so no need for chromosome verification\n",
    "    hgnc_name_id_match <- hgnc_db[,c(\"Approved symbol\",\"ENSG.ID\")]\n",
    "    hgnc_name_prev_check <- hgnc_db[,c(\"Previous symbols\",\"Chromosome\",\"ENSG.ID\")]\n",
    "    hgnc_name_alias_check <- hgnc_db[,c(\"Alias symbols\",\"Chromosome\",\"ENSG.ID\")]\n",
    "  \n",
    "    # Remove NAs\n",
    "    hgnc_name_prev_check <- hgnc_name_prev_check[hgnc_name_prev_check$ENSG.ID != \"\",]\n",
    "    hgnc_name_alias_check <- hgnc_name_alias_check[hgnc_name_alias_check$ENSG.ID != \"\",]\n",
    "\n",
    "    hgnc_name_prev_check <- hgnc_name_prev_check[hgnc_name_prev_check$\"Previous symbols\" != \"\",] \n",
    "    hgnc_name_alias_check <- hgnc_name_alias_check[hgnc_name_alias_check$\"Alias symbols\" != \"\",]\n",
    "\n",
    "    # Seperate symbol column values from list of sybols to individual rows with one each\n",
    "    hgnc_name_prev_check <- separate_rows(hgnc_name_prev_check, \"Previous symbols\", convert = FALSE)\n",
    "    hgnc_name_alias_check <- separate_rows(hgnc_name_alias_check, \"Alias symbols\", convert = FALSE)\n",
    "  \n",
    "    # Convert chomosome info in hgnc database to number for matching with other database\n",
    "    hgnc_name_prev_check <- separate(hgnc_name_prev_check, \"Chromosome\", sep = 'p', into = \"Chrp\", remove = FALSE)\n",
    "    hgnc_name_prev_check <- separate(hgnc_name_prev_check, \"Chromosome\", sep = 'q', into = \"Chrq\", remove = FALSE)\n",
    "    hgnc_name_prev_check <- hgnc_name_prev_check%>%\n",
    "                                mutate(Chr = ifelse(nchar(hgnc_name_prev_check$Chrp) <= 2, Chrp, Chrq))\n",
    "    \n",
    "    hgnc_name_alias_check <- separate(hgnc_name_alias_check, \"Chromosome\", sep = 'p', into = \"Chrp\", remove = FALSE)\n",
    "    hgnc_name_alias_check <- separate(hgnc_name_alias_check, \"Chromosome\", sep = 'q', into = \"Chrq\", remove = FALSE)\n",
    "    hgnc_name_alias_check <- hgnc_name_alias_check%>%\n",
    "                                mutate(Chr = ifelse(nchar(hgnc_name_alias_check$Chrp) <= 2, Chrp, Chrq))\n",
    "  \n",
    "    # For any matched approved id in the psi hg38 annotation and hgnc database, record the corresponding Ensembl ID\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(`ENSG.HGNC` = hgnc_name_id_match$`ENSG.ID`[match(`Gene`, hgnc_name_id_match$\"Approved symbol\")]))\n",
    "  \n",
    "    # Drop hypothetical genes\n",
    "    annotation<-\n",
    "      map(annotation, ~.x%>%\n",
    "            subset(`Gene` != 'Hypothetical'))\n",
    "  \n",
    "    # IN remaining NAs, for any matched alias/previous names and chromosome in the psi hg38 annotation and hgnc database, record the corresponding Ensembl ID\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                      mutate(ENSG.HGNC = ifelse(is.na(`ENSG.HGNC`) | `ENSG.HGNC` == \"\",\n",
    "                                                hgnc_name_alias_check$`ENSG.ID`[match(`Gene`, hgnc_name_alias_check$\"Alias symbols\") & match(`Chromosome`, hgnc_name_alias_check$Chr)],\n",
    "                                                `ENSG.HGNC`))%>%\n",
    "                      mutate(ENSG.HGNC = ifelse(is.na(`ENSG.HGNC`) | `ENSG.HGNC` == \"\",\n",
    "                                                hgnc_name_prev_check$`ENSG.ID`[match(`Gene`, hgnc_name_prev_check$\"Previous symbols\") & match(`Chromosome`, hgnc_name_prev_check$Chr)],\n",
    "                                                `ENSG.HGNC`)))\n",
    "  \n",
    "    # Build the final Ensembl id column base on the gtf file first, then for remaining NAs check the HGNC database record, SUPPA and VASTTOOL record,\n",
    "    # Drop special cases that Ensembl ID is not recorded in VASTTOOLs and SUPPA\n",
    "    # finnally in the remaining NA Ensmbl ID if the gene name in original annotation is NCBI IDs just use it\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(`ENSG.ID` = `ENSG.GTF`)%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`),\n",
    "                                               `ENSG.HGNC`,\n",
    "                                                `ENSG.ID`))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`),\n",
    "                                               `ENSG.VAST`,\n",
    "                                                `ENSG.ID`))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`),\n",
    "                                               `ENSG.SUPPA`,\n",
    "                                                `ENSG.ID`))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(substr(`ENSG.ID`, 1, 4) == 'ENSG',\n",
    "                                                 `ENSG.ID`,\n",
    "                                                 NA))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`) & substr(`Gene`, 1, 3) == 'LOC',\n",
    "                                               `Gene`,\n",
    "                                                `ENSG.ID`)))\n",
    "\n",
    "    # Use the Ensembl IDs to replace gene names, drop remaining NAs\n",
    "    annotation <-\n",
    "    map(annotation, ~.x%>%\n",
    "                       mutate(`Gene` = `ENSG.ID`)%>%\n",
    "            drop_na(`Gene`)\n",
    "          )\n",
    "\n",
    "    # save modified annotation\n",
    "    saveRDS(annotation, file = \"${_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e1bfc8-d691-47f8-80b3-3ed008c5e093",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Generate TAD associated index\n",
    "We use https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7895846/ to define the genotype cis region for fine mapping with different phenotypes. The actual TAD dataframe for each cell types and tissues are download from the hg38 TAD link from  http://3dgenome.fsm.northwestern.edu/publications.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77f00e1-c02a-4977-a1dd-36e4095a8ab7",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hs3163/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/hs3163/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/home/hs3163/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3441: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "[Tad_annotateion]\n",
    "## The tad file downloads from \n",
    "parameter: TAD_list = path\n",
    "parameter: region_list = path\n",
    "parameter: TAD_genotype = path\n",
    "input: TAD_list, region_list\n",
    "output:f'{cwd}/{_input:n}.{region_list:n}.annotation' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    tad = pd.read_csv(\"${_input[0]}\",\"\\t\",header = None)\n",
    "    tad.columns = [\"#chr\",\"start\",\"end\"]\n",
    "    tad[\"tad_index\"] = [f'tad{x}' for x in np.arange(1,len(tad)+1)]\n",
    "    region_list =  pd.read_csv(\"${_input[1]}\",\"\\t\")\n",
    "    tad = tad.merge(region_list, on = \"#chr\").query(\"start_y > start_x &  end_y < end_x\").drop([\"start_y\",\"end_y\"] , axis = 1).rename({\"start_x\":\"start\",\"end_x\",\"end\" })\n",
    "    tad.to_csv(\"${_output}\",\"\\t\") \n",
    "    tad_geno_list = pd.read_csv(\"${_TAD_genotype}\",\"\\t\")\n",
    "    tad_gene_id = tad.merge(tad_geno_list,left_on = \"tad_index\",right_on = \"#id\").iloc[:,[4,6]]\n",
    "    tad_gene_id.columns = [\"ID\",\"dir\"]\n",
    "    tad_gene_id.to_csv(\"${_output}.genotype_files_list.tsv\",\"\\t\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
