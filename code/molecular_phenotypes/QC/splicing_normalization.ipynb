{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "painted-things",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Normalization and phenotype table generation for splicingQTL analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ce7cfe-ab2f-45f8-99e2-b3508d1cf8fb",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e869467-6d24-46ce-8fd2-453270988f0e",
   "metadata": {},
   "source": [
    "Quality control and normalization are performed on output from the leafcutter and psichomics tools. The raw output data is first converted to bed format. Quality control involves the removal of features with high missingness across samples (default 40%) and the replacement of NA values in the remaining samples with mean existing values. Then introns with less than a minimal variation (default of 0.005) are removed from the data. Quantile-Quantile normalization is performed on the quality controlled data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-conclusion",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Input\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "The sample_list_intron_usage_perind.counts.gz file generated by previous splicing_calling.ipynb.\n",
    "\n",
    "Minimal working example input may be found at [google drive](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R), specifically the  `sample_fastq_bam_list_intron_usage_perind.counts.gz` file.\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "The psichomics_raw_data.tsv file generated by previous splicing_calling.ipynb.\n",
    "\n",
    "Minimal working example input may be found at [google drive](https://drive.google.com/drive/folders/1lpcx3eKG2UpauntLUuJ6bMBjHyIhWW_R), specifically the  `psi_raw_data.tsv` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-court",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Output\n",
    "\n",
    "### `leafcutter`\n",
    "\n",
    "`{sample_list}` below refers to the name of the meta-data file input for previous step.\n",
    "\n",
    "Main output include: \n",
    "\n",
    "`{sample_list}_intron_usage_perind.counts.gz_raw_data.qqnorm.txt` a merged table with normalized intron usage ratio for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "` a merged table with normalized intron usage ratio for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "\n",
    "```\n",
    "#Chr       start        end        ID                                                      samp1 samp2 samp3 ...\n",
    "chromosome intron_start intron_end {chr}:{intron_start}:{intron_end}:{cluster_id}_{strand} data  data  data  ...  \n",
    "```\n",
    "\n",
    "(the strand info in \"ID\" column is calculated strandness of junctions via retools in previous workflow)\n",
    "\n",
    "\n",
    "### `psichomics`\n",
    "\n",
    "Main output include: \n",
    "\n",
    "`psichomics_raw_data_bedded.qqnorm.txt` a merged table with normalized intron usage ratio for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "` a merged table with normalized percent spliced in (psi) value for each sample, ready to be phenotype input for tensorQTL in following format:\n",
    "\n",
    "```\n",
    "#Chr       start        end        ID                                               samp1 samp2 samp3 ...\n",
    "chromosome intron_start intron_end {event_type}_{chr}_{strand}_{coordinates}_{gene} data  data  data  ...  \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-directory",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal Working Example Steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e0a72b-3458-4ac7-b751-a76fd0c1ed79",
   "metadata": {},
   "source": [
    "### ii. Splicing QC and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda79648-94fd-4cbc-b150-deb4b48a68f0",
   "metadata": {},
   "source": [
    "#### a. Leafcutter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd65e8f-922d-4ef8-8e95-b16f2da8cf5b",
   "metadata": {},
   "source": [
    "Timing:  ~30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778773e4-89b3-4a02-9706-c2c36d6983e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mleafcutter_norm_1\u001b[0m: \n",
      "INFO: t7dd380a023cbb83f \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t7dd380a023cbb83f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2490147 (\"job_t7dd380a023cbb83f\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mleafcutter_norm_1\u001b[0m output:   \u001b[32m../../output_test/leafcutter/PCC_sample_list_subset_leafcutter_intron_usage_perind.counts.gz_phenotype_file_list.txt\u001b[0m\n",
      "INFO: Running \u001b[32mleafcutter_norm_2\u001b[0m: \n",
      "INFO: tfd09cc5fe382fc6f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2490152 (\"job_tfd09cc5fe382fc6f\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mleafcutter_norm_2\u001b[0m output:   \u001b[32m../../output_test/leafcutter/PCC_sample_list_subset_leafcutter_intron_usage_perind.counts.gz_raw_data.txt\u001b[0m\n",
      "INFO: Running \u001b[32mleafcutter_norm_3\u001b[0m: \n",
      "INFO: t74cfa835e5b5e1e5 \u001b[32mre-execute completed\u001b[0m\n",
      "INFO: t74cfa835e5b5e1e5 \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2490196 (\"job_t74cfa835e5b5e1e5\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mleafcutter_norm_3\u001b[0m output:   \u001b[32m../../output_test/leafcutter/PCC_sample_list_subset_leafcutter_intron_usage_perind.counts.gz_raw_data.qqnorm.txt\u001b[0m\n",
      "INFO: Workflow leafcutter_norm (ID=w2657ca618afceb73) is executed successfully with 3 completed steps and 3 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "!sos run splicing_normalization.ipynb leafcutter_norm \\\n",
    "    --cwd ../../output_test/leafcutter/normalize \\\n",
    "    --ratios ../../output_test/leafcutter/PCC_sample_list_subset_leafcutter_intron_usage_perind.counts.gz \\\n",
    "    --container oras://ghcr.io/cumc/leafcutter_apptainer:latest \\\n",
    "    -s force -c ../csg.yml -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a728731-74db-4a5f-ac33-11b94357e9db",
   "metadata": {},
   "source": [
    "#### a. Psichomics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c4b3d8-ebaf-4373-a640-2e9c3b1e2973",
   "metadata": {},
   "source": [
    "Timing: ~20min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "distant-platform",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Running \u001b[32mpsichomics_norm_1\u001b[0m: \n",
      "INFO: tbc563f768bb4313d \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2490541 (\"job_tbc563f768bb4313d\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mpsichomics_norm_1\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/psichomics/normalize/psichomics_raw_data_bedded.txt\u001b[0m\n",
      "INFO: Running \u001b[32mpsichomics_norm_2\u001b[0m: \n",
      "INFO: te6284c5142d74a6f \u001b[32msubmitted\u001b[0m to neurology with job id Your job 2490565 (\"job_te6284c5142d74a6f\") has been submitted\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: Waiting for the completion of \u001b[32m1\u001b[0m task.\n",
      "INFO: \u001b[32mpsichomics_norm_2\u001b[0m output:   \u001b[32m/restricted/projectnb/xqtl/xqtl_protocol/output_test/psichomics/normalize/psichomics_raw_data_bedded.qqnorm.txt\u001b[0m\n",
      "INFO: Workflow psichomics_norm (ID=w2fbf4cd3c12ccfef) is executed successfully with 2 completed steps and 2 completed tasks.\n"
     ]
    }
   ],
   "source": [
    "!sos run splicing_normalization.ipynb psichomics_norm \\\n",
    "    --cwd ../../output_test/psichomics/normalize \\\n",
    "    --ratios ../../output_test/psichomics/psi_raw_data.tsv \\\n",
    "    --container oras://ghcr.io/cumc/psichomics_apptainer:latest \\\n",
    "    -c ../csg.yml -q neurology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e70e9-8645-4f82-ab7b-e6e10a9d2303",
   "metadata": {},
   "source": [
    "## Troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21a875-7451-4f2c-9910-adaf2c9eba98",
   "metadata": {},
   "source": [
    "| Step | Substep | Problem | Possible Reason | Solution |\n",
    "|------|---------|---------|------------------|---------|\n",
    "|  |  |  |  |  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sorted-wheel",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "senior-light",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run splicing_normalization.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  leafcutter_norm\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        The output directory for generated files.\n",
      "  --ratios VAL (as path, required)\n",
      "                        intron usage ratio file wiht samples after QC\n",
      "  --job-size 1 (as int)\n",
      "                        Raw data directory, default to the same directory as\n",
      "                        sample list parameter: data_dir = path(f\"{ratios:d}\")\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  leafcutter_norm_1:\n",
      "    Workflow Options:\n",
      "      --chr-blacklist  f'{ratios:dd}/black_list.txt'\n",
      "\n",
      "  leafcutter_norm_2:\n"
     ]
    }
   ],
   "source": [
    "sos run splicing_normalization.ipynb -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-roulette",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Setup and global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "laden-detail",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files. \n",
    "parameter: cwd = path(\"output\")\n",
    "# intron usage ratio file wiht samples after QC\n",
    "# optional parameter black list if user want to blacklist some chromosomes and not to analyze\n",
    "parameter: chr_blacklist = path(\".\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "import re\n",
    "parameter: entrypoint= ('micromamba run -a \"\" -n' + ' ' + re.sub(r'(_apptainer:latest|_docker:latest|\\.sif)$', '', container.split('/')[-1])) if container else \"\"\n",
    "from sos.utils import expand_size\n",
    "cwd = path(f'{cwd:a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-western",
   "metadata": {
    "kernel": "SoS",
    "toc-hr-collapsed": true
   },
   "source": [
    "## 0. Generate sample list \n",
    "The rna seq sample IDs are different from the wgs samples. And Hao provided a table for that, but I need to cut that list in a same length with my samples in sQTL analysis.\n",
    "\n",
    "### Step Inputs:\n",
    "\n",
    "* `ROSMAP_JointCall_sample_participant_lookup_fixed`: A sample comparison table for ROSMAP datasets. \n",
    "\n",
    "### Step Outputs:\n",
    "\n",
    "* `ROSMAP_JointCall_sample_participant_lookup_fixed.rnaseq`: A sample comparison table for ROSMAP datasets with same length as length with my samples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-leonard",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[Junc_list]\n",
    "parameter: junc_path = path\n",
    "parameter: file_suffix = \"junc\"\n",
    "parameter: leafcutter_version = \"leafcutter2\"\n",
    "parameter: dataset = \"ROSMAP_DLPFC\"\n",
    "output: f'{cwd}/{leafcutter_version}/{dataset}_intron_usage_perind.junc'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stderr' \n",
    "    cd ${cwd}\n",
    "    realpath  ${junc_path}/*${file_suffix} > ${_output}.tmp\n",
    "    grep -v \"all\" ${_output}.tmp > ${_output}\n",
    "    rm ${_output}.tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-major",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[Jointcall_samples]\n",
    "parameter: sample_table = path\n",
    "parameter: junc_list = path\n",
    "parameter: leafcutter_version = \"leafcutter2\"\n",
    "input: sample_table, junc_list\n",
    "output: f'{cwd}/{leafcutter_version}/{sample_table:b}.rnaseq'\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stderr' \n",
    "    library(data.table)\n",
    "    library(tidyverse)\n",
    "    sample_lookup_hao = read_delim(${_input[0]:r}, \"\\t\" ,col_names = T)\n",
    "    sample_lookup_hao$sample_id<-sample_lookup_hao$sample_id%>%gsub(\".final\",\"\",.)\n",
    "    sample_ids<-list.files(paste0(\"${cwd}\",\"/\",\"${leafcutter_version}\",\"/\"),\"junc$\")%>%stringr::str_split(.,\"[.]\",simplify=T)%>%.[,1]\n",
    "    junc_list<-read.table(${_input[1]:r})\n",
    "    sample_ids<-junc_list$V1%>%basename%>%stringr::str_split(.,\"[.]\",simplify=T)%>%.[,1]\n",
    "    sample_lookup_hao<-sample_lookup_hao[sample_lookup_hao$sample_id%in%sample_ids,]\n",
    "    write.table(sample_lookup_hao,${_output:r},quote = F,row.names = F,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-raleigh",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `leafcutter_norm`\n",
    "\n",
    "Documentation: [`leafcutter`](https://davidaknowles.github.io/leafcutter/index.html). The choices of regtool parameters are [discussed here](https://github.com/davidaknowles/leafcutter/issues/127).\n",
    "\n",
    "\n",
    "### Parameter Annotations\n",
    "\n",
    "* chr_blacklist: file of blacklisted chromosomes to exclude from analysis, one per line. If none is provided, will default blacklist nothing.\n",
    "\n",
    "### Things to keep in mind\n",
    "\n",
    "* Seems leafcutter_norm_1 requires ~ 10G memory (or larger if having large input) or there will be segmentation fault.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adjusted-advertising",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[leafcutter_norm_1]\n",
    "parameter: ratios = path\n",
    "import os\n",
    "if os.path.isfile(f'{ratios:dd}/black_list.txt'):\n",
    "    chr_blacklist = f'{ratios:dd}/black_list.txt'\n",
    "input: ratios, group_by = 'all'\n",
    "output: f'{ratios}_phenotype_file_list.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    # code in [leafcutter_norm_1] and [leafcutter_norm_3] is modified from \n",
    "    # https://github.com/davidaknowles/leafcutter/blob/master/scripts/prepare_phenotype_table.py\n",
    "    import sys\n",
    "    import gzip\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import scipy as sc\n",
    "    import pickle\n",
    "\n",
    "    from sklearn import linear_model\n",
    "\n",
    "    def stream_table(f, ss = ''):\n",
    "        fc = '#'\n",
    "        while fc[0] == \"#\":\n",
    "            fc = f.readline().strip()\n",
    "            head = fc.split(ss)\n",
    "\n",
    "        for ln in f:\n",
    "            ln = ln.strip().split(ss)\n",
    "            attr = {}\n",
    "\n",
    "            for i in range(len(head)):\n",
    "                try: attr[head[i]] = ln[i]\n",
    "                except: break\n",
    "            yield attr\n",
    "\n",
    "    def get_chromosomes(ratio_file):\n",
    "        \"\"\"Get chromosomes from table. Returns set of chromosome names\"\"\"\n",
    "        try: open(ratio_file)\n",
    "        except:\n",
    "            sys.stderr.write(\"Can't find %s..exiting\\n\"%(ratio_file))\n",
    "            return\n",
    "        sys.stderr.write(\"Parsing chromosome names...\\n\")\n",
    "        chromosomes = set()\n",
    "        with gzip.open(ratio_file, 'rt') as f:\n",
    "                f.readline()\n",
    "                for line in f:\n",
    "                    chromosomes.add(line.split(\":\")[0])\n",
    "        return(chromosomes)\n",
    "\n",
    "    def get_blacklist_chromosomes(chromosome_blacklist_file):\n",
    "        \"\"\"\n",
    "        Get list of chromosomes to ignore from a file with one blacklisted\n",
    "        chromosome per line. Returns list. eg. ['X', 'Y', 'MT']\n",
    "        \"\"\"\n",
    "\n",
    "        if os.path.isfile(chromosome_blacklist_file):\n",
    "            with open(chromosome_blacklist_file, 'r') as f:\n",
    "                return(f.read().splitlines())\n",
    "        else:\n",
    "            return([])\n",
    "\n",
    "    def create_phenotype_table(ratio_file, chroms, blacklist_chroms):\n",
    "        dic_pop, fout = {}, {}\n",
    "        try: open(ratio_file)\n",
    "        except:\n",
    "            sys.stderr.write(\"Can't find %s..exiting\\n\"%(ratio_file))\n",
    "            return\n",
    "\n",
    "        sys.stderr.write(\"Starting...\\n\")\n",
    "        for i in chroms:\n",
    "            fout[i] = open(ratio_file+\".phen_\"+i, 'w')\n",
    "            fout_ave = open(ratio_file+\".ave\", 'w')\n",
    "        valRows, valRowsnn, geneRows = [], [], []\n",
    "        finished = False\n",
    "        header = gzip.open(ratio_file, 'rt').readline().split()[1:]\n",
    "\n",
    "        for i in fout:\n",
    "            fout[i].write(\"\\t\".join([\"#chr\",\"start\", \"end\", \"ID\"]+header)+'\\n')\n",
    "\n",
    "        for dic in stream_table(gzip.open(ratio_file, 'rt'),' '):\n",
    "\n",
    "            chrom = dic['chrom']\n",
    "            chr_ = chrom.split(\":\")[0]\n",
    "            if chr_ in blacklist_chroms: continue\n",
    "            NA_indices, aveReads = [], []\n",
    "            tmpvalRow = []\n",
    "\n",
    "            i = 0\n",
    "            for sample in header:\n",
    "\n",
    "                try: count = dic[sample]\n",
    "                except: print([chrom, len(dic)])\n",
    "                num, denom = count.split('/')\n",
    "                if float(denom) < 1:\n",
    "                    count = \"NA\"\n",
    "                    tmpvalRow.append(\"NA\")\n",
    "                    NA_indices.append(i)\n",
    "                else:\n",
    "                    # add a 0.5 pseudocount\n",
    "                    count = (float(num)+0.5)/((float(denom))+0.5)\n",
    "                    tmpvalRow.append(count)\n",
    "                    aveReads.append(count)\n",
    "\n",
    "            chr_, s, e, clu = chrom.split(\":\")\n",
    "            if len(tmpvalRow) > 0:\n",
    "                fout[chr_].write(\"\\t\".join([chr_,s,e,chrom]+[str(x) for x in tmpvalRow])+'\\n')\n",
    "                fout_ave.write(\" \".join([\"%s\"%chrom]+[str(min(aveReads)), str(max(aveReads)), str(np.mean(aveReads))])+'\\n')\n",
    "\n",
    "                valRows.append(tmpvalRow)\n",
    "                geneRows.append(\"\\t\".join([chr_,s,e,chrom]))\n",
    "                if len(geneRows) % 1000 == 0:\n",
    "                    sys.stderr.write(\"Parsed %s introns...\\n\"%len(geneRows))\n",
    "\n",
    "        for i in fout:\n",
    "            fout[i].close()\n",
    "\n",
    "        matrix = np.array(valRows)\n",
    "\n",
    "        # write the corrected tables\n",
    "\n",
    "        sample_names = []\n",
    "\n",
    "        for name in header:\n",
    "            sample_names.append(name.replace('.Aligned.sortedByCoord.out.md', ''))\n",
    "\n",
    "        fout = {}\n",
    "        for i in chroms:\n",
    "            fn=\"%s.qqnorm_%s\"%(ratio_file,i)\n",
    "            print(\"Outputting: \" + fn)\n",
    "            fout[i] = open(fn, 'w')\n",
    "            fout[i].write(\"\\t\".join(['#Chr','start','end','ID'] + sample_names)+'\\n')\n",
    "        lst = []\n",
    "        for i in range(len(matrix)):\n",
    "            chrom, s = geneRows[i].split()[:2]\n",
    "\n",
    "            lst.append((chrom, int(s), \"\\t\".join([geneRows[i]] + [str(x) for x in  matrix[i]])+'\\n'))\n",
    "\n",
    "        lst.sort()\n",
    "        for ln in lst:\n",
    "            fout[ln[0]].write(ln[2])\n",
    "\n",
    "        fout_run = open(\"%s_phenotype_file_list.txt\"%ratio_file, 'w')\n",
    "\n",
    "        fout_run.write(\"#chr\\t#dir\\n\")\n",
    "\n",
    "        for i in fout:\n",
    "            fout[i].close()\n",
    "            fout_run.write(\"%s\\t\"%(i))\n",
    "            fout_run.write(\"%s.qqnorm_%s\\n\"%(ratio_file, i))\n",
    "        fout_run.close()\n",
    "\n",
    "    ratio_file = f'${_input}'\n",
    "    chroms = get_chromosomes(f'${_input}')\n",
    "    blacklist_chroms = get_blacklist_chromosomes(f'${chr_blacklist}')\n",
    "\n",
    "    create_phenotype_table(ratio_file, chroms, blacklist_chroms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "disciplinary-proposition",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[leafcutter_norm_2]\n",
    "import pandas as pd\n",
    "molecular_pheno_chr_inv = pd.read_csv(f'{_input[0]}',sep = \"\\t\")\n",
    "molecular_pheno_chr_inv = molecular_pheno_chr_inv.values.tolist()\n",
    "file_inv = [x[1] for x in molecular_pheno_chr_inv]\n",
    "input: file_inv # This design is necessary to avoid using for_each, as sos can not take chr number as an input.\n",
    "output: f'{_input[0]:n}_raw_data.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "bash: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    head -1 ${_input[0]:r}  > ${_output}\n",
    "    cat ${_input:r} | grep -v \"#Chr\" >> ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-trade",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[leafcutter_norm_3]\n",
    "# minimal NA rate with in sample values for a possible alternative splicing event to be kept (default 0.4, chosen according to leafcutter default na rate)\n",
    "parameter: na_rate = 0.4\n",
    "# minimal variance across samples for a possible alternative splicing event to be kept (default 0.001, chosen according to psichomics suggested minimal variance)\n",
    "parameter: min_variance = 0.001\n",
    "output: f'{_input:n}.qqnorm.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    from scipy.stats import rankdata\n",
    "\n",
    "    def qqnorm(x):\n",
    "        n=len(x)\n",
    "        a=3.0/8.0 if n<=10 else 0.5\n",
    "        return(norm.ppf( (rankdata(x)-a)/(n+1.0-2.0*a) ))\n",
    "\n",
    "    raw_df = pd.read_csv(f'${_input}',sep = \"\\t\")\n",
    "    valRows = raw_df.iloc[:,4:]\n",
    "    headers = list(raw_df)\n",
    "\n",
    "    drop_list = []\n",
    "    na_limit = len(valRows.columns)*${na_rate}\n",
    "\n",
    "    for index, row in valRows.iterrows():\n",
    "\n",
    "        # If ratio is missing for over 40% of the samples, drop\n",
    "        if (row.isna().sum()) > na_limit:\n",
    "            drop_list.append(index)\n",
    "        # Set missing values as the mean of existed values in a row\n",
    "        else:\n",
    "            row.fillna(row.mean())\n",
    "        # drop introns with variance smaller than some minimal value\n",
    "        if np.std(row) < ${min_variance}:\n",
    "            drop_list.append(index)\n",
    "\n",
    "    # save the intron information and sample values for remaining introns/rows\n",
    "    newtable = raw_df.drop(drop_list).iloc[:,0:4]\n",
    "    valRows = valRows.drop(drop_list)\n",
    "\n",
    "    # scale normalize on each row\n",
    "    valRows_matrix = []\n",
    "    for c in (valRows.values.tolist()):\n",
    "        c = preprocessing.scale(c)\n",
    "        valRows_matrix.append(c)\n",
    "    \n",
    "    # qqnorms on the columns\n",
    "    matrix = np.array(valRows_matrix)\n",
    "    for i in range(len(matrix[0,:])):\n",
    "        matrix[:,i] = qqnorm(matrix[:,i])\n",
    "    normalized_table = pd.DataFrame(matrix)\n",
    "\n",
    "    # reset row index for the saved intron infomation so the index will match sample values\n",
    "    newtable = newtable.reset_index(drop=True)\n",
    "    # merge the two parts of table\n",
    "    output = pd.concat([newtable, normalized_table], axis=1)\n",
    "    output.columns = headers\n",
    "\n",
    "    # write normalized table\n",
    "    output.to_csv(f'${_input:n}.qqnorm.txt', sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-fifth",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## `psichomics_norm`\n",
    "\n",
    "Documentation: [`psichomics`](http://bioconductor.org/packages/release/bioc/html/psichomics.html)\n",
    "Consider retaining more information, the only QC on PSI values here are NA removal and a minimal variance filter, however, psichomics team suggested some further QC which can be checked [here](https://github.com/nuno-agostinho/psichomics/issues/450).\n",
    "For reference, default minimal variance in leafcutter QC is 0.005.\n",
    "\n",
    "Due to the difference of alternative splicing event type in psichomics outputs, we are not performing normalization on each sample's data (columns) but only on each event (rows).\n",
    "\n",
    "***TO DO:*** rename the .qqnorm. and test downstreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-defense",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[psichomics_norm_1]\n",
    "parameter: ratios = path\n",
    "input: ratios, group_by = 'all'\n",
    "output: f'{cwd}/psichomics_raw_data_bedded.txt' \n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "R: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    library(psichomics)\n",
    "    library(data.table)\n",
    "\n",
    "    psi_data <- as.matrix(fread(\"${_input}\"),rownames=1)\n",
    "    psi_data = as.data.frame(psi_data)\n",
    "  \n",
    "    # Process PSI df into bed file for tensorQTL. (This part of code is modified from Ryan Yordanoff's work)\n",
    "    parsed_events <- parseSplicingEvent(row.names(psi_data))\n",
    "    \n",
    "    # Create bedfile df and fill values with parsed values\n",
    "    bed_file <- data.frame(\"chr\"=parsed_events$chrom,\"start\"=parsed_events$start,\"end\"=parsed_events$end,\"ID\"=row.names(parsed_events),psi_data,check.names = FALSE)\n",
    "    names(bed_file)[1] <- \"#Chr\"\n",
    "    bed_file$'#Chr' <- sub(\"^\", \"chr\", bed_file$'#Chr')\n",
    "    row.names(bed_file) <- NULL   \n",
    "  \n",
    "    # Create BED file output\n",
    "    write.table(x=bed_file, file = \"${cwd}/psichomics_raw_data_bedded.txt\", quote = FALSE, row.names = FALSE, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-deadline",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[psichomics_norm_2]\n",
    "# minimal NA rate with in sample values for a possible alternative splicing event to be kept (default 0.4, chosen according to leafcutter default na rate)\n",
    "parameter: na_rate = 0.4\n",
    "# minimal variance across samples for a possible alternative splicing event to be kept (default 0.001, chosen according to psichomics suggested minimal variance)\n",
    "parameter: min_variance = 0.001\n",
    "output: f'{_input:n}.qqnorm.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, cores = numThreads\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container=container, entrypoint = entrypoint\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    from scipy.stats import norm\n",
    "    from scipy.stats import rankdata\n",
    "\n",
    "    raw_df = pd.read_csv(f'${_input}',sep = \"\\t\")\n",
    "    valRows = raw_df.iloc[:,4:]\n",
    "    headers = list(raw_df)\n",
    "\n",
    "    drop_list = []\n",
    "    na_limit = len(valRows.columns)*${na_rate}\n",
    "\n",
    "    for index, row in valRows.iterrows():\n",
    "\n",
    "        # If ratio is missing for over 40% of the samples, drop\n",
    "        if (row.isna().sum()) > na_limit:\n",
    "            drop_list.append(index)\n",
    "            # drop introns with variance smaller than some minimal value\n",
    "        elif np.std(row) < ${min_variance}:\n",
    "            drop_list.append(index)\n",
    "\n",
    "    \n",
    "    # OLD codeï¼ŒSaved temporarily for inspection\n",
    "    #for index, row in valRows.iterrows():\n",
    "\n",
    "        ## If ratio is missing for over 40% of the samples, drop\n",
    "        #if (row.isna().sum()) > na_limit:\n",
    "            #drop_list.append(index)\n",
    "        ## Set missing values as the mean of existed values in a row\n",
    "        #else:\n",
    "            #row.fillna(row.mean())\n",
    "        ## drop introns with variance smaller than some minimal value\n",
    "        #if np.std(row) < ${min_variance}:\n",
    "            #drop_list.append(index)\n",
    "\n",
    "    # save the intron information and sample values for remaining introns/rows\n",
    "    newtable = raw_df.drop(drop_list).iloc[:,0:4]\n",
    "    valRows = valRows.drop(drop_list)\n",
    "\n",
    "    valMeans = valRows.mean(axis=1)\n",
    "\n",
    "    # Mean imputation\n",
    "    for i, col in enumerate(valRows):\n",
    "        valRows.iloc[:, i] = valRows.iloc[:, i].fillna(valMeans)\n",
    "\n",
    "    # scale normalize on each row\n",
    "    valRows_matrix = []\n",
    "    for c in (valRows.values.tolist()):\n",
    "        c = preprocessing.scale(c)\n",
    "        valRows_matrix.append(c)\n",
    "    \n",
    "    # qqnorms on the columns\n",
    "    matrix = np.array(valRows_matrix)\n",
    "    normalized_table = pd.DataFrame(matrix)\n",
    "\n",
    "    # reset row index for the saved intron infomation so the index will match sample values\n",
    "    newtable = newtable.reset_index(drop=True)\n",
    "    # merge the two parts of table\n",
    "    output = pd.concat([newtable, normalized_table], axis=1)\n",
    "    output.columns = headers\n",
    "\n",
    "    # write normalized table, avoid scientific notation\n",
    "    output.to_csv(f'${_input:n}.qqnorm.txt', sep=\"\\t\", index=None, float_format='%.16f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     "shell"
    ],
    [
     "Python3",
     "python3",
     "Python3",
     "#FFD91A",
     {
      "name": "ipython",
      "version": 3
     }
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
