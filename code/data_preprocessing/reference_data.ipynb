{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "described-restoration",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Reference data standardization\n",
    "\n",
    "This module provides reference data download, indexing and preprocessing (if necessary), in preparation for use throughout the pipeline.\n",
    "\n",
    "We have included the PDF document compiled by data standardization subgroup in the [on Google Drive](https://drive.google.com/file/d/1R5sw5o8vqk_mbQQb4CGmtH3ldu1T3Vu0/view?usp=sharing) as well as on [ADSP Dashboard](https://www.niagads.org/adsp/content/adspgcadgenomeresources-v2pdf). It contains the reference data to use for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-prescription",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This module is based on the [TOPMed workflow from Broad](https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md). The reference data after we process it (details see Methods section and the rest of the analysis) can be found [in this folder on Google Drive](https://drive.google.com/drive/folders/19fmoII8yS7XE7HFcMU4OfvC2bL1zMD_P). \n",
    "\n",
    "### Processed reference file for RNA-seq based expression quantification\n",
    "\n",
    "**We have decided to use these preprocessed reference files for RNA-seq expression quantification. They may not be applicable to other molecular phenotypes.**\n",
    "\n",
    "Specifically, the list of reference files to be used are:\n",
    "\n",
    "1. `GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.{dict,fasta,fasta.fai}`\n",
    "2. `Homo_sapiens.GRCh38.103.chr.reformatted.collapse_only.gene.ERCC.gtf` for stranded protocol, and `Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf` for unstranded protocol.\n",
    "3. Everything under `STAR_Index` folder\n",
    "4. Everything under `RSEM_Index` folder\n",
    "5. Optionally, for quality control, `gtf_ref.flat`\n",
    "\n",
    "\n",
    "\n",
    "## Methods\n",
    "\n",
    "Workflows implemented include:\n",
    "\n",
    "### Convert transcript feature file gff3 to gtf\n",
    "\n",
    "- Input: an uncompressed gff3 file.(i.e. can be view via cat)\n",
    "- Output: a gtf file.\n",
    "\n",
    "### Collapse transcript features into genes\n",
    "\n",
    "- Input: a gtf file.\n",
    "- Output: a gtf file with collapesed gene model.\n",
    "\n",
    "### Generate STAR index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of STAR index.\n",
    "\n",
    "### Generate RSEM index based on gtf and reference fasta\n",
    "\n",
    "- Input: a gtf file and an acompanying fasta file.\n",
    "- Output: A folder of RSEM index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-running",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Example commands\n",
    "\n",
    "To download reference data, it will take approximately an hour, depending on the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-suite",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb download_hg_reference --cwd reference_data    &\n",
    "sos run pipeline/reference_data.ipynb download_gene_annotation --cwd reference_data &\n",
    "sos run pipeline/reference_data.ipynb download_ercc_reference --cwd reference_data &\n",
    "sos run pipeline/reference_data.ipynb download_dbsnp --cwd reference_data &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-blood",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format reference data, these step should take ~10 min in total, with 16GB of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-average",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run reference_data.ipynb hg_reference \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-reference reference_data/ERCC92.fa \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.fa \\\n",
    "    --container container/rna_quantification.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-reputation",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb hg_gtf \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.gtf \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy.fasta \\\n",
    "    --container containers/rna_quantification.sif --stranded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-karaoke",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To format gene feature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-colonial",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb gene_annotation \\\n",
    "    --cwd reference_data \\\n",
    "    --ercc-gtf reference_data/ERCC92.gtf \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.gtf \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy.fasta \\\n",
    "    --container containers/rna_quantification.sif --stranded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-valuable",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Notice that for un-stranded RNA-seq protocol please use switch `--no-stranded` to the command above instead of `--stranded`. More details can be found later in the document.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-whole",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "Generating STAR index without the GTF annotation file allow customize read lenght lateron in STAR alignment. it will take at least 40G of memory for STAR to build the index. \n",
    "Aproximate time: 30  min\n",
    "Mem: 40 G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-nancy",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb STAR_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --container containers/rna_quantification.sif \\\n",
    "    --mem 40G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-vulnerability",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "**Notice that command above requires at least 40G of memory, and takes quite a while to complete**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-sperm",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate RSEM index with the gtf file **prior** to the gene collapsing step ( **without** the gene tag in its file name. )\n",
    "\n",
    "Aproximate time: 1  min "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-advice",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb RSEM_index \\\n",
    "    --cwd reference_data \\\n",
    "    --hg-reference reference_data/GRCh38_full_analysis_set_plus_decoy_hla.noALT_noHLA_noDecoy_ERCC.fasta \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf \\\n",
    "    --container containers/rna_quantification.sif  &"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f27548a-5475-4dd9-83d0-38a7aeb0b1f7",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "To generate RefFlat annotation for Picard QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583a351-d2ab-4408-bdce-107acd2acba8",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb RefFlat_generation \\\n",
    "    --hg-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.ERCC.gtf "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "objective-export",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regular-minority",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run reference_data.ipynb [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  download_hg_reference\n",
      "  download_gene_annotation\n",
      "  download_ercc_reference\n",
      "  gff3_to_gtf\n",
      "  hg_reference\n",
      "  hg_gtf\n",
      "  ercc_gtf\n",
      "  gene_annotation\n",
      "  STAR_index\n",
      "  RSEM_indexing\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd VAL (as path, required)\n",
      "                        The output directory for generated files.\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 8 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "                        Software container option\n",
      "\n",
      "Sections\n",
      "  download_hg_reference:\n",
      "  download_gene_annotation:\n",
      "  download_ercc_reference:\n",
      "  gff3_to_gtf:\n",
      "    Workflow Options:\n",
      "      --gff3-file VAL (as path, required)\n",
      "  hg_reference_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "                        Path to HG reference file\n",
      "  hg_reference_2:\n",
      "    Workflow Options:\n",
      "      --ercc-reference VAL (as path, required)\n",
      "  hg_reference_3:\n",
      "  hg_gtf_1:\n",
      "    Workflow Options:\n",
      "      --hg-reference VAL (as path, required)\n",
      "      --hg-gtf VAL (as path, required)\n",
      "  hg_gtf_2:\n",
      "    Workflow Options:\n",
      "      --[no-]collapse-only (default to False)\n",
      "                        Use this for stranded protocol (optional)\n",
      "  ercc_gtf:\n",
      "    Workflow Options:\n",
      "      --ercc-gtf VAL (as path, required)\n",
      "  gene_annotation:\n",
      "  STAR_index:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hg-reference VAL (as path, required)\n",
      "      --sjdbOverhang 100 (as int)\n",
      "                        Specifies the length of the genomic sequence around the\n",
      "                        annotated junction to be used in constructing the splice\n",
      "                        junctions database. Ideally, this length should be equal\n",
      "                        to the ReadLength-1, where ReadLength is the length of\n",
      "                        the reads. Default choice follows from TOPMed pipeline\n",
      "                        recommendation.\n",
      "  RSEM_indexing:\n",
      "    Workflow Options:\n",
      "      --hg-gtf VAL (as path, required)\n",
      "      --hg-reference VAL (as path, required)\n"
     ]
    }
   ],
   "source": [
    "sos run reference_data.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-advocacy",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# The output directory for generated files.\n",
    "parameter: cwd = path(\"output\")\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 8\n",
    "# Software container option\n",
    "parameter: container = \"\"\n",
    "cwd = path(f'{cwd:a}')\n",
    "from sos.utils import expand_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-delay",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-framing",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_hg_reference]\n",
    "output: f\"{cwd:a}/GRCh38_full_analysis_set_plus_decoy_hla.fa\"\n",
    "download: dest_dir = cwd\n",
    "    ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-liabilities",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_gene_annotation]\n",
    "output: f\"{cwd:a}/Homo_sapiens.GRCh38.103.chr.gtf\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    http://ftp.ensembl.org/pub/release-103/gtf/homo_sapiens/Homo_sapiens.GRCh38.103.chr.gtf.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-ground",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_ercc_reference]\n",
    "output: f\"{cwd:a}/ERCC92.gtf\", f\"{cwd:a}/ERCC92.fa\"\n",
    "download: dest_dir = cwd, decompress=True\n",
    "    https://tools.thermofisher.com/content/sfs/manuals/ERCC92.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac1c18-c0b4-482b-99c3-cbe79f6e36bf",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[download_dbsnp]\n",
    "output: f\"{cwd:a}/00-All.vcf.gz\", f\"{cwd:a}/00-All.vcf.gz.tbi\"\n",
    "download: dest_dir = cwd\n",
    "    ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz\n",
    "    ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz.tbi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-paradise",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## GFF3 to GTF formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-teens",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gff3_to_gtf]\n",
    "parameter: gff3_file = path\n",
    "input: gff3_file\n",
    "output: f'{cwd}/{_input:bn}.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout'\n",
    "        gffread ${_input} -T -o ${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "august-attraction",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## HG reference file preprocessing\n",
    "1. Remove the HLA/ALT/Decoy record from the fasta -- because none of the downstreams RNA-seq calling pipeline component can handle them properly.\n",
    "2. Adding in ERCC information to the fasta file -- even if ERCC is not included in the RNA-seq library it does not harm to add them.\n",
    "3. Generating index for the fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hydraulic-herald",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_1 (HLA ALT Decoy removal)]\n",
    "# Path to HG reference file\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference\n",
    "output:  f'{cwd}/{_input:bn}.noALT_noHLA_noDecoy.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}', 'r') as fasta:\n",
    "        contigs = fasta.read()\n",
    "        contigs = contigs.split('>')\n",
    "        contig_ids = [i.split(' ', 1)[0] for i in contigs]\n",
    "\n",
    "        # exclude ALT, HLA and decoy contigs\n",
    "        filtered_fasta = '>'.join([c for i,c in zip(contig_ids, contigs)\n",
    "        if not (i[-4:]=='_alt' or i[:3]=='HLA' or i[-6:]=='_decoy')])\n",
    "    \n",
    "    with open('${_output}', 'w') as fasta:\n",
    "        fasta.write(filtered_fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-screening",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_reference_2 (merge with ERCC reference)]\n",
    "parameter: ercc_reference = path\n",
    "output: f'{cwd}/{_input:bn}_ERCC.fasta'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    sed 's/ERCC-/ERCC_/g' ${ercc_reference} >  ${ercc_reference:n}.patched.fa\n",
    "    cat ${_input} ${ercc_reference:n}.patched.fa > ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incredible-table",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[hg_reference_3 (index the fasta file)]\n",
    "output: f'{cwd}/{_input:bn}.dict'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    samtools faidx ${_input}\n",
    "    java -jar /opt/picard-tools/picard.jar \\\n",
    "    CreateSequenceDictionary \\\n",
    "    R=${_input} \\\n",
    "    O=${_output}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "employed-glass",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Transcript and gene model reference processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-colors",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "This step modify the `gtf` file for following reasons:\n",
    "\n",
    "1. RSEM require GTF input to have the same chromosome name format (with `chr` prefix) as the fasta file. **although for STAR, this problem can be solved by the now commented --sjdbGTFchrPrefix \"chr\" option, we have to add `chr` to it for use with RSEM**. \n",
    "2. Gene model collapsing script `collapse_annotation.py` from GTEx require the gtf have `transcript_type` instead `transcript_biotype` in its annotation. We rename it here, although **this problem can also be solved by modifying the collapse_annotation.py while building the docker, since we are doing 1 above we think it is better to add in another customization here.**\n",
    "3. Adding in ERCC information to the `gtf` reference.\n",
    "\n",
    "We may reimplement 1 and 2 if the problem with RSEM is solved, or when RSEM is no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-camel",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_1 (add chr prefix to gtf file)]\n",
    "parameter: hg_reference = path\n",
    "parameter: hg_gtf = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f'{cwd}/{_input[1]:bn}.reformatted.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "R: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    library(\"readr\")\n",
    "    library(\"stringr\")\n",
    "    library(\"dplyr\")\n",
    "    options(scipen = 999)\n",
    "    con <- file(\"${_input[0]}\",\"r\")\n",
    "    fasta <- readLines(con,n=1)\n",
    "    close(con)\n",
    "    gtf = read_delim(\"${_input[1]}\", \"\\t\",  col_names  = F, comment = \"#\", col_types=\"ccccccccc\")\n",
    "    if(!str_detect(fasta,\">chr\")) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = str_remove_all(X1,\"chr\"))\n",
    "    } else if (!any(str_detect(gtf$X1[1],\"chr\"))) {\n",
    "        gtf_mod = gtf%>%mutate(X1 = paste0(\"chr\",X1))\n",
    "    } else (gtf_mod = gtf)\n",
    "    if(any(str_detect(gtf_mod$X9, \"transcript_biotype\"))) {\n",
    "      gtf_mod = gtf_mod%>%mutate(X9 = str_replace_all(X9,\"transcript_biotype\",\"transcript_type\"))\n",
    "    }\n",
    "    gtf_mod%>%write.table(\"${_output}\",sep = \"\\t\",quote = FALSE,col.names = F,row.names = F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worse-notebook",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "**Text below is taken from https://github.com/broadinstitute/gtex-pipeline/tree/master/gene_model**\n",
    "\n",
    "\n",
    "Gene-level expression and eQTLs from the GTEx project are calculated based on a collapsed gene model (i.e., combining all isoforms of a gene into a single transcript), according to the following rules:\n",
    "\n",
    "1. Transcripts annotated as “retained_intron” or “read_through” are excluded. Additionally, transcripts that overlap with annotated read-through transcripts may be blacklisted (blacklists for GENCODE v19, 24 & 25 are provided in this repository; no transcripts were blacklisted for v26).\n",
    "2. The union of all exon intervals of each gene is calculated.\n",
    "3. Overlapping intervals between genes are excluded from all genes.\n",
    "\n",
    "\n",
    "The purpose of step 3 is primarily to exclude overlapping regions from genes annotated on both strands, which can't be unambiguously quantified from unstranded RNA-seq (GTEx samples were sequenced using an unstranded protocol). For stranded protocols, this step can be skipped by adding the `--collapse_only` flag.\n",
    "\n",
    "Further documentation is available on the [GTEx Portal](https://gtexportal.org/home/documentationPage#staticTextAnalysisMethods)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-criticism",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[hg_gtf_2 (collapsed gene model)]\n",
    "parameter: stranded = bool\n",
    "output: f'{_input:n}{\".collapse_only\" if stranded else \"\"}.gene.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    collapse_annotation.py ${\"--collapse_only\" if stranded else \"\"} ${_input} ${_output}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-conducting",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[ercc_gtf (Preprocess ERCC gtf file)]\n",
    "parameter: ercc_gtf = path\n",
    "input: ercc_gtf\n",
    "output: f'{cwd}/{_input:bn}.genes.patched.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "python: expand = \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout', container = container\n",
    "    with open('${_input}') as exon_gtf, open('${_output}', 'w') as gene_gtf:\n",
    "        for line in exon_gtf:\n",
    "            f = line.strip().split('\\t')\n",
    "            f[0] = f[0].replace('-','_')  # required for RNA-SeQC/GATK (no '-' in contig name)\n",
    "        \n",
    "            attr = f[8]\n",
    "            if attr[-1]==';':\n",
    "                attr = attr[:-1]\n",
    "            attr = dict([i.split(' ') for i in attr.replace('\"','').split('; ')])\n",
    "            # add gene_name, gene_type\n",
    "            attr['gene_name'] = attr['gene_id']\n",
    "            attr['gene_type'] = 'ercc_control'\n",
    "            attr['gene_status'] = 'KNOWN'\n",
    "            attr['level'] = 2\n",
    "            for k in ['id', 'type', 'name', 'status']:\n",
    "                attr['transcript_'+k] = attr['gene_'+k]\n",
    "        \n",
    "            attr_str = []\n",
    "            for k in ['gene_id', 'transcript_id', 'gene_type', 'gene_status', 'gene_name',\n",
    "                'transcript_type', 'transcript_status', 'transcript_name']:\n",
    "                attr_str.append('{0:s} \"{1:s}\";'.format(k, attr[k]))\n",
    "            attr_str.append('{0:s} {1:d};'.format('level', attr['level']))\n",
    "            f[8] = ' '.join(attr_str)\n",
    "        \n",
    "            # write gene, transcript, exon\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['gene']+f[3:])+'\\n')\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['transcript']+f[3:])+'\\n')\n",
    "            f[8] = ' '.join(attr_str[:2])\n",
    "            gene_gtf.write('\\t'.join(f[:2]+['exon']+f[3:])+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-applicant",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[gene_annotation]\n",
    "input: output_from(\"hg_gtf_1\"), output_from(\"hg_gtf_2\"), output_from(\"ercc_gtf\")\n",
    "output: f'{cwd}/{_input[0]:bn}.ERCC.gtf', f'{cwd}/{_input[1]:bn}.ERCC.gtf'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'\n",
    "bash: expand = \"${ }\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout', container = container\n",
    "    cat ${_input[0]} ${_input[2]} > ${_output[0]}\n",
    "    cat ${_input[1]} ${_input[2]} > ${_output[1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clean-needle",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Generating index file for `STAR` \n",
    "\n",
    "This step generate the index file for STAR alignment. This file just need to generate once and can be re-used. \n",
    "\n",
    "**At least 40GB of memory is needed**.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. Both of them needs to be unzipped. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads. We use 100 here as recommended by the TOPMed pipeline. See here for [some additional discussions](https://groups.google.com/g/rna-star/c/h9oh10UlvhI/m/BfSPGivUHmsJ). \n",
    "\n",
    "### Step Output\n",
    "\n",
    "* Indexing file stored in `{cwd}/STAR_index`, which will be used by `STAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-karen",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[STAR_index]\n",
    "parameter: hg_reference = path\n",
    "# Specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "# Default choice follows from TOPMed pipeline recommendation.\n",
    "if expand_size(mem) < expand_size('40G'):\n",
    "    print(\"Insufficent memory for STAR, changing to 40G\")\n",
    "    star_mem = '40G'\n",
    "else:\n",
    "    star_mem = mem\n",
    "input: hg_reference\n",
    "output: f\"{cwd}/STAR_Index/chrName.txt\", \n",
    "        f\"{cwd}/STAR_Index/SAindex\", f\"{cwd}/STAR_Index/SA\", f\"{cwd}/STAR_Index/genomeParameters.txt\", \n",
    "        f\"{cwd}/STAR_Index/chrStart.txt\",\n",
    "        f\"{cwd}/STAR_Index/chrLength.txt\", \n",
    "        f\"{cwd}/STAR_Index/Genome\", f\"{cwd}/STAR_Index/chrNameLength.txt\", \n",
    "        f\"{cwd}/STAR_Index/geneInfo.tab\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[1]:n}.stderr', stdout = f'{_output[1]:n}.stdout'\n",
    "    STAR --runMode genomeGenerate \\\n",
    "         --genomeDir ${_output[0]:d} \\\n",
    "         --genomeFastaFiles ${_input[0]} \\\n",
    "         --runThreadN ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "durable-proxy",
   "metadata": {
    "kernel": "Bash"
   },
   "source": [
    "## Generating index file for `RSEM`\n",
    "\n",
    "This step generate the indexing file for `RSEM`. This file just need to generate once.\n",
    "\n",
    "### Step Inputs\n",
    "\n",
    "* `gtf` and `fasta`: path to reference sequence. `gtf` should be the one prior to collapse by gene.\n",
    "* `sjdbOverhang`: specifies the length of the genomic sequence around the annotated junction to be used in constructing the splice junctions database. Ideally, this length should be equal to the ReadLength-1, where ReadLength is the length of the reads.\n",
    "\n",
    "### Step Outputs\n",
    "* Indexing file stored in `RSEM_index_dir`, which will be used by `RSEM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-oregon",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[RSEM_index]\n",
    "parameter: hg_gtf = path\n",
    "parameter: hg_reference = path\n",
    "input: hg_reference, hg_gtf\n",
    "output: f\"{cwd}/RSEM_Index/rsem_reference.n2g.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.grp\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.idx.fa\", f\"{cwd}/RSEM_Index/rsem_reference.ti\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.chrlist\", f\"{cwd}/RSEM_Index/rsem_reference.seq\", \n",
    "        f\"{cwd}/RSEM_Index/rsem_reference.transcripts.fa\"\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime, mem = mem, tags = f'{step_name}_{_output[0]:bd}'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output[1]:n}.stderr', stdout = f'{_output[1]:n}.stdout'\n",
    "    rsem-prepare-reference \\\n",
    "            ${_input[0]} \\\n",
    "            ${_output[1]:n} \\\n",
    "            --gtf ${_input[1]} \\\n",
    "            --num-threads ${numThreads}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-lotus",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Generation of RefFlat file \n",
    "This file is needed for picard CollectRnaSeqMetrics module, which in turn \n",
    ">produces metrics describing the distribution of the bases within the transcripts. It calculates the total numbers and the fractions of nucleotides within specific genomic regions including untranslated regions (UTRs), introns, intergenic sequences (between discrete genes), and peptide-coding sequences (exons). This tool also determines the numbers of bases that pass quality filters that are specific to Illumina data (PF_BASES)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-blank",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[RefFlat_generation]\n",
    "parameter: hg_gtf = path\n",
    "input: hg_gtf\n",
    "output: f'{_input:n}.ref.flat'\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    gtfToGenePred ${_input}  ${_output}.tmp -genePredExt -geneNameAsName2\n",
    "    awk -F'\\t' -v OFS=\"\\t\" '{$1=$12 OFS $1;}7' ${_output}.tmp | cut -f 1-11 > ${_output}\n",
    "    rm ${_output}.tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae142156-b455-4a04-8c61-604949abd5c7",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "### Generation of SUPPA annotation for psichomics.\n",
    "The generation of custom alternative splicing annotation is based on [this tutorial](https://rpubs.com/nuno-agostinho/preparing-AS-annotation). The way to generate the local alternative splicing suppa output is documented on [SUPPA github page](https://github.com/comprna/SUPPA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0925b02-a448-4276-ae51-614e47101d4c",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb SUPPA_annotation \\\n",
    "    --hg_gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformated.gtf \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb074fe-4236-4c15-b815-9aec345e674d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[SUPPA_annotation_1]\n",
    "parameter: hg_gtf = path\n",
    "input: hg_gtf\n",
    "output: f'{cwd}/hg38.{_input:bn}_SE_strict.ioe' # The stderr file must not shared the same start with the output file\n",
    "bash: container=container, expand= \"${ }\", stderr = f'{cwd}/{_input:bn}.stderr', stdout = f'{cwd}/{_input:bn}.stdout'\n",
    "    python ~/GIT/SUPPA/suppa.py generateEvents -i ${_input} -o ${cwd}/hg38.${_input:bn} -f ioe -e SE SS MX RI FL\n",
    "[SUPPA_annotation_2]\n",
    "parameter: hg_gtf = path\n",
    "output: f'{cwd}/{hg_gtf:bn}.SUPPA_annotation.rds'\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    library(\"psichomics\")\n",
    "    suppa <- parseSuppaAnnotation(\"${_input:d}\", genome=\"hg38\") \n",
    "    annot <- prepareAnnotationFromEvents(suppa)\n",
    "    saveRDS(annot, file=${_output:r})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7e86b4-1df2-4fc0-9973-00fa60400171",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "### Modification of psichomics default Hg38 splicing annotation.\n",
    "Since the original annotation provided by psichomics package is using gene symbols, we modified it to use Ensembl IDs. The modified annotation will be used here: [psichomics section](https://github.com/cumc/xqtl-pipeline/blob/main/code/molecular_phenotypes/calling/splicing_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2da2c-5d4e-41f2-b674-e5195337eb3a",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "outputs": [],
   "source": [
    "sos run pipeline/reference_data.ipynb psi_hg38_annotation_modification \\\n",
    "    --hg_gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformated.gtf \\\n",
    "    --hgrc_db reference_data/hgnc_database.txt \\\n",
    "    --container container/psichomics.sif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf704d4-bd43-4af8-b675-1a94b1a3b02a",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "[psi_hg38_annotation_modification_1]\n",
    "parameter: hg_gtf = path\n",
    "parameter: hgrc_db = path\n",
    "input: hg_gtf, hgrc_db\n",
    "output: f'{cwd}/modified_psichomics_hg38_splicing_annotation.rds'\n",
    "R: container=container, expand= \"${ }\", stderr = f'{_output}.stderr', stdout = f'{_output}.stdout'\n",
    "    library(\"psichomics\")\n",
    "    library(\"purrr\")\n",
    "    library(\"dplyr\")\n",
    "    library(\"tidyr\")\n",
    "    library(\"data.table\")\n",
    "  \n",
    "    # load psicomics default annotation, option of hg38 from listSplicingAnnotations()\n",
    "    annotation <- loadAnnotation(\"AH63657\")\n",
    "\n",
    "    \n",
    "    # reduce the demension of annotation file\n",
    "    annotation <- \n",
    "      map(annotation, ~.x%>%\n",
    "                       tidyr::unnest(cols = `Gene`))\n",
    "  \n",
    "    # Create empty colomns for each event for easier mapping\n",
    "    annotation[[\"Tandem UTR\"]][[\"SUPPA.Event.ID\"]] <- NA\n",
    "    annotation[[\"Tandem UTR\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "    annotation[[\"Alternative first exon\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "    annotation[[\"Alternative last exon\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "    annotation[[\"Mutually exclusive exon\"]][[\"VAST-TOOLS.Event.ID\"]] <- NA\n",
    "  \n",
    "    # extract Ensembl ID substring from original SUPPA.ID and VASTTOOL.ID\n",
    "    annotation <- \n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(ENSG.SUPPA = substr(`SUPPA.Event.ID`, 1, 15))%>%\n",
    "                       mutate(ENSG.VAST = substr(`VAST-TOOLS.Event.ID`, 1, 15)))\n",
    "  \n",
    "    # Load gtf file\n",
    "    gtf_sample <- read.table('${_input[0]}',header = FALSE, sep = '\\t')\n",
    "  \n",
    "    # from the gtf file, seperate gene names and corresponding Ensembl ID\n",
    "    gtf_sample <- separate(gtf_sample, V9, sep = \";\",into = c(\"gene_id\", \"transcript_id\", \"exon_number\", \"gene_name\"))\n",
    "    gtf_sample <- separate(gtf_sample, gene_id, sep = \" \",into = c(\"gene_id\", \"gene_id_val\"))\n",
    "    gtf_sample <- separate(gtf_sample, gene_name, sep = \"e \",into = c(\"gene_name\", \"gene_name_val\"))\n",
    "  \n",
    "    gtf_name_id_match <- gtf_sample[,c(\"gene_id_val\",\"gene_name_val\")]\n",
    "    gtf_name_id_match <- gtf_name_id_match[!duplicated(gtf_name_id_match), ]\n",
    "  \n",
    "    # For any matched approved id in the psi hg38 annotation and gtf file, record the corresponding Ensembl ID\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(`ENSG.GTF` = gtf_name_id_match$gene_id_val[match(`Gene`, gtf_name_id_match$gene_name_val)]))\n",
    "  \n",
    "    # load hgnc database\n",
    "    hgnc_db <- fread('${_input[1]}', fill = TRUE, header = TRUE, sep = '\\t', quote=\"\")\n",
    "  \n",
    "    # Combine the `Ensembl.ID.supplied.by.Ensembl.` and `Ensembl.gene.ID` column, if there are any conflict use the former\n",
    "    # For conflict ones (15 total) both the former and latter records are poiting to the same gene name in Ensembl website so the order should not matter\n",
    "    hgnc_db <- hgnc_db %>%\n",
    "    mutate(ENSG.ID = ifelse(`Ensembl ID(supplied by Ensembl)` == \"\", `Ensembl gene ID`, `Ensembl ID(supplied by Ensembl)`))\n",
    "  \n",
    "    # Create a one to one reference list for approved names, previous names and aliases\n",
    "    # There is no duplicate symbol and Ensembl id info for approved symbol so no need for chromosome verification\n",
    "    hgnc_name_id_match <- hgnc_db[,c(\"Approved symbol\",\"ENSG.ID\")]\n",
    "    hgnc_name_prev_check <- hgnc_db[,c(\"Previous symbols\",\"Chromosome\",\"ENSG.ID\")]\n",
    "    hgnc_name_alias_check <- hgnc_db[,c(\"Alias symbols\",\"Chromosome\",\"ENSG.ID\")]\n",
    "  \n",
    "    # Remove NAs\n",
    "    hgnc_name_prev_check <- hgnc_name_prev_check[hgnc_name_prev_check$ENSG.ID != \"\",]\n",
    "    hgnc_name_alias_check <- hgnc_name_alias_check[hgnc_name_alias_check$ENSG.ID != \"\",]\n",
    "\n",
    "    hgnc_name_prev_check <- hgnc_name_prev_check[hgnc_name_prev_check$\"Previous symbols\" != \"\",] \n",
    "    hgnc_name_alias_check <- hgnc_name_alias_check[hgnc_name_alias_check$\"Alias symbols\" != \"\",]\n",
    "\n",
    "    # Seperate symbol column values from list of sybols to individual rows with one each\n",
    "    hgnc_name_prev_check <- separate_rows(hgnc_name_prev_check, \"Previous symbols\", convert = FALSE)\n",
    "    hgnc_name_alias_check <- separate_rows(hgnc_name_alias_check, \"Alias symbols\", convert = FALSE)\n",
    "  \n",
    "    # Convert chomosome info in hgnc database to number for matching with other database\n",
    "    hgnc_name_prev_check <- separate(hgnc_name_prev_check, \"Chromosome\", sep = 'p', into = \"Chrp\", remove = FALSE)\n",
    "    hgnc_name_prev_check <- separate(hgnc_name_prev_check, \"Chromosome\", sep = 'q', into = \"Chrq\", remove = FALSE)\n",
    "    hgnc_name_prev_check <- hgnc_name_prev_check%>%\n",
    "                                mutate(Chr = ifelse(nchar(hgnc_name_prev_check$Chrp) <= 2, Chrp, Chrq))\n",
    "    \n",
    "    hgnc_name_alias_check <- separate(hgnc_name_alias_check, \"Chromosome\", sep = 'p', into = \"Chrp\", remove = FALSE)\n",
    "    hgnc_name_alias_check <- separate(hgnc_name_alias_check, \"Chromosome\", sep = 'q', into = \"Chrq\", remove = FALSE)\n",
    "    hgnc_name_alias_check <- hgnc_name_alias_check%>%\n",
    "                                mutate(Chr = ifelse(nchar(hgnc_name_alias_check$Chrp) <= 2, Chrp, Chrq))\n",
    "  \n",
    "    # For any matched approved id in the psi hg38 annotation and hgnc database, record the corresponding Ensembl ID\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(`ENSG.HGNC` = hgnc_name_id_match$`ENSG.ID`[match(`Gene`, hgnc_name_id_match$\"Approved symbol\")]))\n",
    "  \n",
    "    # Drop hypothetical genes\n",
    "    annotation<-\n",
    "      map(annotation, ~.x%>%\n",
    "            subset(`Gene` != 'Hypothetical'))\n",
    "  \n",
    "    # IN remaining NAs, for any matched alias/previous names and chromosome in the psi hg38 annotation and hgnc database, record the corresponding Ensembl ID\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                      mutate(ENSG.HGNC = ifelse(is.na(`ENSG.HGNC`) | `ENSG.HGNC` == \"\",\n",
    "                                                hgnc_name_alias_check$`ENSG.ID`[match(`Gene`, hgnc_name_alias_check$\"Alias symbols\") & match(`Chromosome`, hgnc_name_alias_check$Chr)],\n",
    "                                                `ENSG.HGNC`))%>%\n",
    "                      mutate(ENSG.HGNC = ifelse(is.na(`ENSG.HGNC`) | `ENSG.HGNC` == \"\",\n",
    "                                                hgnc_name_prev_check$`ENSG.ID`[match(`Gene`, hgnc_name_prev_check$\"Previous symbols\") & match(`Chromosome`, hgnc_name_prev_check$Chr)],\n",
    "                                                `ENSG.HGNC`)))\n",
    "  \n",
    "    # Build the final Ensembl id column base on the gtf file first, then for remaining NAs check the HGNC database record, SUPPA and VASTTOOL record,\n",
    "    # Drop special cases that Ensembl ID is not recorded in VASTTOOLs and SUPPA\n",
    "    # finnally in the remaining NA Ensmbl ID if the gene name in original annotation is NCBI IDs just use it\n",
    "    annotation <-\n",
    "      map(annotation, ~.x%>%\n",
    "                       mutate(`ENSG.ID` = `ENSG.GTF`)%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`),\n",
    "                                               `ENSG.HGNC`,\n",
    "                                                `ENSG.ID`))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`),\n",
    "                                               `ENSG.VAST`,\n",
    "                                                `ENSG.ID`))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`),\n",
    "                                               `ENSG.SUPPA`,\n",
    "                                                `ENSG.ID`))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(substr(`ENSG.ID`, 1, 4) == 'ENSG',\n",
    "                                                 `ENSG.ID`,\n",
    "                                                 NA))%>%\n",
    "                       mutate(`ENSG.ID` = ifelse(is.na(`ENSG.ID`) & substr(`Gene`, 1, 3) == 'LOC',\n",
    "                                               `Gene`,\n",
    "                                                `ENSG.ID`)))\n",
    "\n",
    "    # Use the Ensembl IDs to replace gene names, drop remaining NAs\n",
    "    annotation <-\n",
    "    map(annotation, ~.x%>%\n",
    "                       mutate(`Gene` = `ENSG.ID`)%>%\n",
    "            drop_na(`Gene`)\n",
    "          )\n",
    "\n",
    "    # save modified annotation\n",
    "    saveRDS(annotation, file = \"${_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.22.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
