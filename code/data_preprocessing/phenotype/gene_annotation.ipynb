{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minus-singles",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "# Gene coordinate annotation\n",
    "\n",
    "\n",
    "This workflow adds genomic coordinate annotation to gene-level molecular phenotype files generated in `gct` format and convert them to `bed` format for downstreams analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "industrial-click",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This pipeline is based on [`pyqtl`, as demonstrated here](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py).\n",
    "\n",
    "**FIXME: please explain here what we do with gene symbol vs gene ID**\n",
    "\n",
    "### Alternative implementation\n",
    "\n",
    "Previously we use `biomaRt` package in R instead of code from `pyqtl`. The core function calls are:\n",
    "\n",
    "```r\n",
    "    ensembl = useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "    ensembl_df <- getBM(attributes=c(\"ensembl_gene_id\",\"chromosome_name\", \"start_position\", \"end_position\"),mart=ensembl)\n",
    "```\n",
    "\n",
    "We require ENSEMBL version to be specified explicitly in this pipeline. As of 2021 for the Brain xQTL project, we use ENSEMBL version 103."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "devoted-anchor",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Input\n",
    "\n",
    "1. Molecular phenotype data in `gct` format, with the first column being ENSEMBL ID and other columns being sample names. \n",
    "2. GTF for collapsed gene model\n",
    "    - the gene names must be consistent with the molecular phenotype data matrices (eg ENSG00000000003 vs. ENSG00000000003.1 will not work) \n",
    "3. (Optional) Meta-data to match between sample names in expression data and genotype files\n",
    "    - Tab delimited with header\n",
    "    - Only 2 columns: first column is sample name in expression data, 2nd column is sample name in genotype data\n",
    "    - **must contains all the sample name in expression matrices even if they don't existing in genotype data**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-commodity",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Output\n",
    "\n",
    "Molecular phenotype data in `bed` format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-laugh",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Minimal working example\n",
    "\n",
    "The MWE is uploaded to the [Google Drive](https://drive.google.com/drive/u/0/folders/1Rv2bWHBbX_tastTh49ToYVDMV6rFP5Wk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-limitation",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "sos run gene_annotation.ipynb annotate_coord_gene \\\n",
    "    --cwd output \\\n",
    "    --phenoFile data/MWE.pheno_log2cpm.tsv.gz \\\n",
    "    --annotation-gtf reference_data/Homo_sapiens.GRCh38.103.chr.reformatted.gene.ERCC.gtf \\\n",
    "    --sample-participant-lookup data/sampleSheetAfterQC.txt \\\n",
    "    --container container/rna_quantification.sif --phenotype-id-type gene_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85299e2-20f6-45c3-84f5-58cde3c25e8e",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "covered-myrtle",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Command interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "after-coaching",
   "metadata": {
    "kernel": "Bash"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: sos run gene_annotation.ipynb\n",
      "               [workflow_name | -t targets] [options] [workflow_options]\n",
      "  workflow_name:        Single or combined workflows defined in this script\n",
      "  targets:              One or more targets to generate\n",
      "  options:              Single-hyphen sos parameters (see \"sos run -h\" for details)\n",
      "  workflow_options:     Double-hyphen workflow-specific parameters\n",
      "\n",
      "Workflows:\n",
      "  annotate_coord\n",
      "  annotate_coord_biomart\n",
      "\n",
      "Global Workflow Options:\n",
      "  --cwd output (as path)\n",
      "                        Work directory & output directory\n",
      "  --annotation-gtf VAL (as path, required)\n",
      "                        gene gtf annotation table\n",
      "  --phenoFile VAL (as path, required)\n",
      "                        Molecular phenotype matrix\n",
      "  --phenotype-id-type 'gene_id'\n",
      "                        Whether the input data is named by gene_id or gene_name.\n",
      "                        By default it is gene_id, if not, please change it to\n",
      "                        gene_name\n",
      "  --job-size 1 (as int)\n",
      "                        For cluster jobs, number commands to run per job\n",
      "  --walltime 5h\n",
      "                        Wall clock time expected\n",
      "  --mem 16G\n",
      "                        Memory expected\n",
      "  --numThreads 1 (as int)\n",
      "                        Number of threads\n",
      "  --container ''\n",
      "\n",
      "Sections\n",
      "  annotate_coord:\n",
      "    Workflow Options:\n",
      "      --sample-participant-lookup . (as path)\n",
      "                        A file to map sample ID from expression to genotype,\n",
      "                        must contain two columns, sample_id and participant_id,\n",
      "                        mapping IDs in the expression files to IDs in the\n",
      "                        genotype (these can be the same).\n",
      "  annotate_coord_biomart:\n",
      "    Workflow Options:\n",
      "      --ensembl-version VAL (as int, required)\n"
     ]
    }
   ],
   "source": [
    "sos run gene_annotation.ipynb -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-bedroom",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[global]\n",
    "# Work directory & output directory\n",
    "parameter: cwd = path(\"output\")\n",
    "#  gene gtf annotation table\n",
    "parameter: annotation_gtf = path\n",
    "# Molecular phenotype matrix\n",
    "parameter: phenoFile = path\n",
    "# Whether the input data is named by gene_id or gene_name. By default it is gene_id, if not, please change it to gene_name\n",
    "parameter: phenotype_id_type = 'gene_id'\n",
    "# For cluster jobs, number commands to run per job\n",
    "parameter: job_size = 1\n",
    "# Wall clock time expected\n",
    "parameter: walltime = \"5h\"\n",
    "# Memory expected\n",
    "parameter: mem = \"16G\"\n",
    "# Number of threads\n",
    "parameter: numThreads = 1\n",
    "parameter: container = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f1ac77-8faf-46a1-b471-73f1c1a36427",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Region List generation\n",
    "\n",
    "To partitioning the data by genes require a region list file which:\n",
    "\n",
    "1. have 5 columns: chr,start,end,gene_id,gene_name\n",
    "2. have the same gene as or less gene than that of the bed file\n",
    "\n",
    "Input:\n",
    "\n",
    "1. A gtf file used to generated the bed\n",
    "2. A phenotype bed file, must have a gene_id column indicating the name of genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fde2aa-42d9-46cc-ba9c-29aa57575e2d",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[region_list_generation]\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bnn}.region_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    # get the five column data\n",
    "    bed_template_df_id = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_id\" )\n",
    "    bed_template_df_name = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_name\" )\n",
    "    bed_template_df = bed_template_df_id.merge(bed_template_df_name, on = [\"chr\",\"start\",\"end\"])\n",
    "    bed_template_df.columns = [\"#chr\",\"start\",\"end\",\"gene_id\",\"gene_name\"]\n",
    "    pheno = pd.read_csv(${_input[0]:r}, sep = \"\\t\")\n",
    "    # Retaining only the genes in the data\n",
    "    region_list = bed_template_df[bed_template_df.${phenotype_id_type}.isin(pheno.gene_id)]\n",
    "    region_list.to_csv(\"${_output}\", sep = \"\\t\",index = 0)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-credits",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Implementation using `pyqtl`\n",
    "\n",
    "Implementation based on [GTEx pipeline](https://github.com/broadinstitute/gtex-pipeline/blob/master/qtl/src/eqtl_prepare_expression.py).\n",
    "\n",
    "Following step serves to annotate cood for gene expression file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-cycling",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_gene]\n",
    "# A file to map sample ID from expression to genotype, must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path()\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bn}.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "    def prepare_bed(df, bed_template_df, chr_subset=None):\n",
    "        bed_df = pd.merge(bed_template_df, df, left_index=True, right_index=True)\n",
    "        # sort by start position\n",
    "        bed_df = bed_df.groupby('#chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "        if chr_subset is not None:\n",
    "            # subset chrs from VCF\n",
    "            bed_df = bed_df[bed_df.chr.isin(chr_subset)]\n",
    "        return bed_df\n",
    "    # Load data\n",
    "    df = pd.read_csv(${_input[0]:ar}, sep='\\t', skiprows=0)\n",
    "    sample_participant_lookup = Path(\"${sample_participant_lookup:a}\")\n",
    "    if \"chr\" in df.columns and \"start\" in df.columns and  \"end\" in df.columns:\n",
    "        df = df.drop([\"chr\", \"start\", \"end\" ])\n",
    "    df.set_index( df.columns[0] , inplace=True)\n",
    "    \n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup.is_file():\n",
    "        sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=0, dtype={0:str,1:str}, squeeze=True)\n",
    "        df.rename(columns=sample_participant_lookup_s.to_dict(), inplace=True)\n",
    "\n",
    "    if sum(qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='gene',phenotype_id = \"gene_id\" ).index.duplicated()) >0:\n",
    "        raise ValueError(f\"GTF file ${_input[1]:ar} needs to be collapsed into gene model by reference data processing module\")\n",
    "         \n",
    "    bed_template_df_id = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_id\" )\n",
    "    bed_template_df_name = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_name\" )\n",
    "    bed_template_df = bed_template_df_id.merge(bed_template_df_name, on = [\"chr\",\"start\",\"end\"])\n",
    "    bed_template_df.columns = [\"#chr\",\"start\",\"end\",\"gene_id\",\"gene_name\"]\n",
    "    bed_template_df = bed_template_df.set_index(\"${phenotype_id_type}\", drop = False )\n",
    "    ### Detect duplicated gene_id\n",
    "    bed_df = prepare_bed(df, bed_template_df).drop(\"gene_name\",axis = 1)\n",
    "    bed_df = bed_df.drop_duplicates(\"gene_id\",keep = False)\n",
    "    qtl.io.write_bed(bed_df, ${_output:r})\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1697bf2c-24ea-40b9-930c-7a6bbbe201d7",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "575459b2-6b12-4305-acf6-e3c0dd6b20d8",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "Following step serves to annotate cood for proteiomics data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f1dd43-2868-499b-bcf7-6f9f06da5741",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_protein]\n",
    "# A file to map sample ID from expression to genotype, must contain two columns, sample_id and participant_id, mapping IDs in the expression files to IDs in the genotype (these can be the same).\n",
    "parameter: sample_participant_lookup = path()\n",
    "parameter: protein_name_index = path()\n",
    "parameter: protein_ID_type = \"SOMAseqID\"\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bn}.bed.gz'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "\n",
    "    import pandas as pd\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "    def prepare_bed(df, bed_template_df, chr_subset=None):\n",
    "        bed_df = pd.merge(bed_template_df, df, left_index=True, right_index=True)\n",
    "        # sort by start position\n",
    "        bed_df = bed_df.groupby('#chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "        if chr_subset is not None:\n",
    "            # subset chrs from VCF\n",
    "            bed_df = bed_df[bed_df.chr.isin(chr_subset)]\n",
    "        return bed_df\n",
    "    # Load data\n",
    "    df = pd.read_csv(${_input[0]:ar}, sep=None , skiprows=0)\n",
    "    if \"chr\" in df.columns or \"start\" in df.columns or  \"end\" in df.columns:\n",
    "        df = df.drop([\"chr\", \"start\", \"end\" ])\n",
    "    protein_ID = df.columns.values[0]\n",
    "\n",
    "    protein_name_index = Path(\"${protein_name_index:a}\")\n",
    "    # For other data please specify a protein ID index\n",
    "    if protein_name_index.is_file():\n",
    "        df_info = pd.read_csv(protein_name_index).rename(columns={'${protein_ID_type}': protein_ID, 'EntrezGeneSymbol':'gene_name'})[['gene_name',protein_ID,'UniProt']]\n",
    "        df = df_info.merge(df, on  = protein_ID)\n",
    "    else: # For ROSMAP data\n",
    "        df[['gene_name', 'UniProt']] = df[protein_ID].str.split('|', 1, expand=True)\n",
    "\n",
    "\n",
    "    sample_participant_lookup = Path(\"${sample_participant_lookup:a}\")\n",
    "\n",
    "    if \"chr\" in df.columns and \"start\" in df.columns and  \"end\" in df.columns:\n",
    "        df = df.drop([\"chr\", \"start\", \"end\" ])\n",
    "    df.set_index( df.columns[0] , inplace=True)\n",
    "\n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup.is_file():\n",
    "        sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=0, dtype={0:str,1:str}, squeeze=True)\n",
    "        df.rename(columns=sample_participant_lookup_s.to_dict(), inplace=True)\n",
    "\n",
    "    if sum(qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='gene',phenotype_id = \"gene_id\" ).index.duplicated()) >0:\n",
    "        raise ValueError(f\"GTF file ${_input[1]:ar} needs to be collapsed into gene model by reference data processing module\")\n",
    "         \n",
    "    bed_template_df_id = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_id\" )\n",
    "    bed_template_df_name = qtl.io.gtf_to_tss_bed(${_input[1]:ar}, feature='transcript',phenotype_id = \"gene_name\" )\n",
    "    bed_template_df = bed_template_df_id.merge(bed_template_df_name, on = [\"chr\",\"start\",\"end\"])\n",
    "    bed_template_df.columns = [\"#chr\",\"start\",\"end\",\"gene_id\",\"gene_name\"]\n",
    "    bed_template_df = bed_template_df.set_index(\"${phenotype_id_type}\", drop = False )\n",
    "\n",
    "    ### Detect duplicated gene_id\n",
    "    bed_df = prepare_bed(df, bed_template_df)\n",
    "    bed_df[\"ID\"] = bed_df[\"#chr\"] + \"_\" + bed_df[\"gene_id\"] + \"_\" + bed_df[\"UniProt\"]\n",
    "    bed_df = bed_df.drop_duplicates(\"ID\",keep = False)[[\"#chr\",\"start\",\"end\",\"ID\"] + df.drop([protein_ID,\"UniProt\"],axis = 1 ).columns.values.tolist()]\n",
    "    qtl.io.write_bed(bed_df, ${_output:r})\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output:n}.stderr', stdout = f'{_output:n}.stdout', container = container\n",
    "        stdout=$[_output:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radical-george",
   "metadata": {
    "kernel": "SoS",
    "tags": []
   },
   "source": [
    "## Implementation using biomaRt\n",
    "This workflow adds the annotations of chr pos(TSS where start = end -1) and gene_ID to the `bed` file. **This workflow is obsolete**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-protocol",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_coord_biomart]\n",
    "parameter: ensembl_version=int\n",
    "\n",
    "input: phenoFile\n",
    "output: f'{cwd:a}/{_input:bn}.bed.gz',\n",
    "        f'{cwd:a}/{_input:bn}.region_list'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "R:  expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout' ,container = container\n",
    "    library(\"biomaRt\")\n",
    "    library(dplyr)\n",
    "    library(readr)\n",
    "    biomartCacheClear()\n",
    "    gene_exp = readr::read_delim(\"$[_input[0]]\",delim = \"\\t\")\n",
    "    if(\"#chr\" %in% colnames(gene_exp) ){\n",
    "      # need to re-annotate\n",
    "      gene_exp = gene_exp[,4:ncol(gene_exp)]\n",
    "    }\n",
    "    ensembl = useEnsembl(biomart = \"ensembl\", dataset = \"hsapiens_gene_ensembl\", version = \"$[ensembl_version]\")\n",
    "    ensembl_df <- getBM(attributes=c(\"ensembl_gene_id\",\"chromosome_name\", \"start_position\", \"end_position\"),mart=ensembl)\n",
    "    my_genes = gene_exp$gene_ID\n",
    "    keep_genes =  my_genes\n",
    "    my_genes_ann = ensembl_df[match(my_genes, ensembl_df$ensembl_gene_id),]%>%filter(chromosome_name%in%1:23)%>%dplyr::rename( \"#chr\" = chromosome_name, \"start\" = start_position, \"end\" = end_position,\"gene_ID\" = ensembl_gene_id)%>%filter(gene_ID!=\"NA\", gene_ID%in%keep_genes)\n",
    "    my_genes_ann%>%select(`#chr`,start,end,gene_ID)%>%write_delim(path = \"$[_output[1]]\",\"\\t\")\n",
    "    my_gene_bed = inner_join(my_genes_ann %>%mutate(end = start + 1) %>%select(`#chr`,start,end,gene_ID),gene_exp,by = \"gene_ID\" )%>%arrange(`#chr`,start) \n",
    "    my_gene_bed%>%readr::write_tsv( path = \"$[_output[0]:n]\", na = \"NA\", append = FALSE, col_names = TRUE, quote_escape = \"double\")\n",
    "\n",
    "bash: expand = \"$[ ]\", stderr = f'{_output[0]}.stderr', stdout = f'{_output[0]}.stdout',container = container\n",
    "        bgzip -f $[_output[0]:n]\n",
    "        tabix -p bed $[_output[0]] -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d8e98-4239-450b-87dd-a22efd0ecb1b",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Annotation of leafcutter isoform\n",
    "The following steps processed the output files of leafcutter so that they are TensorQTL ready. Shown below are three intemediate files\n",
    "\n",
    "Exon list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c65439f-42ea-4cd4-a81b-17145d00c953",
   "metadata": {
    "kernel": "Markdown",
    "tags": []
   },
   "source": [
    "chr   |  start  |  end    |  strand  | gene_id | gene_name\n",
    "------|---------|---------|----------|----------|--------------\n",
    "chr1  |  29554  |  30039  |  +       | ENSG00000243485 | MIR1302-2HG\n",
    "chr1  |  30564  |  30667  |  +       | ENSG00000243485 | MIR1302-2HG\n",
    "chr1  |  30976  |  31097  |  +       | ENSG00000243485 | MIR1302-2HG\n",
    "chr1  |  35721  |  36081  |  -       | ENSG00000237613 | FAM138A\n",
    "chr1  |  35277  |  35481  |  -       | ENSG00000237613 | FAM138A\n",
    "chr1  |  34554  |  35174  |  -       | ENSG00000237613 | FAM138A\n",
    "chr1  |  65419  |  65433  |  +       | ENSG00000186092 | OR4F5\n",
    "chr1  |  65520  |  65573  |  +       | ENSG00000186092 | OR4F5\n",
    "chr1  |  69037  |  71585  |  +       | ENSG00000186092 | OR4F5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08980b5-032e-4dd7-9850-4df56bff3fe2",
   "metadata": {
    "kernel": "Markdown"
   },
   "source": [
    "clusters_to_genes\n",
    "\n",
    "\n",
    "|clu    | genes |\n",
    "|--------|---------- |\n",
    "|1:clu_1_+  |    ENSG00000116288|\n",
    "|1:clu_10_+ |    ENSG00000143774|\n",
    "|1:clu_11_+ |    ENSG00000143774|\n",
    "|1:clu_12_+ |    ENSG00000143774|\n",
    "|1:clu_14_- |    ENSG00000126709|\n",
    "|1:clu_15_- |    ENSG00000121753|\n",
    "|1:clu_16_- |    ENSG00000121753|\n",
    "|1:clu_17_- |    ENSG00000116560|\n",
    "|1:clu_18_- |    ENSG00000143549|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfee502-45b2-473e-a405-ac8434a5265e",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "phenotype_group\n",
    "\n",
    "|X1|X2|\n",
    "|-------------|---|\n",
    "| 7:102476270:102478811:clu_309_-:ENSG00000005075 | ENSG00000005075 | \n",
    "| 7:102476270:102478808:clu_309_-:ENSG00000005075 | ENSG00000005075 |\n",
    "| X:47572961:47574002:clu_349_-:ENSG00000008056   | ENSG00000008056 |\n",
    "| X:47572999:47574002:clu_349_-:ENSG00000008056   | ENSG00000008056 |\n",
    "| 8:27236905:27239971:clu_322_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27239279:27239971:clu_322_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27241262:27241677:clu_323_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27241262:27242397:clu_323_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 8:27241757:27242397:clu_323_-:ENSG00000015592   | ENSG00000015592 |\n",
    "| 1:35558223:35559107:clu_4_+:ENSG00000020129     | ENSG00000020129 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037919e2-baf6-4379-9de0-01e01ed8591c",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "The gtf used here should be the collapsed gtf, i.e. the final output of reference_data gtf processing and the one used to called rnaseq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc01f04-fcfa-4cb0-ac06-20251899651a",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[map_leafcutter_cluster_to_gene]\n",
    "## Extract the code in case psichromatic needs to be processed the same way\n",
    "## PheoFile in this step is the intron_count file\n",
    "parameter: intron_count = path\n",
    "input: intron_count, annotation_gtf\n",
    "output: f'{_input[1]}.exon_list', f'{cwd}/{_input[0]:b}.leafcutter.clusters_to_genes.txt'\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "    import pandas as pd\n",
    "    import qtl.annotation\n",
    "    # Load data\n",
    "    annot = qtl.annotation.Annotation(${_input[1]:r})\n",
    "    exon_df = pd.DataFrame([[g.chr, e.start_pos, e.end_pos, g.strand, g.id, g.name]\n",
    "                        for g in annot.genes for e in g.transcripts[0].exons],\n",
    "                       columns=['chr', 'start', 'end', 'strand', 'gene_id', 'gene_name'])\n",
    "    exon_df.to_csv(${_output[0]:r}, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "R:expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "    suppressMessages(library(dplyr, quietly=TRUE))\n",
    "    suppressMessages(library(stringr, quietly=TRUE))\n",
    "    suppressMessages(library(foreach, quietly=TRUE))\n",
    "    # leafcutter functions:\n",
    "    \n",
    "    #' Make a data.frame of meta data about the introns\n",
    "    #' @param introns Names of the introns\n",
    "    #' @return Data.frame with chr, start, end, cluster id\n",
    "    #' @export\n",
    "    get_intron_meta <- function(introns) {\n",
    "      intron_meta <- do.call(rbind, strsplit(introns,\":\"))\n",
    "      colnames(intron_meta) <- c(\"chr\",\"start\",\"end\",\"clu\")\n",
    "      intron_meta <- as.data.frame(intron_meta, stringsAsFactors=FALSE)\n",
    "      intron_meta$start <- as.numeric(intron_meta$start)\n",
    "      intron_meta$end <- as.numeric(intron_meta$end)\n",
    "      intron_meta\n",
    "    }\n",
    "    \n",
    "    #' Work out which gene each cluster belongs to. Note the chromosome names used in the two inputs must match.\n",
    "    #' @param intron_meta Data frame describing the introns, usually from get_intron_meta\n",
    "    #' @param exons_table Table of exons, see e.g. /data/gencode19_exons.txt.gz\n",
    "    #' @return Data.frame with cluster ids and genes separated by commas\n",
    "    #' @import dplyr\n",
    "    #' @export\n",
    "    map_clusters_to_genes <- function(intron_meta, exons_table) {\n",
    "      gene_df <- foreach (chr=sort(unique(intron_meta$chr)), .combine=rbind) %dopar% {\n",
    "    \n",
    "        intron_chr <- intron_meta[ intron_meta$chr==chr, ]\n",
    "        exons_chr <- exons_table[exons_table$chr==chr, ]\n",
    "    \n",
    "        exons_chr$temp <- exons_chr$start\n",
    "        intron_chr$temp <- intron_chr$end\n",
    "        three_prime_matches <- inner_join( intron_chr, exons_chr, by=\"temp\")\n",
    "    \n",
    "        exons_chr$temp <- exons_chr$end\n",
    "        intron_chr$temp <- intron_chr$start\n",
    "        five_prime_matches <- inner_join( intron_chr, exons_chr, by=\"temp\")\n",
    "    \n",
    "        all_matches <- rbind(three_prime_matches, five_prime_matches)[ , c(\"clu\", \"gene_name\")]\n",
    "    \n",
    "        all_matches <- all_matches[!duplicated(all_matches),]\n",
    "    \n",
    "        if (nrow(all_matches)==0) return(NULL)\n",
    "        all_matches$clu <- paste(chr,all_matches$clu,sep=':')\n",
    "        all_matches\n",
    "      }\n",
    "    \n",
    "      clu_df <- gene_df %>% group_by(clu) %>% summarize(genes=paste(gene_name, collapse = \",\"))\n",
    "      class(clu_df) <- \"data.frame\"\n",
    "      clu_df\n",
    "    }\n",
    "\n",
    "    cat(\"LeafCutter: mapping clusters to genes\\n\")\n",
    "    intron_counts <- read.table(${_input[0]:r}, header=TRUE, check.names=FALSE, row.names=1)\n",
    "    intron_meta <- get_intron_meta(rownames(intron_counts))\n",
    "    exon_table <- read.table(${_output[0]:r}, header=TRUE, stringsAsFactors=FALSE)\n",
    "    if(!str_detect(intron_meta,\"chr\")) {\n",
    "        exon_table = exon_table%>%mutate(chr = str_remove_all(chr,\"chr\"))\n",
    "    } else if (!any(str_detect(exon_table$chr[1],\"chr\"))) {\n",
    "        exon_table = exon_table%>%mutate(chr = paste0(\"chr\",chr))\n",
    "    } else (exon_table = exon_table)\n",
    "    stopifnot(is.element('gene_id', colnames(exon_table)))\n",
    "    exon_table[, 'gene_name'] <- exon_table[, 'gene_id']\n",
    "    m <- map_clusters_to_genes(intron_meta, exon_table)\n",
    "    write.table(m, ${_output[1]:r}, sep = \"\\t\", quote=FALSE, row.names=FALSE)\n",
    "  \n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b45bcb-4739-4381-a94f-08d5a39adea4",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_leafcutter_isoforms]\n",
    "parameter: sample_participant_lookup = path()\n",
    "input: phenoFile, annotation_gtf,output_from(\"map_leafcutter_cluster_to_gene\")\n",
    "output: f'{cwd:a}/{_input[0]:bn}.formated.bed.gz', f'{cwd:a}/{_input[0]:bn}.phenotype_group.txt'\n",
    "task: trunk_workers = 1, trunk_size = job_size, walltime = walltime,  mem = mem, tags = f'{step_name}_{_output[0]:bn}'  \n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "    # Load data\n",
    "    tss_df = qtl.io.gtf_to_tss_bed(${_input[1]:r})\n",
    "    bed_df = pd.read_csv(${_input[0]:ar}, sep='\\t', skiprows=0)\n",
    "    bed_df.columns.values[0] = \"#chr\" # Temporary\n",
    "    sample_participant_lookup = Path(\"${sample_participant_lookup:a}\")\n",
    "    cluster2gene_dict = pd.read_csv(${_input[3]:r}, sep='\\t', index_col=0, squeeze=True).to_dict()\n",
    "    print('    ** assigning introns to gene mapping(s)')\n",
    "    n = 0\n",
    "    gene_bed_df = []\n",
    "    group_s = {}\n",
    "    for _,r in bed_df.iterrows():\n",
    "        s = r['ID'].split(':')\n",
    "        cluster_id = s[0]+':'+s[-1]\n",
    "        if cluster_id in cluster2gene_dict:\n",
    "            gene_ids = cluster2gene_dict[cluster_id].split(',')\n",
    "            for g in gene_ids:\n",
    "                gi = r['ID']+':'+g\n",
    "                gene_bed_df.append(tss_df.loc[g, ['chr', 'start', 'end']].tolist() + [gi] + r.iloc[4:].tolist())\n",
    "                group_s[gi] = g\n",
    "        else:\n",
    "            n += 1\n",
    "    if n > 0:\n",
    "        print(f'    ** discarded {n} introns without a gene mapping')\n",
    "\n",
    "    print('  * writing BED files for QTL mapping')\n",
    "    gene_bed_df = pd.DataFrame(gene_bed_df, columns=bed_df.columns)\n",
    "    # sort by TSS\n",
    "    gene_bed_df = gene_bed_df.groupby('#chr', sort=False, group_keys=False).apply(lambda x: x.sort_values('start'))\n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup.is_file():\n",
    "        sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=0, dtype={0:str,1:str}, squeeze=True)\n",
    "        gene_bed_df.rename(columns=sample_participant_lookup_s, inplace=True)\n",
    "    gene_bed_df = gene_bed_df.drop_duplicates()\n",
    "    qtl.io.write_bed(gene_bed_df, ${_output[0]:r})\n",
    "    gene_bed_df[['start', 'end']] = gene_bed_df[['start', 'end']].astype(np.int32)\n",
    "    gene_bed_df[gene_bed_df.columns[4:]] = gene_bed_df[gene_bed_df.columns[4:]].astype(np.float32)\n",
    "    pd.Series(group_s).sort_values().to_csv(${_output[1]:r}, sep='\\t', header=False)\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242e659-c9c8-4061-90df-ab83d761de25",
   "metadata": {
    "kernel": "SoS"
   },
   "source": [
    "## Processing of psichomics output\n",
    "It occurs that the psichomatic by default grouped the isoforms by gene name, so only thing needs to be done is to extract this information and potentially renamed the gene symbol into ENSG ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48571a66-89cc-4930-84a8-7c55ecd8e443",
   "metadata": {
    "kernel": "SoS"
   },
   "outputs": [],
   "source": [
    "[annotate_psichomics_isoforms]\n",
    "parameter: sample_participant_lookup = path()\n",
    "input: phenoFile, annotation_gtf\n",
    "output: f'{cwd:a}/{_input[0]:bn}.formated.bed.gz', f'{cwd:a}/{_input[0]:bn}.phenotype_group.txt'\n",
    "python: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import qtl.io\n",
    "    from pathlib import Path\n",
    "    tss_df = qtl.io.gtf_to_tss_bed(${_input[1]:r}, feature='gene',phenotype_id = \"gene_id\" )\n",
    "    bed_df = pd.read_csv(${_input[0]:ar}, sep='\\t', skiprows=0)\n",
    "    bed_df[\"gene_id\"]  = [x[-1] for x in bed_df.ID.str.split(\"_\")]\n",
    "    sample_participant_lookup = Path(\"${sample_participant_lookup:a}\")\n",
    "    if \"start\" in  bed_df.columns:\n",
    "        bed_df = bed_df.drop([\"#Chr\",\"start\",\"end\"],axis = 1)\n",
    "    output = tss_df.merge(bed_df, left_on = [\"gene_id\"], right_on = [\"gene_id\"],how = \"right\").sort_values([\"chr\",\"start\"])\n",
    "    # change sample IDs to participant IDs\n",
    "    if sample_participant_lookup.is_file():\n",
    "        sample_participant_lookup_s = pd.read_csv(sample_participant_lookup, sep=\"\\t\", index_col=0, dtype={0:str,1:str}, squeeze=True)\n",
    "        output.rename(columns=sample_participant_lookup_s.to_dict(), inplace=True)\n",
    "\n",
    "    # Old code grouping by each gene\n",
    "    #bed_output = output.drop(\"gene_id\" , axis = 1)\n",
    "    #phenotype_group = output[[\"ID\",\"gene_id\"]]\n",
    "\n",
    "    bed_output = output.drop(\"gene_id\" , axis = 1)\n",
    "\n",
    "    # R version \n",
    "    #output = output%>%mutate(gene_type_id = map_chr(str_split( ID ,\"_\"),~paste0(.x[length(.x)],\"_\",.x[1] )) )\n",
    "    \n",
    "    # corrected in python\n",
    "    output = output.assign(gene_type_id = output['ID'].str.split('_').map(lambda x: x[-1] + '_' + x[0]))\n",
    "    phenotype_group = output[[\"ID\",\"gene_type_id\"]]\n",
    "\n",
    "    bed_output.to_csv(${_output[0]:nr},\"\\t\",index = False)\n",
    "    phenotype_group.to_csv(${_output[1]:r},\"\\t\",index = False,header=False)\n",
    "\n",
    "bash: expand= \"${ }\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "    bgzip -f ${_output[0]:n}\n",
    "    tabix ${_output[0]}\n",
    "\n",
    "bash: expand= \"$[ ]\", stderr = f'{_output[0]:n}.stderr', stdout = f'{_output[0]:n}.stdout', container = container\n",
    "        stdout=$[_output[0]:n].stdout\n",
    "        for i in $[_output[0]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `zcat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `zcat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `zcat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        zcat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done\n",
    "        for i in $[_output[1]] ; do \n",
    "        echo \"output_info: $i \" >> $stdout;\n",
    "        echo \"output_size:\" `ls -lh $i | cut -f 5  -d  \" \"`   >> $stdout;\n",
    "        echo \"output_rows:\" `cat $i | wc -l  | cut -f 1 -d \" \"`   >> $stdout;\n",
    "        echo \"output_headerow:\" `cat $i | grep \"##\" | wc -l `   >> $stdout;\n",
    "        echo \"output_column:\" `cat $i | grep -V \"##\" | head -1 | wc -w `   >> $stdout;\n",
    "        echo \"output_preview:\"   >> $stdout;\n",
    "        cat $i  | grep -v \"##\" | head  | cut -f 1,2,3,4,5,6   >> $stdout ; done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SoS",
   "language": "sos",
   "name": "sos"
  },
  "language_info": {
   "codemirror_mode": "sos",
   "file_extension": ".sos",
   "mimetype": "text/x-sos",
   "name": "sos",
   "nbconvert_exporter": "sos_notebook.converter.SoS_Exporter",
   "pygments_lexer": "sos"
  },
  "sos": {
   "kernels": [
    [
     "Bash",
     "bash",
     "Bash",
     "#E6EEFF",
     ""
    ],
    [
     "Markdown",
     "markdown",
     "markdown",
     "",
     ""
    ],
    [
     "SoS",
     "sos",
     "",
     "",
     "sos"
    ]
   ],
   "version": "0.24.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
